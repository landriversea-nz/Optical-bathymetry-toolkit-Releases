from flask import Flask, render_template, url_for, flash, redirect
from flask import send_file,Flask, render_template, url_for, flash, redirect,request,jsonify, make_response
import scipy as sc
import numpy as np
import pandas as pd
from werkzeug.utils import secure_filename
from flask import send_from_directory
import json
from scipy.stats import pearsonr 
import shutil
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import max_error
from sklearn.metrics import explained_variance_score
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
import os,sys
from sklearn.cluster import KMeans
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import GridSearchCV 
from sklearn.manifold import TSNE
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import PowerTransformer
from sklearn.cross_decomposition import PLSRegression
from sklearn.neural_network import MLPRegressor
from xgboost import XGBRegressor
import joblib
import random
import webbrowser
import itertools
from licensing.models import *
from licensing.methods import Key, Helpers
import urllib.request
from threading import Timer
import pytz
import base64
from datetime import datetime 
from tzlocal import get_localzone
import netifaces
from numpy import arange
from scipy.optimize import curve_fit
from numpy import sin
from numpy import sqrt
import geopandas as gpd
import rasterio
import socket
import win32security
import subprocess
import platform
import scipy.spatial as spatial
from sklearn.ensemble import IsolationForest
import matplotlib
matplotlib.use('agg') # Use agg to avoid matplotlib and pyinstaller library clash
cli = sys.modules['flask.cli'] # Get cli - command line interface object
cli.show_server_banner = lambda *x: None # This hides any cli print statements or text - basically to avoid an open terminal

app=Flask(__name__) # Initialize flask

# I set an secret key to protect from forgery and cyber attacks..get random values from python secret token_hex(16)
app.config['SECRET_KEY']='1f49b3737521e1dab26d590be6b56dab'

def open_browser():
      webbrowser.open_new('http://127.0.0.1:5000/') # Open web-page in electron app - this is the localhost server address


#### NOTES ON STORAGE FOLDERS: 
## 'model_files' folder is the storage folder for curve fitting and ml workflow module files
## 'apply_pretrained_model_storage' is the storage folder for Pre-trained module files
## 'data_prep_storage' is the storage folder for Data-Preparation module files
##  No storage folder for smoothing output module

#### IMPORTANT NOTES TO REFER WHILE READING/EDITING CODE
## In this whole code, validation data is sometimes referred to as test data
## In this whole code, the full-data is referred to the final dataset ( which might or might not have actual target values ), over which a trained model predicts target values and outputs a csv or tif
## The word 'actual' in sentences like actual data means the input data's values - and not prediction values
## #https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc - this is standardizing vs normalization
## For processing training data, running ml algorithms and even running full-data functions - there were initially three options for data processing - 1. User selects to use default options only - 'default_all' 2. User manually selects his desired features - a list of all processing options 3. User does not choose any options - the software computes and finds the best processing options on its own - this option is called 'best_options'. Some codes are written for all three methods - so as of the current version, only the best_options is used - the rest are non-functinal, but haven't removed them since we might make use of those codes in the future

#### SECTIONS IN ORDER :
## 1. SECTION 1: HELPER FUNCTIONS FOR FEATURE ENGINEERING AND DATA PROCESSING - ONLY FOR MACHINE-LEARNING MODULE
## 2. SECTION 2: HELPER FUNCTIONS FOR TRAINING MACHINE LEARNING TECHNIQUES AND CALCULATING METRIC SCORES OF MODELS FOR TRAINING AND VALIDATION DATASETS
## 3. SECTION 3: HELPER FUNCTIONS FOR APPLYING FEATURE ENGINEERING AND DATA PRE-PROCESSING METHODS TO FULL DATA (FINAL PREDICTIONS) 
## 4. SECTION 4: FUNCTIONS FOR MAIN PAGE, TO ACCESS ALL MODULES AND LICENSE VALIDATION
## 5. SECTION 5: FUNCTIONS FOR INITIAL INPUT DATA PRE-PROCESSING COMMON TO BOTH CURVE FIT MODULE AND MACHINE LEARNING MODULE
## 6. SECTION 6: FUNCTIONS FOR TRAINING CURVE-FITTING OBJECTIVE EQUATIONS AND APPLYING FOUR DIFFERENT VALIDATION STRATEGIES 
## 7. SECTION 7: GRAPHING FUNCTIONS FOR CURVE-FITTING MODULE AND SAVE PLOTS AS PDF
## 8. SECTION 8: PREDICTION ON FULL-DATA, FUNCTIONS FOR CURVE-FITTING MODULE 
## 9. SECTION 9: FUNCTIONS FOR TRAINING MACHINE LEARNING ALGORITHMS AND APPLYING FOUR DIFFERENT VALIDATION STRATEGIES 
## 10. SECTION 10: GRAPHING FUNCTIONS FOR MACHINE-LEARNING MODULE AND SAVE PLOTS AS PDF
## 11. SECTION 11: PREDICTION ON FULL-DATA, FUNCTIONS FOR MACHINE-LEARNING MODULE
## 12. SECTION 12: PRE-TRAINED MODULE FUNCTIONS - TO PROCESS FULL-DATA AND APPLY PRE-TRAINED MODELS TO GET OUTPUT PREDICTIONS
## 13. SECTION 13: DATA PREPARATION MODULE - RASTER AND SHP SECTION
## 14. SECTION 14: SMOOTHING OUTPUT PREDICTIONS-DATA MODULE




def load_data(data_path): # Function to load csv from model_files folder and return a dataset
    data=pd.read_csv('model_files/'+data_path)
    return data

def load_data_apply_module(data_path): # Function to load csv from apply_pretrained_model_storage folder and return a dataset
    data=pd.read_csv('apply_pretrained_model_storage/'+data_path)
    return data

def distance_cal(x1,y1,x2,y2): # Calculate aggregate distance - mathematical formula is d=√((x_2-x_1)²+(y_2-y_1)²) - between every point and its previous point
   return np.sqrt(((x2-x1)**2)+((y2-y1)**2)) 

def tts(df,y,test_split): #Train test split function, get dataframe, target variables, test split and the function returns split datasets
    x_train,x_test,y_train,y_test=train_test_split(df,y,test_size=test_split,random_state=0,shuffle=True) # Shuffle set to true to shuffle the dataset before randomly dividing into train and test ( validation ) datasets
    return x_train,x_test,y_train,y_test # return the different values back to the function statement - x_train is the training dataset, x_test is the validation dataset,  y_train is target values in training dataset, y_test is target values in validation dataset


#### SECTION 1 :  FUNCTIONS FOR FEATURE ENGINEERING AND DATA PRE-PROCESSING - ONLY FOR MACHINE LEARNING MODULE

def logperm_gen(x,r,g,b, feats): #Create log permutations for the dataset - for now it creates log of b/r and b/g, these two alone have proved to work effectively. Tip for improvement: try all the other combinations like log(r/g), log(r/b) etc, I did not include them as they increase the computation time
    if( ('hsv' not in feats and 'rgb' in feats) or ('hsv' not in feats and 'rgb' not in feats)): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
        x['ln(b/r)']=0 # Initiate ln(b/r) column and set all rows to 0
        x['ln(b/g)']=0 # Initiate ln(b/g) column and set all rows to 0
        x['ln(b/r)'] = x.apply(lambda row: np.log10((row[b]/row[r])), axis=1) # Use lambda function to quickly calculate log(b/r) for all rows. Axis=1 indicates vertical column 
        x['ln(b/g)'] = x.apply(lambda row: np.log10((row[b]/row[g])), axis=1) # Use lambda function to quickly calculate log(b/r) for all rows. Axis=1 indicates vertical column
    
    if( 'hsv' in feats and 'rgb' not in feats): # Feats variable contains what colourspace has been selected. If only hsv is selected in the options
        
        x['ln(v/h)']=0 # Initiate ln(v/h) column and set all rows to 0
        x['ln(v/s)']=0 # Initiate ln(v/s) column and set all rows to 0
        x['ln(v/h)'] = x.apply(lambda row: np.log10((row['V_generated']/row['H_generated'])), axis=1) # Use lambda function to quickly calculate log(v/h) for all rows. Axis=1 indicates vertical column 
        x['ln(v/s)'] = x.apply(lambda row: np.log10((row['V_generated']/row['S_generated'])), axis=1) # Use lambda function to quickly calculate log(v/s) for all rows. Axis=1 indicates vertical column 
    
    if( 'hsv' in feats and 'rgb' in feats): # Feats variable contains what colourspace has been selected. If both rgb and hsv are selected in the options
        
        x['ln(b/r)']=0 # Initiate ln(b/r) column and set all rows to 0
        x['ln(b/g)']=0 # Initiate ln(b/g) column and set all rows to 0
        x['ln(b/r)'] = x.apply(lambda row: np.log10((row[b]/row[r])), axis=1) # Use lambda function to quickly calculate log(b/r) for all rows. Axis=1 indicates vertical column 
        x['ln(b/g)'] = x.apply(lambda row: np.log10((row[b]/row[g])), axis=1) # Use lambda function to quickly calculate log(b/r) for all rows. Axis=1 indicates vertical column
    
        x['ln(v/h)']=0 # Initiate ln(v/h) column and set all rows to 0
        x['ln(v/s)']=0 # Initiate ln(v/s) column and set all rows to 0
        x['ln(v/h)'] = x.apply(lambda row: np.log10((row['V_generated']/row['H_generated'])), axis=1) # Use lambda function to quickly calculate log(v/h) for all rows. Axis=1 indicates vertical column 
        x['ln(v/s)'] = x.apply(lambda row: np.log10((row['V_generated']/row['S_generated'])), axis=1) # Use lambda function to quickly calculate log(v/s) for all rows. Axis=1 indicates vertical column
    
    return x # Return the dataset with logarthmic combinations added for the selected colourspace

def greyscale_gen(x,r,g,b): # Create greyscale values. If only hsv colourspace is selected - calculate greyscale for hsv. If only rgb colourspace is selected - calculate greyscale for rgb. If both rgb and hsv are selected, then calculate greyscale for rgb only - no need to calculate for hsv also, this is one way to not have too many features for data processing
    x['Greyscale']=x.apply(lambda row: (0.3*row[r] + 0.59*row[g] + 0.11*row[b]), axis=1) # Use lambda function to quickly calculate greyscale values for the r,g,b values or h,s,v values for all rows. Axis=1 indicates vertical column 
    return x # Return the dataset with a new column of greyscale values for the selected colourspace

def cluster_data(data_new,r,g,b): # Cluster data into high, medium, low based on colour values. Helpful to identify bright spots and in identifying some shallow regions by the side of the river. Tip for improvement in this function : Try having a tts before clustering, so that it does not fit on full input data and fits just on the training data. tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the correlation of the training data's feature values. It is always better in cases where no validation set is provided, this will be greatly helpful
    data_new['RGB_Combo']=0 # Initiate a column called RGB_Combo and set all its rows to 0
    data_new['RGB_Combo']=data_new.apply(lambda row: row[r]*row[g]*row[b], axis=1) # Use lambda function to quickly multiply r*g*b or h*s*v but not both- depending on the colourspace selected, for all rows. Axis=1 indicates vertical column 
    data_new=data_new.reset_index().drop(['index'],axis=1) # Reset the index to not have random values as index
    rgb=np.vstack((data_new[[r,g,b]].values)) # Create a vertical stack of r,g,b values or h,s,v values depending on the colourspace selected
    wcss=[] # Initiate an empty list called wcss
    mapping={} # Initiate an empty dict called mapping
    for i in range(1,10):
      kmeans = MiniBatchKMeans(n_clusters=i,random_state=10).fit(rgb) # The next few lines will try to find the best parameters for the algorithm to cluster the column values into three parts - high, medium, low
      wcss.append(kmeans.inertia_)
      mapping[i]=kmeans.inertia_

    optimal=0
    diff=[]
    for i in range(2,10):
      diff.append(-(mapping[i]-mapping[i-1]))
    diff_mean=np.mean(diff)
    for i in range(2,10):
      if((mapping[i-1]-mapping[i]>diff_mean)==False):
        optimal=i-1
        break
    kmeans = MiniBatchKMeans(n_clusters=3 ,random_state=10).fit(rgb) # Train the MiniBatchKMeans algorithm by fitting the r,g,b or h,s,v, but not both. Use n_clusters = 3 to group each row of the dataset into one of the 3 different clusters depending on that row's RGB_Combo value
    data_new.loc[:, 'point_cluster'] = kmeans.predict(rgb) # Use this algorithm to cluster the column values into three parts - high, medium, low
    joblib.dump(kmeans,'model_files/'+'kmeans.pkl') # Save the trained algorithm as pkl file to apply over full-data
    data_new['point_cluster']=data_new['point_cluster'].astype('int64') # Make the values integer data type - 0 - low, 1 -medium, 2- high clusters
    # store this csv to use for caluculating mean/med during full river predictions
    data_new.to_csv('model_files/'+'min_med_max.csv',index=False) # Save as csv to access later
    max_indx=data_new.groupby(['point_cluster']).mean()['RGB_Combo'][data_new.groupby(['point_cluster']).mean()['RGB_Combo']==np.max(data_new.groupby(['point_cluster']).mean()['RGB_Combo'])].index # Get the index of the dataset's high cluster's mean value
    min_indx=data_new.groupby(['point_cluster']).mean()['RGB_Combo'][data_new.groupby(['point_cluster']).mean()['RGB_Combo']==np.min(data_new.groupby(['point_cluster']).mean()['RGB_Combo'])].index # Get the index of the dataset's low cluster's mean value
    indexs=list(data_new.groupby(['point_cluster']).mean().index) # List of minimum (low) cluster's mean value's index, maximum(high) cluster's mean value's index and medium cluster's mean value's index
    indexs.remove(min_indx) # Remove the dataset's minimum (low) cluster's mean value's index value from the list
    indexs.remove(max_indx) # Remove the dataset's maximum (high) cluster's mean value's index value from the list
    med_indx=indexs[0] # We removed those two values from the list to get the dataset's medium cluster's mean value's index value from the list
    data_new['Point_cluster']='' #Initiate an empty column called Point_cluster - the 'p' is capital in this new column. The old column with numbers for clusters has non-capital 'p'
    data_new['Point_cluster']=np.where(data_new['point_cluster']==max_indx[0],'High',data_new['Point_cluster']) # Label the clusters properly based on the maximum, medium and minimum cluster index values - Rename from 0,1,2, to Low, medium, high clusters- easier to maintain dataset
    data_new['Point_cluster']=np.where(data_new['point_cluster']==med_indx,'Medium',data_new['Point_cluster']) # Label the clusters properly based on the maximum, medium and minimum cluster index values - Rename from 0,1,2, to Low, medium, high clusters- easier to maintain dataset
    data_new['Point_cluster']=np.where(data_new['point_cluster']==min_indx[0],'Low',data_new['Point_cluster']) # Label the clusters properly based on the maximum, medium and minimum cluster index values - Rename from 0,1,2, to Low, medium, high clusters- easier to maintain dataset
    data_new.drop(['point_cluster'],axis=1,inplace=True) # Now that we have a new column with 0,1,2 renamed to high, medium and low, we can remove the old column with small 'p' in Point_cluster
    data_clean=peak_ratio_rgb(data_new,r,g,b) # Create ratio of every row's mean value to the peak value for every cluster, keep as a separate column - with these values, the ml algorithms can pick up shallow region values and bright spots
    data_clean=pd.get_dummies(data_clean) # Converts categorical variable into dummy/indicator variables. For a single column with three different categorical strings (high,medium, low) - this function will create three separate columns with binary values indicating each row's cluster value. For a row with 'high' cluster value, the three new columns would be column 'low' with value 0, 'medium' with value 0, 'high' with value 1. ML algorithms work properly with non-string values
    data_clean.drop(['RGB_Combo'],axis=1,inplace=True) # Remove the column which was created by multiplying r,g,b values    
    return data_clean # Return the updated dataset

def peak_ratio_rgb(hutt_full,r,g,b): # This function will create ratio of every row's mean value to the peak value for every cluster, and keep it as a separate column
    peak_r=hutt_full.groupby(['Point_cluster']).max()[r] # group the Point_cluster column by r or h - depending on the colourspace, and get the max r or h value on high, medium and low clusters
    peak_g=hutt_full.groupby(['Point_cluster']).max()[g] # group the Point_cluster column by g or s - depending on the colourspace, and get the max g or s value on high, medium and low clusters
    peak_b=hutt_full.groupby(['Point_cluster']).max()[b] # group the Point_cluster column by b or v - depending on the colourspace, and get the max b or v value on high, medium and low clusters
    peak_rgb=hutt_full.groupby(['Point_cluster']).max()['RGB_Combo'] # group the Point_cluster column by RGB_Combo - depending on the colourspace, and get the max RGB_Combo value on high, medium and low clusters
    hutt_full[[r,g,b,'RGB_Combo','Point_cluster']].to_csv('model_files/'+'groupby.csv',index=False) # Save this file to access later while processing full-data csv
    
    hutt_full['Ratio_peak_R']=0.0 #  Initiate Ratio_peak_R column and set all rows to 0
    hutt_full['Ratio_peak_G']=0.0 #  Initiate Ratio_peak_G column and set all rows to 0
    hutt_full['Ratio_peak_B']=0.0 #  Initiate Ratio_peak_B column and set all rows to 0
    hutt_full['Ratio_peak_RGBCombo']=0.0 #  Initiate Ratio_peak_RGBCombo column and set all rows to 0
    
    #low
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[r]/peak_r['Low'],hutt_full['Ratio_peak_R']) # For those r column points in the dataset where the cluster value is 'low', divide every row by the peak r value of all 'low' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'low' cluster value
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[g]/peak_g['Low'],hutt_full['Ratio_peak_G']) # For those g column points in the dataset where the cluster value is 'low', divide every row by the peak g value of all 'low' clusters and save the value to the Ratio_peak_G column. Do this for every row with 'low' cluster value
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[b]/peak_b['Low'],hutt_full['Ratio_peak_B']) # For those b column points in the dataset where the cluster value is 'low', divide every row by the peak b value of all 'low' clusters and save the value to the Ratio_peak_B column. Do this for every row with 'low' cluster value
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='Low',hutt_full['RGB_Combo']/peak_rgb['Low'],hutt_full['Ratio_peak_RGBCombo']) # For those Ratio_peak_RGBCombo column points in the dataset where the cluster value is 'low', divide every row by the peak r*g*b value of all 'low' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'low' cluster value
    
    #medium
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[r]/peak_r['Medium'],hutt_full['Ratio_peak_R']) # For those r column points in the dataset where the cluster value is 'medium', divide every row by the peak r value of all 'medium' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'medium' cluster value
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[g]/peak_g['Medium'],hutt_full['Ratio_peak_G']) # For those g column points in the dataset where the cluster value is 'medium', divide every row by the peak g value of all 'medium' clusters and save the value to the Ratio_peak_G column. Do this for every row with 'medium' cluster value
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[b]/peak_b['Medium'],hutt_full['Ratio_peak_B']) # For those b column points in the dataset where the cluster value is 'medium', divide every row by the peak b value of all 'medium' clusters and save the value to the Ratio_peak_B column. Do this for every row with 'medium' cluster value
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full['RGB_Combo']/peak_rgb['Medium'],hutt_full['Ratio_peak_RGBCombo']) # For those Ratio_peak_RGBCombo column points in the dataset where the cluster value is 'medium', divide every row by the peak r*g*b value of all 'medium' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'medium' cluster value
    
    #high
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='High',hutt_full[r]/peak_r['High'],hutt_full['Ratio_peak_R']) # For those r column points in the dataset where the cluster value is 'high', divide every row by the peak r value of all 'high' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'high' cluster value
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='High',hutt_full[g]/peak_g['High'],hutt_full['Ratio_peak_G']) # For those g column points in the dataset where the cluster value is 'high', divide every row by the peak g value of all 'high' clusters and save the value to the Ratio_peak_G column. Do this for every row with 'high' cluster value
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='High',hutt_full[b]/peak_b['High'],hutt_full['Ratio_peak_B']) # For those b column points in the dataset where the cluster value is 'high', divide every row by the peak b value of all 'high' clusters and save the value to the Ratio_peak_B column. Do this for every row with 'high' cluster value
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='High',hutt_full['RGB_Combo']/peak_rgb['High'],hutt_full['Ratio_peak_RGBCombo']) # For those Ratio_peak_RGBCombo column points in the dataset where the cluster value is 'high', divide every row by the peak r*g*b value of all 'high' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'high' cluster value
   # print(peak_r,peak_g,peak_b) 
    return hutt_full  # Return the updated dataset

def poly_creation_cluster(df,y): # To create polynomial features from existing columns - extends by adding extra predictors,a simple way to provide non-linear approach for the ml algorithms. Tip for improvement in this function : Try having a tts before clustering, so that it does not fit on full input data and fits just on the training data. tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the correlation of the training data's feature values. It is always better in cases where no validation set is provided, this will be greatly helpful
    CAT_COUNTER=0 # A counter for categorical variables
    if(all(x in df.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them while getting polynomial features - no need to create higher order values for binary values
        pcl=df[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for polynomial feature engineering, so we have to remove them before performing polynomial feaure engineering and so save them to a temporary dataframe to merge them later
        df.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    poly = PolynomialFeatures(2) # Initiate a polynomial features object - the '2' means create upto maximum of two-degree polynomial feature combinations
    x_poly=poly.fit_transform(df) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with more extended columns and polynomial feature combinations
    poly_names=poly.get_feature_names(['Feature'+str(l) for l in range(1,len(np.array(poly.get_feature_names())))]) # For all feaures in the new dataset, get their names
    df_poly=pd.DataFrame(x_poly,columns=poly_names) # Create a new dataframe with column names as you got above and data as the above transformed dataset
    df_poly.drop(['1'],inplace=True,axis=1) # There usually is a column '1' created extra, so just drop it
    #append the clusters
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        df_poly=pd.concat([df_poly,pcl],axis=1) # Concat on vertical axis (add on columns)
    return df_poly # Return the updated dataset

def poly_creation_no_cluster(df,y): # To create polynomial features from existing columns - extends by adding extra predictors,a simple way to provide non-linear approach for the ml algorithms. This function runs when there is no clusters already created - it is safe to have as a separate function rather than using if statement in previous function. Tip for improvement in this function : Try having a tts before clustering, so that it does not fit on full input data and fits just on the training data. tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the correlation of the training data's feature values. It is always better in cases where no validation set is provided, this will be greatly helpful
   # print(df.dtypes)
    df=df.reset_index().drop(['index'],axis=1) # Reset the index to not have random values as index
    df=pd.DataFrame(df)
    poly = PolynomialFeatures(2) # Initiate a polynomial features object - the '2' means create upto maximum of two-degree polynomial feature combinations
    x_poly=poly.fit_transform(df) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with more extended columns and polynomial feature combinations
    poly_names=poly.get_feature_names(['Feature'+str(l) for l in range(1,len(np.array(poly.get_feature_names())))]) # For all feaures in the new dataset, get their names
    df_poly=pd.DataFrame(x_poly,columns=poly_names) # Create a new dataframe with column names as you got above and data as the above transformed dataset
    df_poly.drop(['1'],inplace=True,axis=1) # There usually is a column '1' created extra, so just drop it
    #append the clusters
    return df_poly # Return the updated dataset
  

def correl_cluster(df_poly,y,type): # Using correlation to remove some features - Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features. Input to this function is the training data, training target values and type - this value is the type algorithm. Tree based algorithms are called type 'rf', neural network type algorithms are called type 'nn', and rest are called type 'common' 
    x_train,x_test,y_train,y_test=tts(df_poly,y,0.2)# tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the correlation of the training data's feature values. It is always better to have correlation done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    corr_features = set() # Initialize an empty set - at the end if this function, the features in this set will be dropped because they are found to have high multicollinearity
    CAT_COUNTER=0 # A counter for categorical variables
    if(all(x in df_poly.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them while getting polynomial features - gettin correlation matrix for binary values is not useful           
        #remove pcl again, pearsosn corr not appropriate for categorical variables
        pcl_tr=x_train[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for polynomial feature engineering, so we have to remove them before performing correlation engineering and so save them to a temporary dataframe to merge them later
        x_train.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    # create the correlation matrix (default to pearson)
    # updated corr analysis- if two have high corr- check which has <80 to target. If both high - leave. If both low- remove i.
    corr_matrix = x_train.corr() # Get the correlation matrix for the x_train data
    from sklearn.linear_model import Lasso
    from sklearn.feature_selection import SelectFromModel
    for i in range(len(corr_matrix.columns)): # Lookup on how a correlation matrix would be like. For the length of all columns in the correaltion matrix do this loop
        for j in range(i): # For each column number do this loop
            if abs(corr_matrix.iloc[i, j]) > 0.90: # Check if the coorelation value between the Ith column and Jth column is greater than 0.90, if yes then proceed below
                corr_target_i,_=pearsonr(x_train[corr_matrix.columns[i]],y_train) # Now get the pearson's correlation value between the Ith column and the target variable - need statistics to understand these lines
                corr_target_j,_=pearsonr(x_train[corr_matrix.columns[j]],y_train) # Now get the pearson's correlation value between the Jth column and the target variable - need statistics to understand these lines
                if((abs(corr_target_i)>0.60) & (abs(corr_target_j)>0.60)): # If the correlation value between the target variable & Ith column is greater than 0.60 and the correlation value between the target variable & Jth column is greater than 0.60, then proceed below
                   # print("both high", corr_target_i,corr_matrix.columns[i])
                    continue # Don't do anything just proceed to next loop. This means both are highly correlated to each other as well as the target variable, so we will need both of these features for training
                if((abs(corr_target_i)>0.60) & (abs(corr_target_j)<0.60)): # If the correlation value between the target variable & Ith column is greater than 0.60 but the correlation value between the target variable & Jth column is less than 0.60, then proceed below
                    colname = corr_matrix.columns[j] # Get the Jth feature name
                    corr_features.add(colname) # Add the feature value to the corr_features set
                   # print("J LOW", corr_matrix.columns[i],corr_matrix.columns[j])
                if((abs(corr_target_i)<0.60) & (abs(corr_target_j)>0.60)): # If the correlation value between the target variable & Ith column is less than 0.60 but the correlation value between the target variable & Jth column is greater than 0.60, then proceed below
                    colname = corr_matrix.columns[i] # Get the Ith feature name
                    corr_features.add(colname) # Add the feature value to the corr_features set
                   # print("I LOW", corr_matrix.columns[i],corr_matrix.columns[j])
                if((abs(corr_target_i)<0.60) & (abs(corr_target_j)<0.60)): # If the correlation value between the target variable & Ith column is less than 0.60 and the correlation value between the target variable & Jth column is less than 0.60, then proceed below
                    colname = corr_matrix.columns[i] # Get only the Ith feature name - no need to have two similar features both of which do not affect the target variable, only increases the computation time. So drop one of them
                    corr_features.add(colname) # Add the feature value to the corr_features set
                   # print("BOTH LOW", corr_target_i,corr_matrix.columns[i])
    
    #print(corr_features)         
    x_train.drop(labels=corr_features, axis=1, inplace=True) # Now drop the feature names in the corr_features set
    
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        #append back to use lasso on these columns...without any interactions among categorical variables by polynomial fc
        x_train=pd.concat([x_train,pcl_tr],axis=1)
    selected_feat=x_train.columns # Get the final column names
    X_train_selected=df_poly[selected_feat] # Get the full training dataset, with just the feature names that were filtered in the process
    if(type=="rf"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        X_train_selected.to_csv('model_files/'+'correl_cluster_rf.csv',index=False)
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        X_train_selected.to_csv('model_files/'+'correl_cluster_nn.csv',index=False)
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        X_train_selected.to_csv('model_files/'+'correl_cluster_common.csv',index=False)    
    return X_train_selected # Return the updated dataset

def correl_no_cluster(df_poly,y,type): # Using correlation to remove some features - Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features. Input to this function is the training data, training target values and type - this value is the type algorithm. Tree based algorithms are called type 'rf', neural network type algorithms are called type 'nn', and rest are called type 'common' 
    x_train,x_test,y_train,y_test=tts(df_poly,y,0.2)# tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the correlation of the training data's feature values. It is always better to have correlation done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    corr_features = set() # Initialize an empty set - at the end if this function, the features in this set will be dropped because they are found to have high multicollinearity
    # create the correlation matrix (default to pearson)
    corr_matrix = x_train.corr() # Get the correlation matrix for the x_train data
    from sklearn.linear_model import Lasso
    from sklearn.feature_selection import SelectFromModel
    for i in range(len(corr_matrix.columns)): # Lookup on how a correlation matrix would be like. For the length of all columns in the correaltion matrix do this loop
        for j in range(i):  # For each column number do this loop
            if abs(corr_matrix.iloc[i, j]) > 0.90: # Check if the coorelation value between the Ith column and Jth column is greater than 0.90, if yes then proceed below
                corr_target_i,_=pearsonr(x_train[corr_matrix.columns[i]],y_train) # Now get the pearson's correlation value between the Ith column and the target variable - need statistics to understand these lines
                corr_target_j,_=pearsonr(x_train[corr_matrix.columns[j]],y_train) # Now get the pearson's correlation value between the Jth column and the target variable - need statistics to understand these lines
                if((abs(corr_target_i)>0.60) & (abs(corr_target_j)>0.60)): # If the correlation value between the target variable & Ith column is greater than 0.60 and the correlation value between the target variable & Jth column is greater than 0.60, then proceed below
                   # print("both high", corr_target_i,corr_matrix.columns[i])
                    continue # Don't do anything just proceed to next loop. This means both are highly correlated to each other as well as the target variable, so we will need both of these features for training
                if((abs(corr_target_i)>0.60) & (abs(corr_target_j)<0.60)): # If the correlation value between the target variable & Ith column is greater than 0.60 but the correlation value between the target variable & Jth column is less than 0.60, then proceed below
                    colname = corr_matrix.columns[j] # Get the Jth feature name
                    corr_features.add(colname) # Add the feature value to the corr_features set
                  #  print("J LOW", corr_matrix.columns[i],corr_matrix.columns[j])
                if((abs(corr_target_i)<0.60) & (abs(corr_target_j)>0.60)): # If the correlation value between the target variable & Ith column is less than 0.60 but the correlation value between the target variable & Jth column is greater than 0.60, then proceed below
                    colname = corr_matrix.columns[i] # Get the Ith feature name
                    corr_features.add(colname) # Add the feature value to the corr_features set
                  #  print("I LOW", corr_matrix.columns[i],corr_matrix.columns[j])
                if((abs(corr_target_i)<0.60) & (abs(corr_target_j)<0.60)): # If the correlation value between the target variable & Ith column is less than 0.60 and the correlation value between the target variable & Jth column is less than 0.60, then proceed below
                    colname = corr_matrix.columns[i] # Get only the Ith feature name - no need to have two similar features both of which do not affect the target variable, only increases the computation time. So drop one of them
                    corr_features.add(colname) # Add the feature value to the corr_features set
                  #  print("BOTH LOW", corr_target_i,corr_matrix.columns[i])
                
    x_train.drop(labels=corr_features, axis=1, inplace=True) # Now drop the feature names in the corr_features set
    selected_feat=x_train.columns # Get the final column names
    X_train_selected=df_poly[selected_feat] # Get the full training dataset, with just the feature names that were filtered in the process
    if(type=="rf"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        X_train_selected.to_csv('model_files/'+'correl_cluster_rf.csv',index=False)
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        X_train_selected.to_csv('model_files/'+'correl_cluster_nn.csv',index=False)
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        X_train_selected.to_csv('model_files/'+'correl_cluster_common.csv',index=False)      
    
    return X_train_selected # Return the updated dataset

def lasso_reg(df,y):#The lasso regression allows you to shrink or regularize these coefficients to avoid overfitting and make them work better on different datasets. This type of regression is used when the dataset shows high multicollinearity or when you want to automate variable elimination and feature selection
    # scaler = StandardScaler()
    # scaler.fit(x_train)
    sel_=Lasso(alpha=0.01, max_iter=10e5) # Initiate lasso object with those two parameter values
    x_train,x_test,y_train,y_test=tts(df,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the lasso on the training data's feature values. It is always better to have lasso done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    #defining parameter range 
    param_grid = {'alpha': [0.01]}       
    ls = GridSearchCV(Lasso(max_iter=10e5), param_grid, refit = True, verbose = 3) # Get best parameters for the lasso regression algorithm
    
    ls.fit(x_train, y_train) # Fit the lasso object to find the best parameters
    best_coeff=ls.best_params_  # Get the best parameters coefficients to fit on the full training data
    sel_=Lasso(alpha=best_coeff['alpha'],max_iter=10e5) # Re-initiate the lasso object with best parametrs coefficients
    sel_.fit(x_train, y_train) # Fit the object to training data and training target values
    selected_feat = x_train.columns[sel_.coef_!=0] # Select those columns which are highly correlated to the target variable by taking those features whose coefficients are not zero
    X_train_selected =df[selected_feat] # Get the full training dataset, with just the feature names that were filtered in the process
    X_train_selected.to_csv('model_files/'+'lasso_reg.csv',index=False) # Save the updated csv to use later during the full-data part
    return X_train_selected # Return the updated dataset

def lasso_reg_bestop(df,y): # The same as above function, but without grid search. This function is used only when the software calculates the best feature engineering processes - to speed things up, we remove grid search and have a constant value
    print("in this new lasso")
    sel_=Lasso(alpha=0.01) # Initiate lasso object with those two parameter values
    x_train,x_test,y_train,y_test=tts(df,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the lasso on the training data's feature values. It is always better to have lasso done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    #defining parameter range 
   
    sel_=Lasso(alpha=0.01,max_iter=10e5) # Re-initiate the lasso object with some more proper parameter coefficients
    sel_.fit(x_train, y_train) # Fit the object to training data and training target values
    selected_feat = x_train.columns[sel_.coef_!=0] # Select those columns which are highly correlated to the target variable by taking those features whose coefficients are not zero
    X_train_selected =df[selected_feat] # Get the full training dataset, with just the feature names that were filtered in the process
    print(selected_feat)
    X_train_selected.to_csv('model_files/'+'lasso_reg.csv',index=False) # Save the updated csv to use later during the full-data part
    return X_train_selected # Return the updated dataset

def pca_reduction_no_cluster(temp,y,type): # The use of PCA is to represent a multivariate data table as smaller set of variables. This overview may uncover the relationships between observations and variables - this is a non-cluster version.
    pca1=PCA(n_components=3) # Initiate a PCA object with n components as 3 - n components represents the total number of features the dataset should have after getting processed by PCA 
    x_train,x_test,y_train,y_test=tts(temp,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the PCA on the training data's feature values. It is always better to have PCA done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    X_train= pca1.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with reduced feature columns to the number we had specified while initiating the pca object
    X_test = pca1.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pca1,'model_files/'+'pca_lr.pkl') # Save the pkl file for later use and to apply on full-data
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pca1,'model_files/'+'pca_common.pkl')   # Save the pkl file for later use and to apply on full-data
    dxr=pd.DataFrame(X_train, columns=[['pca-one','pca-two','pca-three']],index=x_train.index) # Save the PCA computed values on training dataset to a new dataframe with three columns as specified while initiating PCA object
    dxt=pd.DataFrame(X_test, columns=[['pca-one','pca-two','pca-three']],index=x_test.index)  # Save the PCA computed values on validation dataset to a new dataframe with three columns as specified while initiating PCA object       
    sub=pd.concat([dxr,dxt],axis=0) # Append those two separate dataframes back into a single dataframe
    sub.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return sub # Return the updated dataset

def pca_reduction_cluster(temp,y,type): # The same as above, but if the data has clusters included then use this function to not consider the cluster column while doing PCA
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in temp.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for PCA    
        pcl_tr=temp[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for PCA feature engineering, so we have to remove them before performing PCA engineering and so save them to a temporary dataframe to merge them later
        temp.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    pca1=PCA(n_components=3) # Initiate a PCA object with n components as 3 - n components represents the total number of features the dataset should have after getting processed by PCA 
    x_train,x_test,y_train,y_test=tts(temp,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit PCA on the training data's feature values. It is always better to have PCA done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    X_train= pca1.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with reduced feature columns to the number we had specified while initiating the pca object
    X_test = pca1.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pca1,'model_files/'+'pca_lr.pkl') # Save the pkl file for later use and to apply on full-data
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pca1,'model_files/'+'pca_common.pkl') # Save the pkl file for later use and to apply on full-data
    dxr=pd.DataFrame(X_train, columns=[['pca-one','pca-two','pca-three']],index=x_train.index) # Save the PCA computed values on training dataset to a new dataframe with three columns as specified while initiating PCA object
    dxt=pd.DataFrame(X_test, columns=[['pca-one','pca-two','pca-three']],index=x_test.index)   # Save the PCA computed values on validation dataset to a new dataframe with three columns as specified while initiating PCA object      
    sub=pd.concat([dxr,dxt],axis=0) # Append those two separate dataframes back into a single dataframe
    sub.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    sub.to_csv('model_files/'+'pca_x.csv',index=False) # Save the dataframe as csv for later use
    subb=pd.read_csv('model_files/'+'pca_x.csv') # Just read again to make sure it's imported correctly
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        subb=pd.concat([subb,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
        return subb # Return the updated dataset if categorical columns were present
    else:
        return subb # Return the updated dataset if categorical columns were not present - when categorical columns are not present this function would not even run - but still let this line be there

def pls_reduction_cluster(temp,y,type): # Not functional - might change later
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in temp.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for PCA  
        pcl_tr=temp[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for PCA feature engineering, so we have to remove them before performing PCA engineering and so save them to a temporary dataframe to merge them later
        temp.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    pls2=PLSRegression(n_components=3,scale=False)
    x_train,x_test,y_train,y_test=tts(temp,y,0.2)
    X_train= pls2.fit_transform(x_train,y_train)
    X_test = pls2.transform(x_test,y_train)
    if(type=="lr"):
        joblib.dump(pls2,'model_files/'+'pls_lr.pkl')
    if(type=="nn"):
        joblib.dump(pls2,'model_files/'+'pls_nn.pkl')
    if(type=="common"):
        joblib.dump(pls2,'model_files/'+'pls2_common.pkl')
    dxr=pd.DataFrame(X_train, columns=[['pls-one','pls-two','pls-three']],index=x_train.index)
    dxt=pd.DataFrame(X_test, columns=[['pls-one','pls-two','pls-three']],index=x_test.index)       
    sub=pd.concat([dxr,dxt],axis=0)
    sub.sort_index(inplace=True)
    sub.to_csv('model_files/'+'pls_x.csv',index=False)
    subb=pd.read_csv('model_files/'+'pls_x.csv')
    if(CAT_COUNTER==1):
        subb=pd.concat([subb,pcl_tr],axis=1)
    return subb

def pls_reduction_no_cluster(temp,y): # Not functional - might change later
    pls2=PLSRegression(n_components=3,scale=False)
    x_train,x_test,y_train,y_test=tts(temp,y,0.2)
    X_train= pls2.fit_transform(x_train,y_train)
    X_test = pls2.transform(x_test,y_test)
    dxr=pd.DataFrame(X_train, columns=[['pls-one','pls-two','pls-three']],index=x_train.index)
    dxt=pd.DataFrame(X_test, columns=[['pls-one','pls-two','pls-three']],index=x_test.index)       
    sub=pd.concat([dxr,dxt],axis=0)
    sub.sort_index(inplace=True)
    sub.to_csv('model_files/'+'pls_x.csv',index=False)
    subb=pd.read_csv('model_files/'+'pls_x.csv')
    return subb

def standard_scale_no_cluster(new_data,y,type): # One of the four scaling processes done to dataset - StandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way. This is a non-cluster version
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    sc=StandardScaler() # Initiate standard scaler object
    X_train=sc.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=sc.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_nn.pkl')
    if(type=="rf"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_rf.pkl')     
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_common.pkl')        
    cols=new_data.columns # Get the column names
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)   # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset    
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def standard_scale_cluster(new_data,y,type): # One of the four scaling processes done to dataset - the same as above function but used when data has cluster column
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in new_data.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling  
        pcl_tr=new_data[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        new_data.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    sc=StandardScaler() # Initiate standard scaler object
    X_train=sc.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=sc.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_nn.pkl')
    if(type=="rf"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_rf.pkl')    
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(sc,'model_files/'+'sc_common.pkl')
    cols=new_data.columns # Get the column names
   # print(cols)
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index) # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset        
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([scaled_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def min_max_scale_cluster(new_data,y,type): # One of the four scaling processes done to dataset -  MinMaxScaler scales all the data features in the range [0, 1] or else in the range [-1, 1] if there are negative values in the dataset.The min-max scalar form of normalization uses the mean and standard deviation to box all the data into a range of values.
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in new_data.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling 
        pcl_tr=new_data[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        new_data.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    ms=MinMaxScaler() # Initiate minmax scaler object
    X_train=ms.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=ms.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(ms,'model_files/'+'ms_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(ms,'model_files/'+'ms_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(ms,'model_files/'+'ms_common.pkl')
    cols=new_data.columns # Get the column names
   # print(cols)
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)   # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset    
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([scaled_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def min_max_scale_no_cluster(new_data,y,type): # One of the four scaling processes done to dataset - the same as above function but a non-cluster column version
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    ms=MinMaxScaler() # Initiate minmax scaler object
    X_train=ms.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=ms.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(ms,'model_files/'+'ms_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(ms,'model_files/'+'ms_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(ms,'model_files/'+'ms_common.pkl')
    cols=new_data.columns # Get the column names
    #print(cols)
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)  # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset     
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def robust_scale_cluster(new_data,y,type): # One of the four scaling processes done to dataset - Robust Scaler scales features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range. Robust scaler is one of the best-suited scalers for outlier data sets
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in new_data.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling 
        pcl_tr=new_data[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        new_data.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    rs=RobustScaler() # Initiate robust scaler object
    X_train=rs.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=rs.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"):  # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(rs,'model_files/'+'rs_lr.pkl')
    if(type=="nn"):  # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(rs,'model_files/'+'rs_nn.pkl')
    if(type=="common"):  # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(rs,'model_files/'+'rs_common.pkl')
    cols=new_data.columns # Get the column names
    #print(cols)
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)    # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset   
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([scaled_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def robust_scale_no_cluster(new_data,y,type): # One of the four scaling processes done to dataset - the same as above function but a non-cluster column version
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    rs=RobustScaler() # Initiate robust scaler object
    X_train=rs.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=rs.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(rs,'model_files/'+'rs_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(rs,'model_files/'+'rs_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(rs,'model_files/'+'rs_common.pkl')
    cols=new_data.columns # Get the column names
   # print(cols)
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)   # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset    
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def power_scale_cluster(new_data,y,type): # One of the four scaling processes done to dataset - Apply a power transform featurewise to make data more Gaussian-like - by performing a Yeo-Johnson power algorithm
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in new_data.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling 
        pcl_tr=new_data[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        new_data.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    pt=PowerTransformer() # Initiate power scaler object
    X_train=pt.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=pt.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pt,'model_files/'+'pt_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pt,'model_files/'+'pt_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pt,'model_files/'+'pt_common.pkl')
    cols=new_data.columns # Get the column names
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)   # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset    
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([scaled_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def power_scale_no_cluster(new_data,y,type): # One of the four scaling processes done to dataset - the same as above function but a non-cluster column version
    print(new_data.shape)
    x_train,x_test,y_train,y_test=tts(new_data,y,0.2) # tts coz we should not consider properties of test set -avoid data leak. We split the input training data into further train and test set is just to not overfit the Scaling on the training data's feature values. It is always better to have Scaling done on major subset of the input data. In cases where no validation set is provided, this will be greatly helpful
    pt=PowerTransformer() # Initiate power scaler object
    X_train=pt.fit_transform(x_train) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with scaled feature values
    X_test=pt.transform(x_test) # We just transform on test dataset because we use the object that was already fit on train dataset and we should not fit again on test dataset - learn from train dataset and apply the trained info on training dataset and test dataset 
    if(type=="lr"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pt,'model_files/'+'pt_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pt,'model_files/'+'pt_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, save the csv in that name - will be useful later during full-data part
        joblib.dump(pt,'model_files/'+'pt_common.pkl')
    cols=new_data.columns # Get the column names
   # print(cols)
    dxr=pd.DataFrame(X_train, columns=cols,index=x_train.index) # Save the scaled feature values on training dataset to a new dataframe with same column names as the input dataset
    dxt=pd.DataFrame(X_test, columns=cols,index=x_test.index)   # Save the scaled feature values on validation dataset to a new dataframe with same column names as the input dataset    
    scaled_data=pd.concat([dxr,dxt],axis=0) # Append the two datasets into a single dataframe
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset





#### SECTION 2 : FUNCTIONS FOR TRAINING MACHINE LEARNING TECHNIQUES AND CALCULATING METRIC SCORES OF MODELS FOR TRAINING AND VALIDATION DATASETS


def train_result(model,x_train,y_train,model_name): # Inputs the model variable, train data, train target values, and the name of the model to calculate and return training metric results as a dataframe
    predictions_all=pd.DataFrame(y_train) # Save the training target values to separate dataframe called predictions_all 
    y_pred_model=model.predict(x_train) #Use model to predict values of train data
    col1=predictions_all.columns.tolist()[0] # Convert the column names present in predictions_all dataframe to a list
    predictions_all['Model_full_Pred']=y_pred_model # Now add the model's prediction on training dataset, to the predictions_all dataframe
    r2s=predictions_all[col1].corr(predictions_all['Model_full_Pred']) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse(predictions_all[col1],predictions_all['Model_full_Pred'])) #calculate mse metric
    maes=mean_absolute_error(predictions_all[col1],predictions_all['Model_full_Pred']) #calculate mae metric
    evars=explained_variance_score(predictions_all[col1],predictions_all['Model_full_Pred']) #calculate variance score metric
    maxs=max_error(predictions_all[col1],predictions_all['Model_full_Pred']) #calculate max error metric
    tmp=[] # Create an empty list
    tmp.append([model_name,rmses,maes,r2s,maxs,evars]) # Join all metric values into a list
    final_scores_train=pd.DataFrame(tmp,columns=['1. Model Name','2. Root Mean Squared Error(RMSE)','3. Mean Absolute Error(MAE)','4. Pearson Correlation Coefficient','5. Maximum Residual Error','6. Explained variance Score'])   # Convert the metric values list to a dataframe with proper column names 
    return final_scores_train # Return the training metric table

def test_result(model,x_test,y_test,model_name): # Inputs the model variable, validation data, validation target values, and the name of the model to calculate and return validation metric results as a dataframe
    predictions_all=pd.DataFrame(y_test)  # Save the validation target values to separate dataframe called predictions_all    
    y_pred_model=model.predict(x_test) #Use model to predict values of validation data
    print(list(y_pred_model))
    col1=predictions_all.columns.tolist()[0] # Convert the column names present in predictions_all dataframe to a list
    predictions_all['Model_full_Pred']=y_pred_model # Now add the model's prediction on validation dataset, to the predictions_all dataframe
    r2s=predictions_all[col1].corr(predictions_all['Model_full_Pred']) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse(predictions_all[col1],predictions_all['Model_full_Pred'])) #calculate mse metric
    maes=mean_absolute_error(predictions_all[col1],predictions_all['Model_full_Pred']) #calculate mae metric
    evars=explained_variance_score(predictions_all[col1],predictions_all['Model_full_Pred']) #calculate variance score metric
    maxs=max_error(predictions_all[col1],predictions_all['Model_full_Pred']) #calculate max error metric
    tmp=[] # Create an empty list
    tmp.append([model_name,rmses,maes,r2s,maxs,evars]) # Join all metric values into a list
    final_scores_test=pd.DataFrame(tmp,columns=['1. Model Name','2. Validation_Root Mean Squared Error(RMSE)','3. Validation_Mean Absolute Error(MAE)','4. Validation_Pearson Correlation Coefficient','5. Validation_Maximum Residual Error','6. Validation_Explained variance Score'])    # Convert the metric values list to a dataframe with proper column names 
    return final_scores_test # Return the validation metric table



def LR_reg(totrain,x_train,x_test,y_train,y_test,predictions_all): # This is the main part - Technique 1 - input for this functions are the whole input data, train data, validation data, train target variable, validation target variable and an empty dataframe into which we append the predictions of whole input data after training linear regression on the dataset
    lr=LinearRegression() # Initiate linear reg object
    lr.fit(x_train,y_train) # Train lr on training data
    #predictions_all=pd.DataFrame(y_train)
    y_pred_xgb_all=lr.predict(totrain) # Predict input data (whole input data)

    #Just training preds
    df = pd.DataFrame(columns = ['Technique1_train_Pred'] ) # Create a new dataframe with Technique1_train_Pred column name
    df['Technique1_train_Pred'] = lr.predict(x_train) # Predict on training data with lr and add the values to the newly created dataframe under the Technique1_train_Pred column name 
    df.to_csv('model_files/Technique1_Training_preds.csv',index=False) # Save the training predictions as csv - use this to export the training and validation csv

    predictions_all['Technique1_full_Pred']=y_pred_xgb_all # Add lr predictions of whole input data into a dataframe under column name Technique1_full_Pred
    final_scores_test=train_result(lr,x_train,y_train,'Technique 1') # Calculate training metrics
    final_test=test_result(lr,x_test,y_test,'Technique 1') # Calculate validation metrics
    return lr,predictions_all,final_scores_test,final_test # Return model object, full input-data predictions, training metric dataframe and validation metric dataframe
             
def XGB_reg(totrain,x_train,x_test,y_train,y_test,predictions_all): # This is the main part - Technique 3 - input for this functions are the whole input data, train data, validation data, train target variable, validation target variable and an empty dataframe into which we append the predictions of whole input data after training xgboost regression on the dataset
    eval_set = [(x_test, y_test)] # This line is only for xgboost - we perform grid search just for xgboost so as to find the proper parameters for the algorithm that will not overfit on training data and also giving good results
    # A parameter grid for XGBoost
    params = {
            'min_child_weight': [4,6,9],
            'subsample': [0.7,0.5],
            'max_depth': [5,8],
            'learning_rate':[0.1 , 0.03]
            
            }
    
    # magic with cv
    from sklearn.model_selection import GridSearchCV 
    num_of_cores=os.cpu_count()
    xgb = GridSearchCV(XGBRegressor(n_estimators=500,early_stopping_rounds=10, eval_metric='mae', eval_set=eval_set,random_state=10), params, refit = True, verbose = 3, cv=3)  # Initiate a grid search and configure the xgb object with the best parameters for the algorithm
    xgb.fit(x_train,y_train)    # Train the configured xgb model object on training data
    y_pred_xgb_all=xgb.predict(totrain) # Predict on full input data with xgb

    #Just training preds
    df = pd.DataFrame(columns = ['Technique3_train_Pred'] ) # Create a new dataframe with Technique3_train_Pred column name
    df['Technique3_train_Pred'] = xgb.predict(x_train) # Predict on training data with xgb and add the values to the newly created dataframe under the Technique3_train_Pred column name 
    df.to_csv('model_files/Technique3_Training_preds.csv',index=False) # Save the training predictions as csv - use this to export the training and validation csv

    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique3_full_Pred']=y_pred_xgb_all # Add xgb predictions of whole input data into a dataframe under column name Technique3_full_Pred
    final_scores_test=train_result(xgb,x_train,y_train,'Technique 3') # Calculate train metrics
    final_test=test_result(xgb,x_test,y_test,'Technique 3')  # Calculate validation metrics
 #   print(xgb.get_params())
    return xgb,predictions_all,final_scores_test,final_test # Return model object, full input-data predictions, training metric dataframe and validation metric dataframe

def RF_reg(totrain,x_train,x_test,y_train,y_test,predictions_all): # This is the main part - Technique 2 - input for this functions are the whole input data, train data, validation data, train target variable, validation target variable and an empty dataframe into which we append the predictions of whole input data after training random forest regression on the dataset
    rf=RandomForestRegressor(random_state=10) # Initiate random forest algorithm object - this random state makes sure the weights or initial weights remain constant
    rf.fit(x_train,y_train) # Train rf on training data
    y_pred_xgb_all=rf.predict(totrain) # Predict input data (whole input data)

    #Just training preds
    df = pd.DataFrame(columns = ['Technique2_train_Pred'] ) # Create a new dataframe with Technique2_train_Pred column name
    df['Technique2_train_Pred'] = rf.predict(x_train) # Predict on training data with rf and add the values to the newly created dataframe under the Technique2_train_Pred column name 
    df.to_csv('model_files/Technique2_Training_preds.csv',index=False) # Save the training predictions as csv - use this to export the training and validation csv

    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique2_full_Pred']=y_pred_xgb_all # Add rf predictions of whole input data into a dataframe under column name Technique2_full_Pred
    final_scores_test=train_result(rf,x_train,y_train,'Technique 2') # Calculate training metrics
    final_test=test_result(rf,x_test,y_test,'Technique 2') # Calculate validation metrics
    return rf,predictions_all,final_scores_test,final_test # Return model object, full input-data predictions, training metric dataframe and validation metric dataframe
                    

def NN_reg(totrain,x_train,x_test,y_train,y_test,predictions_all): # This is the main part - Technique 4 - input for this functions are the whole input data, train data, validation data, train target variable, validation target variable and an empty dataframe into which we append the predictions of whole input data after training neural network regression on the dataset
    ann=MLPRegressor(hidden_layer_sizes=(200,),activation='relu', solver='adam', alpha=0.001, batch_size='auto',learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True, random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # Initiate neural network object with different parametrs to best fit on the dataset - Tip for improvement : Try changing these values and implement a full proper neural network algorithm
    ann.fit(x_train,y_train) # Train nn on training data
    # sc_t4=joblib.load('model_files/'+'sc_forT4only.pkl')
    # totrain=sc_t4.transform(totrain)
    y_pred_xgb_all=ann.predict(totrain) # Predict input data (whole input data)

    #Just training preds
    df = pd.DataFrame(columns = ['Technique4_train_Pred'] ) # Create a new dataframe with Technique4_train_Pred column name
    df['Technique4_train_Pred'] = ann.predict(x_train) # Predict on training data with nn and add the values to the newly created dataframe under the Technique4_train_Pred column name 
    df.to_csv('model_files/Technique4_Training_preds.csv',index=False) # Save the training predictions as csv - use this to export the training and validation csv
    
    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique4_full_Pred']=y_pred_xgb_all # Add nn predictions of whole input data into a dataframe under column name Technique4_full_Pred
    final_scores_test=train_result(ann,x_train,y_train,'Technique 4') # Calculate training metrics
    final_test=test_result(ann,x_test,y_test,'Technique 4') # Calculate validation metrics
    
    return ann,predictions_all,final_scores_test,final_test # Return model object, full input-data predictions, training metric dataframe and validation metric dataframe
                                      
def SVR_reg(totrain,x_train,x_test,y_train,y_test,predictions_all): # This is the main part - Technique 5 - input for this functions are the whole input data, train data, validation data, train target variable, validation target variable and an empty dataframe into which we append the predictions of whole input data after training svr regression on the dataset
    svr=SVR(kernel='rbf',C=10) # Initiate svr algorithm object. Tip for improvement: Try grid search to get best parameters and try many other parameters
    #grid.fit(x_train,y_train)   
    #from sklearn.model_selection import GridSearchCV 
  
    # defining parameter range 
    #param_grid = {'C': [0.1, 1, 10, 100],  
    #              'gamma': [1, 0.1, 0.01, 0.001], 
    #              'kernel': ['rbf']}  
      
     
    # fitting the model for grid search 
    svr.fit(x_train, y_train) # Train svr on training data
    y_pred_xgb_all=svr.predict(totrain) # Predict input data (whole input data)

    #Just training preds
    df = pd.DataFrame(columns = ['Technique5_train_Pred'] ) # Create a new dataframe with Technique5_train_Pred column name
    df['Technique5_train_Pred'] = svr.predict(x_train) # Predict on training data with svr and add the values to the newly created dataframe under the Technique5_train_Pred column name 
    df.to_csv('model_files/Technique5_Training_preds.csv',index=False) # Save the training predictions as csv - use this to export the training and validation csv

    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique5_full_Pred']=y_pred_xgb_all # Add svr predictions of whole input data into a dataframe under column name Technique5_full_Pred
    final_scores_test=train_result(svr,x_train,y_train,'Technique 5') # Calculate training metrics
    final_test=test_result(svr,x_test,y_test,'Technique 5') # Calculate validation metrics
 
    return svr,predictions_all,final_scores_test,final_test # Return model object, full input-data predictions, training metric dataframe and validation metric dataframe
               
def All_reg(sub_x,nn_x,rf_x,x_train,x_test,y_train,y_test,x_train2,x_test2,y_train2,y_test2,x_train3,x_test3,y_train3,y_test3,predictions_all): # This function is when the user chooses to run all techniques at once - then run all the 5 techniques separately one by one, and merge all results, predictions into a single dataframe 

    lr,predictions_all1,fts1,vts1=LR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Calling separate ml techniques - this line calls technique 1 (lr) and gets the output values
    rf,predictions_all2,fts2,vts2=RF_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all1) # Calling separate ml techniques - this line calls technique 2 (rf) and gets the output values
    xgb,predictions_all3,fts3,vts3=XGB_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all2) # Calling separate ml techniques - this line calls technique 3 (xgb) and gets the output values
    dt,predictions_all4,fts4,vts4=NN_reg(nn_x,x_train3,x_test3,y_train3,y_test3,predictions_all3) # Calling separate ml techniques - this line calls technique 4 (nn) and gets the output values
    svr,predictions_all5,fts5,vts5=SVR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all4) # Calling separate ml techniques - this line calls technique 5 (svr) and gets the output values
    
    vtss=[vts1,vts2,vts3,vts4,vts5] # Store all the validation scores dataframe of all 5 techinques in a list
    valid_results=pd.concat(vtss,axis=0) # Merge validation scores of all 5 techniques into a new dataframe

    ftss=[fts1,fts2,fts3,fts4,fts5] # Store all the training scores dataframe of all 5 techinques in a list
    final_scores_test=pd.concat(ftss,axis=0) #  Merge training scores of all 5 techniques into a new dataframe

   # print("final train score",final_scores_test)
   # print("final test score",valid_results)
    
    dfs=[predictions_all1,predictions_all2,predictions_all3,predictions_all4,predictions_all5]  # This line is non-functional
    all_preds=pd.DataFrame(predictions_all1[predictions_all1.columns[0]]) # Create a new dataframe with column name same as the actual target variable name - this was stored first in the linear regression techique's full-input data prediction dataframe before appending all technique's prediction columns
    all_preds['Technique1_full_Pred']=predictions_all1['Technique1_full_Pred'] # Add technique 1's full-input data predictions into the newly created dataframe
    all_preds['Technique3_full_Pred']=predictions_all3['Technique3_full_Pred'] # Add technique 3's full-input data predictions into the newly created dataframe
    all_preds['Technique2_full_Pred']=predictions_all2['Technique2_full_Pred'] # Add technique 2's full-input data predictions into the newly created dataframe
    all_preds['Technique4_full_Pred']=predictions_all4['Technique4_full_Pred'] # Add technique 4's full-input data predictions into the newly created dataframe
    all_preds['Technique5_full_Pred']=predictions_all5['Technique5_full_Pred'] # Add technique 5's full-input data predictions into the newly created dataframe
    #validation results:
    return lr,dt,rf,xgb,svr,all_preds,final_scores_test,valid_results # Return all the 5 models, full-input data predictions of all 5 techniques merged in a single dataframe, merged training scores and merged validation scores of all 5 techniques


#### No vali 
# Below is the same 5 functions as above - only difference is that the below ones run when the user chooses not to have any validation split. All steps are same except for predicting and merging validation data

def LR_reg_no_vali(totrain,x_train,y_train,predictions_all):
    lr=LinearRegression() # Initiate linear reg object
    lr.fit(x_train,y_train) # Train lr on training data
    #predictions_all=pd.DataFrame(y_train)
    y_pred_xgb_all=lr.predict(totrain) # Predict input data (whole input data)
    predictions_all['Technique1_full_Pred']=y_pred_xgb_all  # Add lr predictions of whole input data into a dataframe under column name Technique1_full_Pred
    final_scores_test=train_result(lr,x_train,y_train,'Technique 1') # Calculate training metrics
    return lr,predictions_all,final_scores_test # Return model object, full input-data predictions and training metric dataframe
             
def XGB_reg_no_vali(totrain,x_train,x_test,y_train,y_test,predictions_all): # Check here, there is an extra parameter in this function x_test and y_test - this is to use this as evaluation set for the grid search for xgb algorithm
    eval_set = [(x_test, y_test)] # This line is only for xgboost - we perform grid search just for xgboost so as to find the proper parameters for the algorithm that will not overfit on training data and also giving good results
    # A parameter grid for XGBoost
    params = {
            'min_child_weight': [4,6,9],
            'subsample': [0.7,0.5],
            'max_depth': [5,8],
            'learning_rate':[0.1 , 0.03]
            
            }
    
    # magic with cv
    from sklearn.model_selection import GridSearchCV 
    num_of_cores=os.cpu_count()
    xgb = GridSearchCV(XGBRegressor(n_estimators=500,early_stopping_rounds=10, eval_metric='mae', eval_set=eval_set,random_state=10), params, refit = True, verbose = 3, cv=3) # Initiate a grid search and configure the xgb object with the best parameters for the algorithm
    xgb.fit(x_train,y_train)  # Train the configured xgb model object on training data   
    y_pred_xgb_all=xgb.predict(totrain) # Predict on full input data with xgb
    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique3_full_Pred']=y_pred_xgb_all # Add xgb predictions of whole input data into a dataframe under column name Technique3_full_Pred
    final_scores_test=train_result(xgb,x_train,y_train,'Technique 3') # Calculate train metrics
 #   print(xgb.get_params())
    return xgb,predictions_all,final_scores_test # Return model object, full input-data predictions and training metric dataframe

def RF_reg_no_vali(totrain,x_train,y_train,predictions_all):
    rf=RandomForestRegressor(random_state=10) # Initiate random forest algorithm object - this random state makes sure the weights or initial weights remain constant
    rf.fit(x_train,y_train) # Train rf on training data
    y_pred_xgb_all=rf.predict(totrain) # Predict input data (whole input data)
    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique2_full_Pred']=y_pred_xgb_all # Add rf predictions of whole input data into a dataframe under column name Technique2_full_Pred
    final_scores_test=train_result(rf,x_train,y_train,'Technique 2') # Calculate train metrics
    return rf,predictions_all,final_scores_test # Return model object, full input-data predictions and training metric dataframe
                    

def NN_reg_no_vali(totrain,x_train,y_train,predictions_all):
    ann=MLPRegressor(hidden_layer_sizes=(200,),activation='relu', solver='adam', alpha=0.001, batch_size='auto',learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True, random_state=9, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,early_stopping=True, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # Initiate neural network object with different parametrs to best fit on the dataset - Tip for improvement : Try changing these values and implement a full proper neural network algorithm
    ann.fit(x_train,y_train) # Train nn on training data
    # sc_t4=joblib.load('model_files/'+'sc_forT4only.pkl')
    # totrain=sc_t4.transform(totrain)
    y_pred_xgb_all=ann.predict(totrain) # Predict input data (whole input data)
    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique4_full_Pred']=y_pred_xgb_all # Add nn predictions of whole input data into a dataframe under column name Technique4_full_Pred
    final_scores_test=train_result(ann,x_train,y_train,'Technique 4') # Calculate training metrics
    
    return ann,predictions_all,final_scores_test # Return model object, full input-data predictions and training metric dataframe
                                      
def SVR_reg_no_vali(totrain,x_train,y_train,predictions_all):
    svr=SVR(kernel='rbf',C=10) # Initiate svr algorithm object. Tip for improvement: Try grid search to get best parameters and try many other parameters
    #grid.fit(x_train,y_train)   
    #from sklearn.model_selection import GridSearchCV 
  
    # defining parameter range 
    #param_grid = {'C': [0.1, 1, 10, 100],  
    #              'gamma': [1, 0.1, 0.01, 0.001], 
    #              'kernel': ['rbf']}  
      
     
    # fitting the model for grid search 
    svr.fit(x_train, y_train)  # Train svr on training data
    y_pred_xgb_all=svr.predict(totrain) # Predict input data (whole input data)
    #predictions_all=pd.DataFrame(y_train)
    predictions_all['Technique5_full_Pred']=y_pred_xgb_all # Add svr predictions of whole input data into a dataframe under column name Technique5_full_Pred
    final_scores_test=train_result(svr,x_train,y_train,'Technique 5') # Calculate training metrics
 
    return svr,predictions_all,final_scores_test # Return model object, full input-data predictions, training metric dataframe and validation metric dataframe
               
def All_reg_no_vali(sub_x,nn_x,rf_x,x_train,x_test,y_train,y_test,x_train2,y_train2,x_train3,y_train3,predictions_all):

    lr,predictions_all1,fts1=LR_reg_no_vali(sub_x,x_train2,y_train2,predictions_all) # Calling separate ml techniques - this line calls technique 1 (lr) and gets the output values
    rf,predictions_all2,fts2=RF_reg_no_vali(rf_x,x_train,y_train,predictions_all1) # Calling separate ml techniques - this line calls technique 2 (rf) and gets the output values
    xgb,predictions_all3,fts3=XGB_reg_no_vali(rf_x,x_train,x_test,y_train,y_test,predictions_all2) # Calling separate ml techniques - this line calls technique 3 (xgb) and gets the output values. Check here, there is an extra parameter in this function x_test and y_test - this is to use this as evaluation set for the grid search for xgb algorithm
    dt,predictions_all4,fts4=NN_reg_no_vali(nn_x,x_train3,y_train3,predictions_all3) # Calling separate ml techniques - this line calls technique 4 (nn) and gets the output values
    svr,predictions_all5,fts5=SVR_reg_no_vali(sub_x,x_train2,y_train2,predictions_all4) # Calling separate ml techniques - this line calls technique 5 (svr) and gets the output values

    ftss=[fts1,fts2,fts3,fts4,fts5] # Store all the training scores dataframe of all 5 techinques in a list
    final_scores_test=pd.concat(ftss,axis=0) #  Merge training scores of all 5 techniques into a new dataframe
   # print("final train score",final_scores_test)
   # print("final test score",valid_results)
    
    dfs=[predictions_all1,predictions_all2,predictions_all3,predictions_all4,predictions_all5]
    all_preds=pd.DataFrame(predictions_all1[predictions_all1.columns[0]]) # Create a new dataframe with column name same as the actual target variable name - this was stored first in the linear regression techique's full-input data prediction dataframe before appending all technique's prediction columns
    all_preds['Technique1_full_Pred']=predictions_all1['Technique1_full_Pred'] # Add technique 1's full-input data predictions into the newly created dataframe
    all_preds['Technique3_full_Pred']=predictions_all3['Technique3_full_Pred'] # Add technique 3's full-input data predictions into the newly created dataframe
    all_preds['Technique2_full_Pred']=predictions_all2['Technique2_full_Pred'] # Add technique 2's full-input data predictions into the newly created dataframe
    all_preds['Technique4_full_Pred']=predictions_all4['Technique4_full_Pred'] # Add technique 4's full-input data predictions into the newly created dataframe
    all_preds['Technique5_full_Pred']=predictions_all5['Technique5_full_Pred'] # Add technique 5's full-input data predictions into the newly created dataframe
    #validation results:
    return lr,dt,rf,xgb,svr,all_preds,final_scores_test # Return all the 5 models, full-input data predictions of all 5 techniques merged in a single dataframe and merged training scores of all 5 techniques

def get_download_path(): # Not functional anymore
    """Returns the default downloads path for linux or windows"""
    if os.name == 'nt':
        import winreg
        sub_key = r'SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders'
        downloads_guid = '{374DE290-123F-4565-9164-39C4925E467B}'
        with winreg.OpenKey(winreg.HKEY_CURRENT_USER, sub_key) as key:
            location = winreg.QueryValueEx(key, downloads_guid)[0]
        return location
    else:
        return os.path.join(os.path.expanduser('~'), 'Downloads')


def calculate_metrics(model_name ,predictions_all,sel_tar, col_name): # This function is to calculate metrics for the full-data predictions if the users already has a target variable with the full-data input. So if the target variable is present in the full-data predictions part, then the metrics are calculated by comparing the already present target values to the predictions made by the software
    new_col_name=str(col_name)+'_Pred' # Get the technique's predicted values column name
    r2s=predictions_all[sel_tar].corr(predictions_all[new_col_name]) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse(predictions_all[sel_tar],predictions_all[new_col_name])) # Calculate mse metric score of full-data predicted values against the full-data actual target values
    maes=mean_absolute_error(predictions_all[sel_tar],predictions_all[new_col_name]) # Calculate mae metric score of full-data predicted values against the full-data actual target values
    evars=explained_variance_score(predictions_all[sel_tar],predictions_all[new_col_name]) # Calculate variance metric score of full-data predicted values against the full-data actual target values
    maxs=max_error(predictions_all[sel_tar],predictions_all[new_col_name]) # Calculate max error metric score of full-data predicted values against the full-data actual target values
    tmp=[] # Create an empty list
    tmp.append([model_name,rmses,maes,r2s,maxs,evars]) # Join all metric values into a list

    full_river_preds_metrics=pd.DataFrame(tmp,columns=['1. Model Name','2. Root Mean Squared Error(RMSE)','3. Mean Absolute Error(MAE)','4. Pearson Correlation Coefficient','5. Maximum Residual Error','6. Explained variance Score'])    # Convert the metric values list to a dataframe with proper column names 
    return full_river_preds_metrics # Return the full-data prediction's metric table

def calculate_metrics_temp(predictions_all, sel_tar, col_name): # Used only twice -  Peforms same function as above, but its just a temp function - sometimes there will be problems while sending multiple lines data from python to javascript using ajax, so we use this temp function to add twice the number of rows of the dataframe and send it back - and in javascript it will remove the extra rows and take only correct rows and values 
    new_col_name=str(col_name)+'_Pred'
    r2s=predictions_all[sel_tar].corr(predictions_all[new_col_name]) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse(predictions_all[sel_tar],predictions_all[new_col_name]))
    maes=mean_absolute_error(predictions_all[sel_tar],predictions_all[new_col_name])
    evars=explained_variance_score(predictions_all[sel_tar],predictions_all[new_col_name])
    maxs=max_error(predictions_all[sel_tar],predictions_all[new_col_name])
    tmp=[]
    model_name="This_is_just_temp_variable"
    tmp.append([model_name,rmses,maes,r2s,maxs,evars])

    full_river_preds_metrics_temp=pd.DataFrame(tmp,columns=['1. Model Name_temp','2. Root Mean Squared Error(RMSE)_temp','3. Mean Absolute Error(MAE)_temp','4. Pearson Correlation Coefficient_temp','5. Maximum Residual Error_temp','6. Explained variance Score_temp'])    
    return full_river_preds_metrics_temp

## Just a separate function to process all the feature engineering functions at the same time - this function is used in a mulit-loop while performing feature engineering process with different combinations of feature engineering methods to find the best set of feature engineering process combination

def processing_options(processed_data,process_options,r,g,b,y,colourspaces): # This function is used to process the data with each option - main inputs to this function are the dataset, options to process ( function is called for every unique combination of processes)
    if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
        processed_data=logperm_gen(processed_data,r,g,b, colourspaces)

    if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
        processed_data=greyscale_gen(processed_data,r,g,b)

    if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
        processed_data=cluster_data(processed_data,r,g,b)

    if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=poly_creation_cluster(processed_data,y)

    if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=poly_creation_no_cluster(processed_data,y)

    #SCALING PART

    if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=standard_scale_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=standard_scale_no_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=min_max_scale_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=min_max_scale_no_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=robust_scale_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=robust_scale_no_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=power_scale_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=power_scale_no_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    #FEAT REDUCTION PART
    if(("pearson_correl" in process_options) & ("cluster_gen" in process_options)): # This line checks if pearson_correl and cluster_gen is in the process_options list and if it's true, then the correl_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=correl_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if(("pearson_correl" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and if it's true, then the correl_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=correl_no_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
        processed_data=lasso_reg_bestop(processed_data,y)

    if(("pca_trans" in process_options) & ("cluster_gen" in process_options) & (processed_data.shape[1]>=3)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
        processed_data=pca_reduction_cluster(processed_data,y,"common")   # Common specifies same data for all ml techniques  

    if(("pca_trans" in process_options) & ("cluster_gen" not in process_options) & (processed_data.shape[1]>=3)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
        processed_data=pca_reduction_no_cluster(processed_data,y,"common") # Common specifies same data for all ml techniques

    return processed_data    # Return the updated dataset



#### SECTION 3 : FUNCTIONS TO APPLY FEATURE ENGINEERING AND DATA PRE-PROCESSING TO FULL DATA (FINAL PREDICTIONS)

#############################################  FULL RIVER FUNCTIONS  ###############################################################
 
def logperm_gen_full(hutt_full,r,g,b, feats): #Create log permutations for the dataset - for now it creates log of b/r and b/g, these two alone have proved to work effectively. Tip for improvement: try all the other combinations like log(r/g), log(r/b) etc, I did not include them as they increase the computation time
    if( ('hsv' not in feats and 'rgb' in feats) or ('hsv' not in feats and 'rgb' not in feats)): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
        lnbr=[] # Initiate an empty list for ln(b/r) 
        lnbg=[] # Initiate an empty list for ln(b/g) 
        
            # check if 0 value is present in r,g,b - If present in any of r,g,b change the zero value to 0.1 - this will avoid logarthmic division by zero error, and also if there are zero values for r,g,b it is a wrong data and so change it to 0.1
        if(hutt_full.describe()[r]['min']==0): # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator
            print('0 present')                  
            hutt_full[r]=np.where(hutt_full[r]==0,0.1,hutt_full[r])
        if(hutt_full.describe()[g]['min']==0): # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            hutt_full[g]=np.where(hutt_full[g]==0,0.1,hutt_full[g])
        if(hutt_full.describe()[b]['min']==0): # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            hutt_full[b]=np.where(hutt_full[b]==0,0.1,hutt_full[b])
        
        if(hutt_full.describe()[r]['min']<0): # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator
            print('0 present')                  
            hutt_full[r]=np.where(hutt_full[r]<0,0.1,hutt_full[r])
        if(hutt_full.describe()[g]['min']<0): # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                  
            hutt_full[g]=np.where(hutt_full[g]<0,0.1,hutt_full[g])
        if(hutt_full.describe()[b]['min']<0): # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                  
            hutt_full[b]=np.where(hutt_full[b]<0,0.1,hutt_full[b])

        #hut full has r,g,b,w only
        r_in=hutt_full.columns.get_loc(r) #Get column index of r
        g_in=hutt_full.columns.get_loc(g) #Get column index of g
        b_in=hutt_full.columns.get_loc(b) #Get column index of b
        te=hutt_full.values # Get the numpy array values of the dataset 
        for i in range(len(te)): # For all rows of data, calculate the logarthmic combination of - log(b/r) and log(b/g). You can improve this by adding other combinations here and in the input data processing
            val=np.log10(te[i,b_in]/te[i,r_in]) # Quickly calculate log(b/r) for all rows of the dataset - 'i' indicates the row and b_in is B and r_in is R
            lnbr.append(val)  # Append the value to the lnbr list
            val1=np.log10(te[i,b_in]/te[i,g_in]) # Quickly calculate log(b/r) for all rows of the dataset - 'i' indicates the row and b_in is B and g_in is G
            lnbg.append(val1) # Append the value to the lnbg list
                
        hutt_full['ln(b/r)']=np.array(lnbr) # Add the ln(b/r) column with numpy values of the list, into the existing dataframe
        hutt_full['ln(b/g)']=np.array(lnbg) # Add the ln(b/g) column with numpy values of the list, into the existing dataframe
    
    if( 'hsv' in feats and 'rgb' not in feats): #If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
        
        lnvh=[] # Initiate an empty list for ln(v/h) 
        lnvs=[] # Initiate an empty list for ln(v/s) 
        
            # check if 0 value is present in r,g,b
        if(hutt_full.describe()['H_generated']['min']==0):  # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator
            print('0 present')                  
            hutt_full['H_generated']=np.where(hutt_full['H_generated']==0,0.1,hutt_full['H_generated'])
        if(hutt_full.describe()['S_generated']['min']==0):  # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                 
            hutt_full['S_generated']=np.where(hutt_full['S_generated']==0,0.1,hutt_full['S_generated'])
        if(hutt_full.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                
            hutt_full['V_generated']=np.where(hutt_full['V_generated']==0,0.1,hutt_full['V_generated'])
        
        #hut full has r,g,b,w only
        h_in=hutt_full.columns.get_loc('H_generated') #Get column index of h
        s_in=hutt_full.columns.get_loc('S_generated') #Get column index of s
        v_in=hutt_full.columns.get_loc('V_generated') #Get column index of v
        te=hutt_full.values # Get the numpy array values of the dataset 
        for i in range(len(te)): # For all rows of data, calculate the logarthmic combination of - log(v/h) and log(v/s).
            val=np.log10(te[i,v_in]/te[i,h_in]) # Quickly calculate log(v/h) for all rows of the dataset - 'i' indicates the row and v_in is V and h_in is H
            lnvh.append(val)   # Append the value to the lnvh list   
            val1=np.log10(te[i,v_in]/te[i,s_in]) # Quickly calculate log(v/s) for all rows of the dataset - 'i' indicates the row and v_in is V and s_in is S
            lnvs.append(val1)  # Append the value to the lnvs list
                
        hutt_full['ln(v/h)']=np.array(lnvh) # Add the ln(v/h) column with numpy values of the list, into the existing dataframe
        hutt_full['ln(v/s)']=np.array(lnvs) # Add the ln(v/s) column with numpy values of the list, into the existing dataframe
    
    if( 'hsv' in feats and 'rgb' in feats): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
        
        lnbr=[] # Initiate an empty list for ln(b/r) 
        lnbg=[] # Initiate an empty list for ln(b/g) 
        
            # check if 0 value is present in r,g,b
        if(hutt_full.describe()[r]['min']==0): # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator
            print('0 present')                  
            hutt_full[r]=np.where(hutt_full[r]==0,0.1,hutt_full[r])
        if(hutt_full.describe()[g]['min']==0): # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            hutt_full[g]=np.where(hutt_full[g]==0,0.1,hutt_full[g])
        if(hutt_full.describe()[b]['min']==0): # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            hutt_full[b]=np.where(hutt_full[b]==0,0.1,hutt_full[b])
        
        if(hutt_full.describe()[r]['min']<0): # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator
            print('0 present')                  
            hutt_full[r]=np.where(hutt_full[r]<0,0.1,hutt_full[r])
        if(hutt_full.describe()[g]['min']<0): # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                  
            hutt_full[g]=np.where(hutt_full[g]<0,0.1,hutt_full[g])
        if(hutt_full.describe()[b]['min']<0): # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                  
            hutt_full[b]=np.where(hutt_full[b]<0,0.1,hutt_full[b])

            # check if 0 value is present in h,s,v
        if(hutt_full.describe()['H_generated']['min']==0): # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator
            print('0 present')                  
            hutt_full['H_generated']=np.where(hutt_full['H_generated']==0,0.1,hutt_full['H_generated'])
        if(hutt_full.describe()['S_generated']['min']==0): # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            hutt_full['S_generated']=np.where(hutt_full['S_generated']==0,0.1,hutt_full['S_generated'])
        if(hutt_full.describe()['V_generated']['min']==0): # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            hutt_full['V_generated']=np.where(hutt_full['V_generated']==0,0.1,hutt_full['V_generated'])
        

        #hut full has r,g,b,w only
        r_in=hutt_full.columns.get_loc(r) #Get column index of r
        g_in=hutt_full.columns.get_loc(g) #Get column index of g
        b_in=hutt_full.columns.get_loc(b) #Get column index of b
        te=hutt_full.values # Get the numpy array values of the dataset 
        for i in range(len(te)): # For all rows of data, calculate the logarthmic combination of - log(b/r) and log(b/g). You can improve this by adding other combinations here and in the input data processing
            val=np.log10(te[i,b_in]/te[i,r_in]) # Quickly calculate log(b/r) for all rows of the dataset - 'i' indicates the row and b_in is B and r_in is R
            lnbr.append(val)    # Append the value to the lnbr list  
            val1=np.log10(te[i,b_in]/te[i,g_in]) # Quickly calculate log(b/r) for all rows of the dataset - 'i' indicates the row and b_in is B and g_in is G
            lnbg.append(val1)   # Append the value to the lnbg list
                
        hutt_full['ln(b/r)']=np.array(lnbr) # Add the ln(b/r) column with numpy values of the list, into the existing dataframe
        hutt_full['ln(b/g)']=np.array(lnbg) # Add the ln(b/g) column with numpy values of the list, into the existing dataframe


        lnvh=[] # Initiate an empty list for ln(v/h) 
        lnvs=[] # Initiate an empty list for ln(v/s) 
        te = 0
        
        #hut full has r,g,b,w only
        h_in=hutt_full.columns.get_loc('H_generated') #Get column index of h
        s_in=hutt_full.columns.get_loc('S_generated') #Get column index of s
        v_in=hutt_full.columns.get_loc('V_generated') #Get column index of v
        te=hutt_full.values # Get the numpy array values of the dataset 
        for i in range(len(te)): # For all rows of data, calculate the logarthmic combination of - log(v/h) and log(v/s).
            val=np.log10(te[i,v_in]/te[i,h_in]) # Quickly calculate log(v/h) for all rows of the dataset - 'i' indicates the row and v_in is V and h_in is H
            lnvh.append(val)  # Append the value to the lnvh list     
            val1=np.log10(te[i,v_in]/te[i,s_in]) # Quickly calculate log(v/s) for all rows of the dataset - 'i' indicates the row and v_in is V and s_in is S
            lnvs.append(val1) # Append the value to the lnvs list 
                
        hutt_full['ln(v/h)']=np.array(lnvh) # Add the ln(v/h) column with numpy values of the list, into the existing dataframe
        hutt_full['ln(v/s)']=np.array(lnvs) # Add the ln(v/s) column with numpy values of the list, into the existing dataframe
    
    return hutt_full # Return the updated dataset

# All functions below upto the next section are the same functions used for input data processing - only changes made are instead of computing 
# some calculations and running some algorithms again, we import trained model files or pre-calculated values during input data processing, which were store as csv or pkl files and use these values to process full-data

def greyscale_gen_full(hutt_full,r,g,b):
    tes=np.array(hutt_full) # Get the numpy array values of the dataset 
    r_in=hutt_full.columns.get_loc(r) #Get column index of r
    g_in=hutt_full.columns.get_loc(g) #Get column index of g
    b_in=hutt_full.columns.get_loc(b) #Get column index of b
    i=0
    grs=[] # Initialize and empty list
    for i in range(len(tes)): # For every row in the dataset, do this loop
        combo=0.3*tes[i,r_in]+ 0.59*tes[i,g_in]+ 0.11*tes[i,b_in] # Calculate greyscale values for the r,g,b values or h,s,v values for the Ith row.
        grs.append(combo) # Append the greyscale value of every row to this list
    
    hutt_full['Greyscale']=np.array(grs) # Add the numpy values of the grs list, into the existing dataframe under the Greyscale column
    return hutt_full  # Return the updated dataset

def cluster_data_full(hutt_full,r,g,b): # Cluster data into high, medium, low based on colour values.
    rgbc=[] # Initialize empty list for rgb combo values
    r_in=hutt_full.columns.get_loc(r) #Get column index of r
    g_in=hutt_full.columns.get_loc(g) #Get column index of g
    b_in=hutt_full.columns.get_loc(b) #Get column index of b
    te=hutt_full.values # Get the numpy array values of the dataset 
    for i in range(len(te)): # For every row in the dataset, do this loop
        combo=te[i,r_in]*te[i,g_in]*te[i,b_in] # Calculate the rgb combo values by multiplying r*g*b
        rgbc.append(combo) # Append the rgb combo value to the rgbc list
    
    hutt_full['RGB_Combo']=0 # Initialize a new column in the dataset and fill it with 0 values 
    hutt_full['RGB_Combo']=np.array(rgbc) # Add the numpy values of the rgbc list to teh dataframe's new column
    #load kmeans
    kmeans=joblib.load('model_files/'+'kmeans.pkl') # Load the trained algorithm from pkl file to apply over full-data
    hutt_full.loc[:, 'point_cluster'] = kmeans.predict(hutt_full[[r,g,b]]) # Use this algorithm to cluster the column values into three parts - high, medium, low for every row and add these values to the new column 'point_cluster'
    hutt_full['point_cluster']=hutt_full['point_cluster'].astype('int64') # Make the values integer data type - 0 - low, 1 -medium, 2- high clusters
    mm=load_data('min_med_max.csv')  # Load this csv which was computed in the model training phase
    
    max_indx=mm.groupby(['point_cluster']).mean()['RGB_Combo'][mm.groupby(['point_cluster']).mean()['RGB_Combo']==np.max(mm.groupby(['point_cluster']).mean()['RGB_Combo'])].index # Get the index of the dataset's high cluster's mean value
    min_indx=mm.groupby(['point_cluster']).mean()['RGB_Combo'][mm.groupby(['point_cluster']).mean()['RGB_Combo']==np.min(mm.groupby(['point_cluster']).mean()['RGB_Combo'])].index # Get the index of the dataset's low cluster's mean value
    indexs=list(mm.groupby(['point_cluster']).mean().index) # List of minimum (low) cluster's mean value's index, maximum(high) cluster's mean value's index and medium cluster's mean value's index
    indexs.remove(min_indx) # Remove the dataset's minimum (low) cluster's mean value's index value from the list
    indexs.remove(max_indx) # Remove the dataset's maximum (high) cluster's mean value's index value from the list
    med_indx=indexs[0] # We removed those two values from the list to get the dataset's medium cluster's mean value's index value from the list
    
    hutt_full['Point_cluster']='' #Initiate an empty column called Point_cluster - the 'p' is capital in this new column. The old column with numbers for clusters has non-capital 'p'
    hutt_full['Point_cluster']=np.where(hutt_full['point_cluster']==max_indx[0],'High',hutt_full['Point_cluster']) # Label the clusters properly based on the maximum, medium and minimum cluster index values - Rename from 0,1,2, to Low, medium, high clusters- easier to maintain dataset
    hutt_full['Point_cluster']=np.where(hutt_full['point_cluster']==med_indx,'Medium',hutt_full['Point_cluster']) # Label the clusters properly based on the maximum, medium and minimum cluster index values - Rename from 0,1,2, to Low, medium, high clusters- easier to maintain dataset
    hutt_full['Point_cluster']=np.where(hutt_full['point_cluster']==min_indx[0],'Low',hutt_full['Point_cluster']) # Label the clusters properly based on the maximum, medium and minimum cluster index values - Rename from 0,1,2, to Low, medium, high clusters- easier to maintain dataset
    hutt_full=hutt_full.reset_index().drop(['index'],axis=1) # Reset the index and remove the index column that will be created while resetting the index of the dataset
    
    dn=load_data('groupby.csv')   # Load this pre-computed csv 
    #st.write(dn.head())

    gr=dn.columns[0] # Get the column name of R from the newly loaded data
    gg=dn.columns[1] # Get the column name of G from the newly loaded data
    gb=dn.columns[2] # Get the column name of B from the newly loaded data
    
    peak_r=dn.groupby(['Point_cluster']).max()[gr] # group the Point_cluster column by r or h - depending on the colourspace, and get the max r or h value on high, medium and low clusters
    peak_g=dn.groupby(['Point_cluster']).max()[gg] # group the Point_cluster column by g or s - depending on the colourspace, and get the max g or s value on high, medium and low clusters
    peak_b=dn.groupby(['Point_cluster']).max()[gb] # group the Point_cluster column by b or v - depending on the colourspace, and get the max b or v value on high, medium and low clusters
    peak_rgb=dn.groupby(['Point_cluster']).max()['RGB_Combo'] # group the Point_cluster column by RGB_Combo - depending on the colourspace, and get the max RGB_Combo value on high, medium and low clusters
    # st.write(peak_r,peak_g,peak_b)
    
    hutt_full['Ratio_peak_R']=0.0 #  Initiate Ratio_peak_R column and set all rows to 0
    hutt_full['Ratio_peak_G']=0.0 #  Initiate Ratio_peak_G column and set all rows to 0
    hutt_full['Ratio_peak_B']=0.0 #  Initiate Ratio_peak_B column and set all rows to 0
    hutt_full['Ratio_peak_RGBCombo']=0.0 #  Initiate Ratio_peak_RGBCombo column and set all rows to 0
    
    #low
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[r]/peak_r['Low'],hutt_full['Ratio_peak_R']) # For those r column points in the dataset where the cluster value is 'low', divide every row by the peak r value of all 'low' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'low' cluster value
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[g]/peak_g['Low'],hutt_full['Ratio_peak_G']) # For those g column points in the dataset where the cluster value is 'low', divide every row by the peak g value of all 'low' clusters and save the value to the Ratio_peak_G column. Do this for every row with 'low' cluster value
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[b]/peak_b['Low'],hutt_full['Ratio_peak_B']) # For those b column points in the dataset where the cluster value is 'low', divide every row by the peak b value of all 'low' clusters and save the value to the Ratio_peak_B column. Do this for every row with 'low' cluster value
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='Low',hutt_full['RGB_Combo']/peak_rgb['Low'],hutt_full['Ratio_peak_RGBCombo']) # For those Ratio_peak_RGBCombo column points in the dataset where the cluster value is 'low', divide every row by the peak r*g*b value of all 'low' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'low' cluster value
    
    #medium
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[r]/peak_r['Medium'],hutt_full['Ratio_peak_R']) # For those r column points in the dataset where the cluster value is 'medium', divide every row by the peak r value of all 'medium' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'medium' cluster value
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[g]/peak_g['Medium'],hutt_full['Ratio_peak_G']) # For those g column points in the dataset where the cluster value is 'medium', divide every row by the peak g value of all 'medium' clusters and save the value to the Ratio_peak_G column. Do this for every row with 'medium' cluster value
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[b]/peak_b['Medium'],hutt_full['Ratio_peak_B']) # For those b column points in the dataset where the cluster value is 'medium', divide every row by the peak b value of all 'medium' clusters and save the value to the Ratio_peak_B column. Do this for every row with 'medium' cluster value
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full['RGB_Combo']/peak_rgb['Medium'],hutt_full['Ratio_peak_RGBCombo']) # For those Ratio_peak_RGBCombo column points in the dataset where the cluster value is 'medium', divide every row by the peak r*g*b value of all 'medium' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'medium' cluster value
    
    #high
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='High',hutt_full[r]/peak_r['High'],hutt_full['Ratio_peak_R']) # For those r column points in the dataset where the cluster value is 'high', divide every row by the peak r value of all 'high' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'high' cluster value
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='High',hutt_full[g]/peak_g['High'],hutt_full['Ratio_peak_G']) # For those g column points in the dataset where the cluster value is 'high', divide every row by the peak g value of all 'high' clusters and save the value to the Ratio_peak_G column. Do this for every row with 'high' cluster value
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='High',hutt_full[b]/peak_b['High'],hutt_full['Ratio_peak_B']) # For those b column points in the dataset where the cluster value is 'high', divide every row by the peak b value of all 'high' clusters and save the value to the Ratio_peak_B column. Do this for every row with 'high' cluster value
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='High',hutt_full['RGB_Combo']/peak_rgb['High'],hutt_full['Ratio_peak_RGBCombo']) # For those Ratio_peak_RGBCombo column points in the dataset where the cluster value is 'high', divide every row by the peak r*g*b value of all 'high' clusters and save the value to the Ratio_peak_R column. Do this for every row with 'high' cluster value
    
    hutt_full=pd.get_dummies(hutt_full) # Converts categorical variable into dummy/indicator variables. For a single column with three different categorical strings (high,medium, low) - this function will create three separate columns with binary values indicating each row's cluster value. For a row with 'high' cluster value, the three new columns would be column 'low' with value 0, 'medium' with value 0, 'high' with value 1. ML algorithms work properly with non-string values
    # to confirm if all 3 clusters are there..else add dummy column
    clus_all=['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']
    fk=[] # Initialize and empty list
    for c in clus_all: # For every cluster in the clus_all
        if(c not in hutt_full.columns): # If there is a value in clus_all that is not a column in the dataset, then append to the list 
            fk.append(c)
            
    for fkc in fk: # For every cluster name in this list (values in this list are not present as columns in the dataset after clustering was done)
        hutt_full[fkc]=0 # Create the column with the name as the missing cluster column name - and make all values as 0
    # To maintain the order in low, med ,high....
    lowp=hutt_full['Point_cluster_Low'].values # Rearrange to maintain column order - this is important since feature engineering techniques like polynomial clustering expects the same order as it was trained on the training data 
    medp=hutt_full['Point_cluster_Medium'].values # Rearrange to maintain column order - this is important since feature engineering techniques like polynomial clustering expects the same order as it was trained on the training data 
    highp=hutt_full['Point_cluster_High'].values # Rearrange to maintain column order - this is important since feature engineering techniques like polynomial clustering expects the same order as it was trained on the training data 
    
    hutt_full.drop(clus_all,axis=1,inplace=True) # Drop existing cluster column names
    hutt_full['Point_cluster_Low']=lowp # Now put them new columns in order of low, medium and high
    hutt_full['Point_cluster_Medium']=medp # Now put them new columns in order of low, medium and high
    hutt_full['Point_cluster_High']=highp # Now put them new columns in order of low, medium and high
    hf=hutt_full.drop(['point_cluster','RGB_Combo'],axis=1).copy() # Drop both the point_cluster and rgb_combo - this was done during training phase too

    return hf # Return the updated dataset



def cluster_data_full_pt(hutt_full,r,g,b): # This is clustering function for full-data, but in pre-trained module. Everything is just the same as the above function. Only notable difference is the folder from which different files are loaded. Instead of model_files it's apply_pretrained_model_storage folder
    rgbc=[]
    r_in=hutt_full.columns.get_loc(r)
    g_in=hutt_full.columns.get_loc(g)
    b_in=hutt_full.columns.get_loc(b)
    te=hutt_full.values
    for i in range(len(te)):
        combo=te[i,r_in]*te[i,g_in]*te[i,b_in]
        rgbc.append(combo)
    
    hutt_full['RGB_Combo']=0
    hutt_full['RGB_Combo']=np.array(rgbc)
    #load kmeans
    kmeans=joblib.load('apply_pretrained_model_storage/'+'kmeans.pkl')
    hutt_full.loc[:, 'point_cluster'] = kmeans.predict(hutt_full[[r,g,b]])
    hutt_full['point_cluster']=hutt_full['point_cluster'].astype('int64')
    mm=load_data('min_med_max.csv')
    
    max_indx=mm.groupby(['point_cluster']).mean()['RGB_Combo'][mm.groupby(['point_cluster']).mean()['RGB_Combo']==np.max(mm.groupby(['point_cluster']).mean()['RGB_Combo'])].index
    min_indx=mm.groupby(['point_cluster']).mean()['RGB_Combo'][mm.groupby(['point_cluster']).mean()['RGB_Combo']==np.min(mm.groupby(['point_cluster']).mean()['RGB_Combo'])].index
    indexs=list(mm.groupby(['point_cluster']).mean().index)
    indexs.remove(min_indx)
    indexs.remove(max_indx)
    med_indx=indexs[0]
    
    hutt_full['Point_cluster']=''
    hutt_full['Point_cluster']=np.where(hutt_full['point_cluster']==max_indx[0],'High',hutt_full['Point_cluster'])
    hutt_full['Point_cluster']=np.where(hutt_full['point_cluster']==med_indx,'Medium',hutt_full['Point_cluster'])
    hutt_full['Point_cluster']=np.where(hutt_full['point_cluster']==min_indx[0],'Low',hutt_full['Point_cluster'])
    hutt_full=hutt_full.reset_index().drop(['index'],axis=1)
    
    dn=pd.read_csv('apply_pretrained_model_storage/groupby.csv')  
    #st.write(dn.head())

    gr=dn.columns[0]
    gg=dn.columns[1]
    gb=dn.columns[2]
    
    peak_r=dn.groupby(['Point_cluster']).max()[gr]
    peak_g=dn.groupby(['Point_cluster']).max()[gg]
    peak_b=dn.groupby(['Point_cluster']).max()[gb]
    peak_rgb=dn.groupby(['Point_cluster']).max()['RGB_Combo']
    # st.write(peak_r,peak_g,peak_b)
    
    hutt_full['Ratio_peak_R']=0.0
    hutt_full['Ratio_peak_G']=0.0
    hutt_full['Ratio_peak_B']=0.0
    hutt_full['Ratio_peak_RGBCombo']=0.0
    
    #low
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[r]/peak_r['Low'],hutt_full['Ratio_peak_R'])
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[g]/peak_g['Low'],hutt_full['Ratio_peak_G'])
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='Low',hutt_full[b]/peak_b['Low'],hutt_full['Ratio_peak_B'])
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='Low',hutt_full['RGB_Combo']/peak_rgb['Low'],hutt_full['Ratio_peak_RGBCombo'])
    
    #medium
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[r]/peak_r['Medium'],hutt_full['Ratio_peak_R'])
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[g]/peak_g['Medium'],hutt_full['Ratio_peak_G'])
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full[b]/peak_b['Medium'],hutt_full['Ratio_peak_B'])
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='Medium',hutt_full['RGB_Combo']/peak_rgb['Medium'],hutt_full['Ratio_peak_RGBCombo'])
    
    #high
    hutt_full['Ratio_peak_R']=np.where(hutt_full['Point_cluster']=='High',hutt_full[r]/peak_r['High'],hutt_full['Ratio_peak_R'])
    hutt_full['Ratio_peak_G']=np.where(hutt_full['Point_cluster']=='High',hutt_full[g]/peak_g['High'],hutt_full['Ratio_peak_G'])
    hutt_full['Ratio_peak_B']=np.where(hutt_full['Point_cluster']=='High',hutt_full[b]/peak_b['High'],hutt_full['Ratio_peak_B'])
    hutt_full['Ratio_peak_RGBCombo']=np.where(hutt_full['Point_cluster']=='High',hutt_full['RGB_Combo']/peak_rgb['High'],hutt_full['Ratio_peak_RGBCombo'])
    
    hutt_full=pd.get_dummies(hutt_full)
    # to confirm if all 3 clusters are there..else add dummy column
    clus_all=['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']
    fk=[]
    for c in clus_all:
        if(c not in hutt_full.columns):
            fk.append(c)
            
    for fkc in fk:
        hutt_full[fkc]=0
    # To maintain the order in low, med ,high....
    lowp=hutt_full['Point_cluster_Low'].values
    medp=hutt_full['Point_cluster_Medium'].values
    highp=hutt_full['Point_cluster_High'].values
    
    hutt_full.drop(clus_all,axis=1,inplace=True)
    hutt_full['Point_cluster_Low']=lowp
    hutt_full['Point_cluster_Medium']=medp
    hutt_full['Point_cluster_High']=highp
    hf=hutt_full.drop(['point_cluster','RGB_Combo'],axis=1).copy()

    return hf

    
def poly_creation_cluster_full(new_data): # To create polynomial features from existing columns - extends by adding extra predictors,a simple way to provide non-linear approach for the ml algorithms. 
    pcl=new_data[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for polynomial feature engineering, so we have to remove them before performing polynomial feaure engineering and so save them to a temporary dataframe to merge them later
    new_data.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
    print(new_data.shape)
    poly = PolynomialFeatures(2) # Initiate a polynomial features object - the '2' means create upto maximum of two-degree polynomial feature combinations
    x_poly=poly.fit_transform(new_data) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with more extended columns and polynomial feature combinations
    poly_names=poly.get_feature_names(['Feature'+str(l) for l in range(1,len(np.array(poly.get_feature_names())))]) # For all feaures in the new dataset, get their names
    df_poly1=pd.DataFrame(x_poly,columns=poly_names) # Create a new dataframe with column names as you got above and data as the above transformed dataset
    to_send=df_poly1.drop(['1'],axis=1) # There usually is a column '1' created extra, so just drop it
    #append the clusters
    to_send=pd.concat([to_send,pcl],axis=1) # Concat on vertical axis (add on columns)
    return to_send    # Return the updated dataset

def poly_creation_no_cluster_full(new_data): # To create polynomial features from existing columns - extends by adding extra predictors,a simple way to provide non-linear approach for the ml algorithms. This function runs when there is no clusters already created - it is safe to have as a separate function rather than using if statement in previous function.
    poly = PolynomialFeatures(2) # Initiate a polynomial features object - the '2' means create upto maximum of two-degree polynomial feature combinations
    x_poly=poly.fit_transform(new_data) # Fit_transform will first fit( like train ) and then transform the dataset to a new one with more extended columns and polynomial feature combinations
    poly_names=poly.get_feature_names(['Feature'+str(l) for l in range(1,len(np.array(poly.get_feature_names())))]) # For all feaures in the new dataset, get their names
    df_poly1=pd.DataFrame(x_poly,columns=poly_names) # Create a new dataframe with column names as you got above and data as the above transformed dataset
    to_send=df_poly1.drop(['1'],axis=1) # There usually is a column '1' created extra, so just drop it
    return to_send # Return the updated dataset

def correl_full(hf,type): # Using correlation to remove some features - Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features. Input to this function is the training data, training target values and type - this value is the type algorithm. Tree based algorithms are called type 'rf', neural network type algorithms are called type 'nn', and rest are called type 'common' 
    if(type=="rf"): # For each type of ml algorithm, load the csv in that name -to apply on full-data
        org=pd.read_csv('model_files/'+'correl_cluster_rf.csv')
    if(type=="nn"): # For each type of ml algorithm, load the csv in that name -to apply on full-data
        org=pd.read_csv('model_files/'+'correl_cluster_nn.csv')
    if(type=="common"): # For each type of ml algorithm, load the csv in that name -to apply on full-data
        org=pd.read_csv('model_files/'+'correl_cluster_common.csv')  
    spe_cols=list(org.columns) # Get only those columns present in the imported csv, from the existing dataset. The columns from the loaded csv are the ones which were saved after performing correlation cluster during input-data's feature engineering phase  
    rf_data=hf[spe_cols].copy() # Copy only those selected columns, from the current dataset
    return rf_data   # Return the updated dataset


def correl_full_pt(hf,type): # This is correlation function for full-data, but in pre-trained module. Everything is just the same as the above function. Only notable difference is the folder from which different files are loaded. Instead of model_files it's apply_pretrained_model_storage folder
    if(type=="rf"):
        org=pd.read_csv('apply_pretrained_model_storage/'+'correl_cluster_rf.csv')
    if(type=="nn"):
        org=pd.read_csv('apply_pretrained_model_storage/'+'correl_cluster_nn.csv')
    if(type=="common"):
        org=pd.read_csv('apply_pretrained_model_storage/'+'correl_cluster_common.csv')  
    spe_cols=list(org.columns)
    rf_data=hf[spe_cols].copy()
    return rf_data  


def lasso_reg_full(hf,org): #The lasso regression allows you to shrink or regularize these coefficients to avoid overfitting and make them work better on different datasets. This type of regression is used when the dataset shows high multicollinearity or when you want to automate variable elimination and feature selection
    spe_cols=list(org.columns) # Copy the column names present in the org dataset - org dataset was saved after computing lasso reg during feature engineering phase
    rf_data=hf[spe_cols].copy() # Get only those selected columns, from the current dataset
    return rf_data # Return the updated dataset

def pca_reduction_no_cluster_full(hf,type): # The use of PCA is to represent a multivariate data table as smaller set of variables. This overview may uncover the relationships between observations and variables - this is a non-cluster version.
    if(type=="lr"): # For each type of ml algorithm, load the pca pkl file in that name to apply over the full-data
        pca=joblib.load('model_files/'+'pca_lr.pkl')
    if(type=="common"): # For each type of ml algorithm, load the pca pkl file in that name to apply over the full-data
        pca=joblib.load('model_files/'+'pca_common.pkl') 
    pca_result1=pca.transform(hf) # After loading the model, use it to transform the dataset to a new one with reduced feature columns

    sub1=pd.DataFrame() # Initialize an empty dataframe
    sub1['pca-one']=0 # Add a new column called pca-one with all rows to 0
    sub1['pca-two']=0 # Add a new column called pca-two with all rows to 0
    sub1['pca-three']=0 # Add a new column called pca-three with all rows to 0
    
    sub1['pca-one'] = pca_result1[:,0] # Use the result from transforming the dataset and copy the first column values to the existing dataset
    sub1['pca-two'] = pca_result1[:,1] # Use the result from transforming the dataset and copy the second column values to the existing dataset
    sub1['pca-three'] = pca_result1[:,2] # Use the result from transforming the dataset and copy the third column values to the existing dataset
    svr_data1=sub1.copy()
    return svr_data1 # Return the updated dataset


def pca_reduction_no_cluster_full_pt(hf,type): # This is PCA function for non-clustered full-data, but in pre-trained module. Everything is just the same as the above function. Only notable difference is the folder from which different files are loaded. Instead of model_files it's apply_pretrained_model_storage folder
    if(type=="lr"):
        pca=joblib.load('apply_pretrained_model_storage/'+'pca_lr.pkl')
    if(type=="common"):
        pca=joblib.load('apply_pretrained_model_storage/'+'pca_common.pkl') 
    pca_result1=pca.transform(hf)

    sub1=pd.DataFrame()
    sub1['pca-one']=0
    sub1['pca-two']=0
    sub1['pca-three']=0
    
    sub1['pca-one'] = pca_result1[:,0]
    sub1['pca-two'] = pca_result1[:,1] 
    sub1['pca-three'] = pca_result1[:,2]
    svr_data1=sub1.copy()
    return svr_data1

def pca_reduction_cluster_full(hf,type): # The same as above, but if the data has clusters included then use this function to not consider the cluster column while doing PCA
    CAT_COUNTER=0 # Initiate a categorical counter
  #  print(hf.columns)
    if(all(x in hf.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for PCA    
        pcl_tr=hf[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for PCA feature engineering, so we have to remove them before performing PCA engineering and so save them to a temporary dataframe to merge them later
        hf.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    if(type=="lr"): # For each type of ml algorithm, load the pca pkl file in that name to apply over the full-data
        pca=joblib.load('model_files/'+'pca_lr.pkl')
    if(type=="common"): # For each type of ml algorithm, load the pca pkl file in that name to apply over the full-data
        pca=joblib.load('model_files/'+'pca_common.pkl')    
    pca_result1=pca.transform(hf)

    sub1=pd.DataFrame() # Initialize an empty dataframe
    sub1['pca-one']=0 # Add a new column called pca-one with all rows to 0
    sub1['pca-two']=0 # Add a new column called pca-two with all rows to 0
    sub1['pca-three']=0 # Add a new column called pca-three with all rows to 0
    
    sub1['pca-one'] = pca_result1[:,0] # Use the result from transforming the dataset and copy the first column values to the existing dataset
    sub1['pca-two'] = pca_result1[:,1]  # Use the result from transforming the dataset and copy the second column values to the existing dataset
    sub1['pca-three'] = pca_result1[:,2] # Use the result from transforming the dataset and copy the third column values to the existing dataset
    svr_data1=sub1.copy()
    
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        subb=pd.concat([svr_data1,pcl_tr],axis=1)  # Append those removed cluster columns back into the updated dataset
        return subb # Return the updated dataset if categorical columns were present
    else:
        return svr_data1  # Return the updated dataset if categorical columns were not present - when categorical columns are not present this function would not even run - but still let this line be there  


def pca_reduction_cluster_full_pt(hf,type): # This is PCA function for clustered full-data, but in pre-trained module. Everything is just the same as the above function. Only notable difference is the folder from which different files are loaded. Instead of model_files it's apply_pretrained_model_storage folder
    CAT_COUNTER=0
  #  print(hf.columns)
    if(all(x in hf.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])):
        pcl_tr=hf[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy()
        hf.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True)
        CAT_COUNTER=1
    if(type=="lr"):
        pca=joblib.load('apply_pretrained_model_storage/'+'pca_lr.pkl')
    if(type=="common"):
        pca=joblib.load('apply_pretrained_model_storage/'+'pca_common.pkl')    
    pca_result1=pca.transform(hf)

    sub1=pd.DataFrame()
    sub1['pca-one']=0
    sub1['pca-two']=0
    sub1['pca-three']=0
    
    sub1['pca-one'] = pca_result1[:,0]
    sub1['pca-two'] = pca_result1[:,1] 
    sub1['pca-three'] = pca_result1[:,2]
    svr_data1=sub1.copy()
    
    if(CAT_COUNTER==1):
        subb=pd.concat([svr_data1,pcl_tr],axis=1)
        return subb
    else:
        return svr_data1    

def standard_scale_no_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - StandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way. This is a non-cluster version
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_nn.pkl')
    if(type=="rf"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_rf.pkl')    
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_common.pkl')
    
    sv_data=sc.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    return sv_data # Return the updated dataset

def standard_scale_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - the same as above function but used when data has cluster column
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_nn.pkl')
    if(type=="rf"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_rf.pkl')     
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        sc=joblib.load('model_files/'+'sc_common.pkl')
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling  
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    print(svr_data1.shape)    
    sv_data=sc.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    print(sv_data.shape)
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True)    # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def min_max_scale_cluster_full(svr_data1,type):  # One of the four scaling processes done to dataset - MinMaxScaler scales all the data features in the range [0, 1] or else in the range [-1, 1] if there are negative values in the dataset.The min-max scalar form of normalization uses the mean and standard deviation to box all the data into a range of values.
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        ms=joblib.load('model_files/'+'ms_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        ms=joblib.load('model_files/'+'ms_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        ms=joblib.load('model_files/'+'ms_common.pkl')
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling  
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    sv_data=ms.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled       
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1)  # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def min_max_scale_no_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - the same as above function but used when data has no cluster column
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        ms=joblib.load('model_files/'+'ms_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        ms=joblib.load('model_files/'+'ms_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        ms=joblib.load('model_files/'+'ms_common.pkl')
    sv_data=ms.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    return sv_data # Return the updated dataset

def robust_scale_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - Robust Scaler scales features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range. Robust scaler is one of the best-suited scalers for outlier data sets
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        rs=joblib.load('model_files/'+'rs_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        rs=joblib.load('model_files/'+'rs_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        rs=joblib.load('model_files/'+'rs_common.pkl')
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling  
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    sv_data=rs.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def robust_scale_no_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - the same as above function but used when data has no cluster column
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        rs=joblib.load('model_files/'+'rs_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        rs=joblib.load('model_files/'+'rs_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        rs=joblib.load('model_files/'+'rs_common.pkl')
    sv_data=rs.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    return sv_data # Return the updated dataset

def power_scale_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - Apply a power transform featurewise to make data more Gaussian-like - by performing a Yeo-Johnson power algorithm
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        pt=joblib.load('model_files/'+'pt_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        pt=joblib.load('model_files/'+'pt_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        pt=joblib.load('model_files/'+'pt_common.pkl')
    CAT_COUNTER=0 # Initiate a categorical counter
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])): # Here, if there are clusters created ( see later that different combinations of feature engineering processes will be used to get best combination - which means some combinations might have data clustered and some may not ), and so if clusters are created then don't consider them for scaling  
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy() # If this part of code runs, it means the if statement has passed and that the dataset has been clustered and it contains cluster columns. We should not use cluster values (binary values) for scaling, so we have to remove them before scaling and so save them to a temporary dataframe to merge them later
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True) # After saving the three columns to a temporary dataframe, drop those three columns
        CAT_COUNTER=1 # Update the categorical counter to 1
    sv_data=pt.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    if(CAT_COUNTER==1): # If clusters were present, then append them back to this updates dataset - three new columns into this updated dataframe
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1) # Append those removed cluster columns back into the updated dataset
    scaled_data.sort_index(inplace=True) # Sort index on index values - some indices might be different after merging different dataframes, so just sort them
    return scaled_data # Return the updated dataset

def power_scale_no_cluster_full(svr_data1,type): # One of the four scaling processes done to dataset - the same as above function but used when data has no cluster column
    if(type=="lr"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        pt=joblib.load('model_files/'+'pt_lr.pkl')
    if(type=="nn"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        pt=joblib.load('model_files/'+'pt_nn.pkl')
    if(type=="common"): # For each type of ml algorithm, load the scaling pkl file in that name to apply over the full-data
        pt=joblib.load('model_files/'+'pt_common.pkl')
    sv_data=pt.transform(svr_data1) # After loading the scaling pkl file, apply that over the full-data
    cols=svr_data1.columns # Get the columns of the dataset before it was scaled - scaling returns an array and not dataframe. So to convert to dataframe, copy the column names of the dataframe before it was converted to array and in the next line, create a new dataframe with same names and scaled values
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index) # Just create another dataset with same values and same column names, and copy the index of the dataset before it was scaled
    return sv_data # Return the updated dataset


## The next few scaling functions are for the full-data but in pre-trained module. Everything is just the same as the above function. Only notable difference is the folder from which different files are loaded. Instead of model_files it's apply_pretrained_model_storage folder

def standard_scale_no_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_lr.pkl')
    if(type=="nn"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_nn.pkl')
    if(type=="rf"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_rf.pkl')    
    if(type=="common"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_common.pkl')
    
    sv_data=sc.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    return sv_data

def standard_scale_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_lr.pkl')
    if(type=="nn"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_nn.pkl')
    if(type=="rf"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_rf.pkl')     
    if(type=="common"):
        sc=joblib.load('apply_pretrained_model_storage/'+'sc_common.pkl')
    CAT_COUNTER=0
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])):
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy()
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True)
        CAT_COUNTER=1
    print(svr_data1.shape)    
    sv_data=sc.transform(svr_data1)
    print(sv_data.shape)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    if(CAT_COUNTER==1):
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1)
    scaled_data.sort_index(inplace=True)    
    return scaled_data

def min_max_scale_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        ms=joblib.load('apply_pretrained_model_storage/'+'ms_lr.pkl')
    if(type=="nn"):
        ms=joblib.load('apply_pretrained_model_storage/'+'ms_nn.pkl')
    if(type=="common"):
        ms=joblib.load('apply_pretrained_model_storage/'+'ms_common.pkl')
    CAT_COUNTER=0
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])):
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy()
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True)
        CAT_COUNTER=1
    sv_data=ms.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)       
    if(CAT_COUNTER==1):
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1)
    scaled_data.sort_index(inplace=True)
    return scaled_data

def min_max_scale_no_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        ms=joblib.load('apply_pretrained_model_storage/'+'ms_lr.pkl')
    if(type=="nn"):
        ms=joblib.load('apply_pretrained_model_storage/'+'ms_nn.pkl')
    if(type=="common"):
        ms=joblib.load('apply_pretrained_model_storage/'+'ms_common.pkl')
    sv_data=ms.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    return sv_data

def robust_scale_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        rs=joblib.load('apply_pretrained_model_storage/'+'rs_lr.pkl')
    if(type=="nn"):
        rs=joblib.load('apply_pretrained_model_storage/'+'rs_nn.pkl')
    if(type=="common"):
        rs=joblib.load('apply_pretrained_model_storage/'+'rs_common.pkl')
    CAT_COUNTER=0
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])):
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy()
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True)
        CAT_COUNTER=1
    sv_data=rs.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    if(CAT_COUNTER==1):
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1)
    scaled_data.sort_index(inplace=True)
    return scaled_data

def robust_scale_no_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        rs=joblib.load('apply_pretrained_model_storage/'+'rs_lr.pkl')
    if(type=="nn"):
        rs=joblib.load('apply_pretrained_model_storage/'+'rs_nn.pkl')
    if(type=="common"):
        rs=joblib.load('apply_pretrained_model_storage/'+'rs_common.pkl')
    sv_data=rs.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    return sv_data

def power_scale_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        pt=joblib.load('apply_pretrained_model_storage/'+'pt_lr.pkl')
    if(type=="nn"):
        pt=joblib.load('apply_pretrained_model_storage/'+'pt_nn.pkl')
    if(type=="common"):
        pt=joblib.load('apply_pretrained_model_storage/'+'pt_common.pkl')
    CAT_COUNTER=0
    if(all(x in svr_data1.columns for x in ['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'])):
        pcl_tr=svr_data1[['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High']].copy()
        svr_data1.drop(['Point_cluster_Low','Point_cluster_Medium','Point_cluster_High'],axis=1,inplace=True)
        CAT_COUNTER=1
    sv_data=pt.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    if(CAT_COUNTER==1):
        scaled_data=pd.concat([sv_data,pcl_tr],axis=1)
    scaled_data.sort_index(inplace=True)
    return scaled_data

def power_scale_no_cluster_full_pt(svr_data1,type):
    if(type=="lr"):
        pt=joblib.load('apply_pretrained_model_storage/'+'pt_lr.pkl')
    if(type=="nn"):
        pt=joblib.load('apply_pretrained_model_storage/'+'pt_nn.pkl')
    if(type=="common"):
        pt=joblib.load('apply_pretrained_model_storage/'+'pt_common.pkl')
    sv_data=pt.transform(svr_data1)
    cols=svr_data1.columns
    sv_data=pd.DataFrame(sv_data, columns=cols, index=svr_data1.index)
    return sv_data



#### SECTION 4 : FUNCTIONS FOR MAIN PAGE, TO ACCESS ALL MODULES AND LICENSE VALIDATION


# check internet connectivity
def connect(host='http://google.com'):
    try:
        urllib.request.urlopen(host) # Use urllib to request a connection and check if you get response back.
        return True # If you get a response True, then the internet is connected
    except:
        return False # If you get a response False, then the internet is not connected

# CRYPTO CODE:
# Validate license and expiry dates - if correct - take to normal page. Else, take to Upgrade page
    # RSAPubKey = "<RSAKeyValue><Modulus>rJZnuXHdYBVVYt5TFFpw1nCFICw581I00rJoVCPW/I7i+1QoAdAQ//3XGPRN8uZEW5bvMr8kBL+j7pOqfoskb2G6kH1kHYC3ShPgZ479B22jPj0IgztBTslY6FVClc+5ehIp53Qo3tCznG7c7nSUilimGYPnv7GZl79URcP1KcPcbRBEQ34DYmfkiXiDGI5nJX5h3GLk3MjOJ7a+EPSFeF/KC1eNsRQcYtUrvkTWFFjy6xeqtFEyEb6jA3m0rQ0J48l2RGvHHSGRIzsSbP+Dw8oC7mG4upLakvLdbHdqBTeO48JX7diFGIo+JryK+ukPS4ajssfayy9vTas8yeZwJQ==</Modulus><Exponent>AQAB</Exponent></RSAKeyValue>"
    # auth = "WyIxNzg3MjAiLCJsck54ZVgzVVpoQUVxaUxUOTJnbWt2bnZXU2h5MmYyTzhPOTcydmpwIl0="
    # # Change auth key for every user - unique to each device (or how many specified)

    # if(connect()):
    #     result = Key.activate(token=auth,\
    #                rsa_pub_key=RSAPubKey,\
    #                product_id=8303, \
    #                key="ITAVV-SHXJW-GUDBC-PXFRL",\
    #                machine_code=Helpers.GetMachineCode())

    #     if(result[0] == None):
    #         # an error occurred or the key is invalid or it cannot be activated
    #         # (eg. the limit of activated devices was achieved)
    #         print("The license does not work: {0}".format(result[1]))
    #         return render_template('upgrade.html',title='Optical Bathymetry Toolkit', message=" Your current subscription has expired! Please Upgrade to use the software ")
        
    #     elif(not Helpers.IsOnRightMachine(result[0])):
    #         return render_template('upgrade.html',title='Optical Bathymetry Toolkit', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")
      
    #     else:
    #         # everything went fine if we are here!
    #         print("The license is valid!")
    #         with open('licensefile.skm', 'w') as f:
    #             f.write(result[0].save_as_string())
    #             return render_template('home.html',title='Optical Bathymetry Toolkit',dropdown_options="home")

    # else:
    #     with open('licensefile.skm', 'r') as f:
    #         license_key = LicenseKey.load_from_string(RSAPubKey, f.read())
    #         if(result[0] == None):
    #             # an error occurred or the key is invalid or it cannot be activated
    #             # (eg. the limit of activated devices was achieved)
    #             print("The license does not work: {0}".format(result[1]))
    #             return render_template('upgrade.html',title='Optical Bathymetry Toolkit', message=" Your current subscription has expired! Please Upgrade to use the software ")
            
    #         elif(not Helpers.IsOnRightMachine(result[0])):
    #             return render_template('upgrade.html',title='Optical Bathymetry Toolkit', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")
      
    #         else:
    #             print("The license is valid!")
    #             print("License expires: " + str(license_key.expires))
################################################################ MAIN FUNCTIONS FROM HERE ########################################

# These functions below are for monitoring and validating license for machines - do not change any of these, will affect reading license files

# The next 5 functions are to get the details of current machine over which the application runs

def get_mac_id_pc(): # Get macid of local machine - The mac address is listed as series of 12 digits, listed as the Physical Address and is unique to each machine
    import netifaces
    mac_ids=[] # Initialize empty list
    for i in netifaces.interfaces(): # For all network interfaces (to include mac ids of virtual mac and multiple os)
        mac_ids.append(netifaces.ifaddresses(i)[netifaces.AF_LINK][0]['addr']) # Append every mac addr to the list
    print(mac_ids)
    return mac_ids  # Return the mac ids list

def get_time_zone_pc(): # Get time zone of local machine - we need time zone to properly calculate the subscription days
    from datetime import datetime 
    from tzlocal import get_localzone
    
    local = get_localzone() # Get the local zone name
    print(str(local))
    return str(local) # Return the local time zone name

def get_current_time_pc(pc_timezone,file_timezone): # Get the local time in the machine's time zone - input to function are the local timezone and the time zone from license file
    from datetime import datetime 
    from tzlocal import get_localzone
    print(file_timezone)
    if(pc_timezone!=file_timezone):  # Just compare if the two timezons are same - it does not matter if the user is in different time zone, the time will still be calculated based on the license's timezone
        print(" Different time zone ")
    # using now() to get current time  
    format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
    file_timezone=pytz.timezone(file_timezone) # Convert the string timezone value to a timezone object
    current_time_1 = datetime.now(file_timezone).strftime("%Y-%m-%d %H:%M") # Use datetime function to get the current time and in the specified format - returns a string
    current_time_1 = datetime.strptime(current_time_1, format)    # Convert the string date-time value to date-time object 
    return current_time_1 # Return the date-time object

def get_current_hostname(): # Get the host name of local machine - In recent updates we do not compare the username/host names as they can be changed by the user on the same machine
    import socket
    print(socket.gethostname())
    host_name= str(socket.gethostname())   # Get the hostname as string value
    return host_name # Return the string hostname

def get_current_machinesid(): # Get the current machine SID -A machine SID is a unique identifier generated by Windows Setup that Windows uses as the basis for the SIDs for administrator-defined local accounts and groups.
    # import win32security
    # desc = win32security.GetFileSecurity(".", win32security.OWNER_SECURITY_INFORMATION)
    # sid = desc.GetSecurityDescriptorOwner()
    # sidstr = str(win32security.ConvertSidToStringSid(sid))  
    # print(sidstr)
    import subprocess
    st= 'wmic useraccount get sid' # This is a cmd prompt command to get machine's all SID
    result = subprocess.run(st, stdout=subprocess.PIPE) # Use subprocess to run cmd-prompt commands in python
    l =result.stdout.decode('utf-8').splitlines() # Use the subprocess pipeline results and decode them on utf-8 and split different lines into a list of values
    while("" in l) : # remove blank spaces or empty values from the list of SID
        l.remove("")

    final_sid= [] # Initialize empty list
    for i,z in enumerate(l): # For all the values and index in the list
        z=z.strip() # Strip is to remove any whitespaces or tab-spaces at the beginning and end of the string values
        if(z!="SID"): # The values in the above list have all SIDs and a heading value called SID - so we just take the SID values and not the string name "SID"
            final_sid.append( str(z.strip()) ) # If the value is not "SID", then append the values to the list

    return final_sid # Return this list of machine SID values

# The next 7 functions are to extract details from the license 

def extract_mac_id_file(lic_content): # Get macid from the license file - The mac address is listed as series of 12 digits, listed as the Physical Address and is unique to each machine
    print(lic_content)
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    mac_ids=[]
    for i in file_str[0].split('___'): # File_str[0] means the first element after splitting on the _^^_ symbol. Now we split this string value again on ___ symbol because there are multiple mac IDs , each separated by ___ symbol. We get a list of all ids by doing this
        mac_ids.append(str(i))

    return mac_ids

def extract_time_zone_file(lic_content): # Get time zone from the license file - we need time zone to properly calculate the subscription days
    from datetime import datetime 
    from tzlocal import get_localzone
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    timezone=file_str[-4] # File_str[-4] means the fourth element from last, after splitting on the _^^_ symbol. This is the timezone where the license was generated - this is also the timezone where the user of the application intended to use in.
    return timezone

def extract_start_time_file(lic_content): # Get the start time from the license file - the start time indicates when the license file got activated 
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    starttime=file_str[-6] # File_str[-6] means the sixth element from last, after splitting on the _^^_ symbol - this gives the exact date at which the license was generated, We use this to later calculate the expiry status of the license
    return starttime

def extract_subscription_days(lic_content): # Extract the validity days from the license file -  use this to calculate the time difference between the current time in machine and license start time to check if the license is still valid or not
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    subscription_days=file_str[-5] # File_str[-5] means the fifth element from last, after splitting on the _^^_ symbol. This gives the total subscription days and is used to check the license's validity 
    return subscription_days


def extract_hostname(lic_content): # Get the host name from the license file - In recent updates we do not compare the username/host names as they can be changed by the user on the same machine
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    host_name=file_str[-3] # File_str[-3] means the third element from last, after splitting on the _^^_ symbol. This is the name of the machine - also used to confirm if the application is still running on the same machine/ machine name
    return host_name

def extract_machinesid(lic_content): # Get the machine SID list from the license file -A machine SID is a unique identifier generated by Windows Setup that Windows uses as the basis for the SIDs for administrator-defined local accounts and groups.
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    machine_sid=file_str[-2] # File_str[-2] means the second element from last, after splitting on the _^^_ symbol. This gives us the machine SID - which is unique to each pc. Also, just like mac id, the machine sid is also a list of sids - this is to allow the application to run even if logged in from another account from the same machine
    machine_sids=[] # Initialize empty list
    for i in machine_sid.split('___'): # Now we split this string again on ___ symbol because there are multiple mac IDs , each separated by ___ symbol. We get a list of all ids by doing this
        machine_sids.append(str(i)) # Append all sid values to the new list
        
    return machine_sids


def extract_tool_unique_key(lic_content): # Extract an unique key - this is the tool name and used to identify which tool a license belongs to
    file_str=str(lic_content).split("_^^_") # While generating license files, different elements of the key - like mac id, SID etc were separated by _^^_ symbol - so we have to split the entire string on this _^^_ symbol to get all the elements back 
    unique_key=file_str[-1] # File_str[-1] is the last element, after splitting on the _^^_ symbol. This will give the software associated with the license - this would not allow running a bathymetry license with mesh tool etc
    return unique_key


###Part 1----
@app.route('/')
@app.route('/home')
def home(): # Main page
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    # First thing - remove the Bathymetry_PORT.txt file. A new text file with another random port will be generated everytime the app is opened. So delete the generated file here, and so it does not affect time user opens the app
    # if(os.path.exists('Bathymetry_PORT.txt')):
    #     os.remove('Bathymetry_PORT.txt')

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-07-06 14:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release - FORMAT - YEAR/DAY/MONTH TIME
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('home.html',title='Optical Bathymetry Toolkit',dropdown_options="home") # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error


@app.route('/about')
def about(): # For about page
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-04-05 16:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('about.html',title='About Us') # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error


@app.route('/simplified_workflow')
def simplified_workflow(): # For ml workflow page
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-04-05 16:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('simplified_workflow.html',title='Simplified Workflow') # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error


@app.route('/apply_pretrained_model')
def apply_pretrained_model(): # For pre-trained model page
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-04-05 16:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('apply_pretrained_model.html',title="Apply Pretrained Model") # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error

@app.route('/upload_file_lic',methods=['GET','POST'])
def upload_file_lic(): #Upload license file - if it's either the first time or updating current license file after an upgrade in subscription plans
    if( request.method=="POST" ): # Check if request is POST
        if( request.files ): # If there are incoming files as input from user 
            file=request.files["file"] # Get all file names
            print(file)

            # getting my documents path to save license and not ask to upload at every update release
            import ctypes.wintypes
            CSIDL_PERSONAL = 5       # My Documents
            SHGFP_TYPE_CURRENT = 0   # Get current, not default value
            buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
            ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

            print(" file saved ")

            # Move this license file to My Documents, before that check if folder is present in My Documents. 
            # If not present create a folder
            # If present check if a license file is already there, if yes then remove and copy the new file.
            if( os.path.exists( str(buf.value)+'/Optical Bathymetry Toolkit' ) == False ):
                os.mkdir( str(buf.value)+'/Optical Bathymetry Toolkit')
            
            if( os.path.exists( str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm' ) ): # If the file path already exists, then remove it and make a new directory and copy the license file into the directory
                os.remove( str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm' )
            
            # shutil.copy("Licensefile_LRSC_Bathymetry.skm", str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')
            file.save( str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm') # Save the file to the path

            return " license uploaded "
        return render_template('simplified_workflow.html/upload_file_lic.html')



@app.route('/auto_upgrade')
def auto_upgrade(): # For auto-upgrade page - Non functional
    return render_template('auto_upgrade.html',title='Upgrade- Autodetect')


@app.route('/generate_manual_request',methods=['POST','GET'])
def generate_manual_request(): # If they user wants to generate license file, they click this generate request button and the software will create a text file with all the machine info, and the user has to send that text file to us so that we can genarate a license file for them
    import base64

    # macid, starttime, subscription days, time zone of user, unique key for user
    from getmac import get_mac_address as gma
    mac_id=gma()
    import netifaces
    mac_ids=[] # Initialize empty list
    for i in netifaces.interfaces(): # For all network interfaces (to include mac ids of virtual mac and multiple os)
        print(netifaces.ifaddresses(i)[netifaces.AF_LINK])
        mac_ids.append(str(netifaces.ifaddresses(i)[netifaces.AF_LINK][0]['addr'])) # Append every mac addr to the list
    # use this - gets macid for all interfaces 

    from datetime import datetime 
    # for timezone
    from tzlocal import get_localzone
    local = get_localzone() # Get the local zone name
    print(str(local))
   
    # Machine Name:
    import socket
    print(socket.gethostname())
    host_name= str(socket.gethostname())  # Get the hostname as string value   

    # Machine SID (c drive info)
    # pip install pywin32
    # import win32security
    # desc = win32security.GetFileSecurity(".", win32security.OWNER_SECURITY_INFORMATION)
    # sid = desc.GetSecurityDescriptorOwner()
    # sidstr = str(win32security.ConvertSidToStringSid(sid))
    # print("Sid is", sidstr)
    import subprocess
    st= 'wmic useraccount get sid' # This is a cmd prompt command to get machine's all SID
    result = subprocess.run(st, stdout=subprocess.PIPE) # Use subprocess to run cmd-prompt commands in python
    l =result.stdout.decode('utf-8').splitlines() # Use the subprocess pipeline results and decode them on utf-8 and split different lines into a list of values
    while("" in l) : # remove blank spaces or empty values from the list of SID
        l.remove("")
    final_sid= '' # Initialize empty string
    for i,z in enumerate(l): # For all the values and index in the list
        z=z.strip() # Strip is to remove any whitespaces or tab-spaces at the beginning and end of the string values
        if(z!="SID"): # The values in the above list have all SIDs and a heading value called SID - so we just take the SID values and not the string name "SID"
            final_sid+="___"   # Add this ___ symbol at the prefix of every SID value
            final_sid+=z.strip() # Add the stripped string value to the string 
    print(final_sid)

    unique_key_bathy= "Optical Bathymetry Toolkit" # Unique tool key
    filename_bathy="Software_Generated_MachineInfo-LRSC.txt" # Filename for the output text file

    # For naming license files - get os name and version
    import platform
    os_full= str(platform.system()) + str(platform.release())

    # Final key
    final_macid=''
    for i in mac_ids: # For every mac id in the list
        final_macid+="___" # Add this ___ symbol at the prefix of every mac id value
        final_macid+=i # Add the stripped string value to the string 
        
    message=str(final_macid)+'_^^_'+str(local)+'_^^_'+ str(host_name) + '_^^_' + str(os_full) + '_^^_' + str(final_sid) + '_^^_' + str(unique_key_bathy) # Join the final string by combining all elements of the license content, with a "_^^_" symbol separating them
    print(message)

    print(" STARTED ENCRYPTING  ")
    message_bytes = message.encode('ascii') # Encode the string to ascii value
    base64_bytes = base64.b64encode(message_bytes) # Encode the ascii byte value to a base64 value
    print(base64_bytes)
    # Write into file
    text_file = open( filename_bathy , "wb") # Open a text file with the above specified output file name
    n = text_file.write(base64_bytes) # Write into the text file - write the license key as byte values
    text_file.close() # Close the object and save the file

    # import os
    # desktop = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop') 
    # print(desktop)
    # shutil.copy("Software_Generated_MachineInfo-LRSC.txt", desktop)
    
    return "File generated"


@app.route('/copy_manual_request_file',methods=['POST','GET'])
def copy_manual_request_file(): # Once the text file is generated - copy the output text file to the user's selected location
    import os
    lic_file_path_obj=request.form['lic_file_path'] # Get the file path object from javascript
    lic_file_path=json.loads(lic_file_path_obj) # Get the file path string from the object value

    shutil.copy("Software_Generated_MachineInfo-LRSC.txt", lic_file_path+"/Software_Generated_MachineInfo-LRSC.txt") # Copy the info text file from current path to the user's selected location
    return "File saved"

## The below function is non-functional
@app.route('/auto_request',methods=['POST','GET'])
def auto_request():
    import os
    if(os.path.exists('mac_id_request.txt')):
        return jsonify(" You have already requested for a License file. Kindly wait, you will recieve a mail soon ")
    else:
        upgrade_name_obj=request.form['upgrade_name']
        upgrade_name=json.loads(upgrade_name_obj)

        upgrade_email_obj=request.form['upgrade_email']
        upgrade_email=json.loads(upgrade_email_obj)
        
        from tzlocal import get_localzone
        local = get_localzone()
        
        mac_ids=get_mac_id_pc()
        pc_hostname= get_current_hostname()
        pc_machinesid = get_current_machinesid()
        # For naming license files - get os name and version
        import platform
        os_full= str(platform.system()) + str(platform.release())

        # Write into file
        print("local ",local)
        text_file = open("mac_id_request.txt", "w")
        n = text_file.write('{}\n{}\n{}\n{}\n{}\n{}\n{}\n'.format( upgrade_name, upgrade_email , mac_ids, pc_hostname, os_full, pc_machinesid, local ))
        text_file.close()

        #Auto mailing license :
        from email import encoders
        from email.mime.base import MIMEBase
        from email.mime.multipart import MIMEMultipart
        from email.mime.text import MIMEText
        import smtplib

        email = MIMEMultipart()
        recipients = ['paveethran@landriversea.com', 'software.support@landriversea.com']
        email["From"] = "paveethran@landriversea.com"
        email["To"] = ", ".join(recipients)
        email["Subject"] = "LICENSE_REQUEST_KEY_BATHYMETRY_TOOLKIT"
        body = "Name : "+ upgrade_name + " | Business email: " + upgrade_email + " "

        # Add body and attachment to email
        email.attach(MIMEText(body, "plain"))
        attach_file = open("mac_id_request.txt", "r") # open the file
        report = MIMEBase("application", "octate-stream")
        report.set_payload((attach_file).read())
        encoders.encode_base64(report)
        #add report header with the file name
        attach_file_name="mac_id_request.txt"
        import os
        report.add_header("Content-Decomposition", "attachment", filename = os.path.basename(attach_file_name))
        email.attach(report)

        server = smtplib.SMTP('smtp.outlook.com', 587)
        server.starttls()
        server.login("paveethran@landriversea.com", "LandRiverSea77")
        text = email.as_string()
        server.sendmail("paveethran@landriversea.com", recipients, text)

        return jsonify("requested")



#### SECTION 5: INPUT DATA PRE-PROCESSING COMMON TO BOTH CURVE FIT MODULE AND SIMPLIFIED WORKFLOW MODULE


app.config['FILE_SAVE']="model_files/" # Set the app config to model_files. This will allow the flask app to store input data files directly into the model_files path. We change this later for every module
@app.route('/upload_file',methods=['POST','GET'])
def upload_file(): # This function gets the input csv data from the user and saves a copy of it to the app's working directory 
    print("in file present")
    # cluster_count_obj=request.form['cluster_count_train']
    # cluster_count=json.loads(cluster_count_obj)
    # downloads_path=get_download_path()
    # if(str(cluster_count)!= "-9"):
    #     downloads_path=str(downloads_path)+ '/train_data_from_bctk_software' + str(cluster_count) + '.csv'
    # elif(str(cluster_count)=="-9"):
    #     downloads_path=str(downloads_path)+ '/train_data_from_bctk_software.csv'
    # print(downloads_path)
    # if(os.path.exists(downloads_path)):
    #     os.remove(downloads_path)
    #     return jsonify(" Exisiting file deleted - Great")
    # else:
    #     return jsonify(" No path present - Great")    
    if( request.method=="POST" ): # If the request is POST
        if( request.files ): # If there are input files from the user
            if os.path.exists('model_files'): # If model_files folder already exists then delete and and remake the folder - This is the folder to store all dump files and csv
                shutil.rmtree('model_files', ignore_errors=True)
            if not os.path.exists('model_files'): # If the folder does not already exist, then create one
                os.mkdir('model_files')
            file=request.files["file"] # request variable is the flask variable which contains all the information sent from javascript side to python for processing, we access values by request.files[ name_of_variable_given_in_javascript_side ]
            print(file)
            file.save(os.path.join(app.config['FILE_SAVE'], file.filename)) # On the top line of this function, we have set app config to model_files. This will allow the flask app to store input data files directly into the model_files path. In this line we do file.save to save files with names file.filename into the recently set app.config value - which is model_files
            print(" file saved ")
            #f"{file.filename}
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'],"temp_pipeline.csv") )): # Rename the input data to temp_pipeline.csv. Note that the original location of the file is unchanged - meaning the original file untouched, but a copy of the file is stored in model_files folder and that file is renamed to temp_pipeline.csv which will be used almost everywhere in the code
                os.remove( os.path.join(app.config['FILE_SAVE'],"temp_pipeline.csv") ) # If another file named temp_pipeline.csv already exists, then delete that and rename the new file to temp_pipeline.csv
            os.rename( os.path.join(app.config['FILE_SAVE'], file.filename), os.path.join(app.config['FILE_SAVE'],"temp_pipeline.csv") ) # rename the new file to temp_pipeline.csv
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'], file.filename)) ): # Remove the initial copy of the user's copied file - we only rename it, so this line won't probably run, but still if multiple copies of same file are created somehow, then delete them
                os.remove( os.path.join(app.config['FILE_SAVE'], file.filename) ) # Remove the duplicate files
            
            res = make_response(jsonify({"message":  f"File uploaded successfully "}),200) # Send back response to user saying files are uploaded

            return res
        return render_template('simplified_workflow.html/upload_file.html')



# full river input and process
@app.route('/train_river_input',methods=['POST'])
def train_river_input():
    #data=pd.read_csv('')
    print("Merging")
    # if os.path.exists('model_files'):
    #     shutil.rmtree('model_files')
    # os.mkdir('model_files')
    # cluster_count_obj=request.form['cluster_count_train']
    # cluster_count=json.loads(cluster_count_obj)
    # full_river=pd.DataFrame()
    # copy_done=0
    # downloads_path=get_download_path()
    # if(str(cluster_count) != "-9"):
    #     for i in range(1,int(cluster_count)+1):           
    #         file_path=str(downloads_path)+ '/train_data_from_bctk_software' + str(i) + '.csv'
    #         df=pd.read_csv(file_path)
    #         full_river=pd.concat([full_river,df],axis=0)
    #         full_river=full_river.reset_index().drop(['index'],axis=1)
    #     copy_done=1

    # elif(str(cluster_count)=="-9"): 
    #     print("-9")
    #     file_s=''
    #     downloads_path=''
    #     downloads_path=get_download_path()
    #     file_s=str(downloads_path)+ '/train_data_from_bctk_software.csv'
    #     full_river=pd.read_csv(file_s)
    #     copy_done=1
    #     print("copy done")
            
    # print("Train data sample" , full_river.head())
    # print(full_river.tail())
    # print(full_river.isnull().sum())
    # if(True in list(full_river.iloc[-1].isnull()[:])):
    #     drop_ind=full_river.index[-1]
    #     print(drop_ind)
    #     full_river.drop(drop_ind,axis=0,inplace=True)
    # full_columns=full_river.columns
    # print(list(full_columns))    
    # full_river.to_csv('model_files/'+'temp_pipeline.csv',index=False)
 
    # if( (str(cluster_count) != "-9") & (copy_done==1) ):
    #     for i in range(1,int(cluster_count)+1):
    #         file_path=str(downloads_path)+ '/train_data_from_bctk_software' + str(i) + ".csv"
    #         if(os.path.exists(file_path)):
    #             os.remove(file_path)
    #             print(" Deleted ",cluster_count)

    # elif( (str(cluster_count) == "-9") & (copy_done==1) ):
    #     downloads_path=get_download_path()
    #     file_path=str(downloads_path)+ '/train_data_from_bctk_software.csv'
    #     if(os.path.exists(file_path)):
    #         os.remove(file_path)
    #         print(" Deleted single file ")
        
    df=pd.read_csv('model_files/'+'temp_pipeline.csv', nrows=10) # read first 10 rows and send to javascript to view in app page
    full_columns=df.columns # Get all the column names of the dataframe
    df_jsonfiles = json.loads(df.head().to_json(orient='records')) # Always load as json and then jsonify the data (next two lines) before sending - only way to communicate between python and javascript
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))


# @app.route('/store_local',methods=['GET','POST'])
# def store_file():
#     #   f = request.files['file']
#     #   f.save(secure_filename(f.filename))
#     #   return "done "
#     # only reuqest.form works
#     data = request.form['akey']
#     data_ujson = ujson.loads( data )
#     #data = str(data[0:20])
#     df=pd.io.json.json_normalize(data_ujson)
#   #  print(df.head())
  
#     if(True in list(df.iloc[-1].isnull()[:])):
#         drop_ind=df.index[-1]
#         print(drop_ind)
#         df.drop(drop_ind,axis=0,inplace=True)  
    
#     #create a dir for all temp files
#     if os.path.exists('model_files'):
#         shutil.rmtree('model_files')
#         os.mkdir('model_files')
#     else:
#         os.mkdir('model_files')
#     print(df)    
#     df.to_csv('model_files/'+'temp_pipeline.csv',index=False)
#     del df
#     return ("Success from store_file")


# Convert to HSV Color Model 
def rgb_to_hsv(r, g, b): 

    # R, G, B values are divided by 255 
    # to change the range from 0..255 to 0..1: 
    r, g, b = r / 255.0, g / 255.0, b / 255.0
  
    # h, s, v = hue, saturation, value 
    cmax = max(r, g, b)    # maximum of r, g, b 
    cmin = min(r, g, b)    # minimum of r, g, b 
    diff = cmax-cmin       # diff of cmax and cmin. 
  
    # if cmax and cmax are equal then h = 0 
    if cmax == cmin:  
        h = 0
      
    # if cmax equal r then compute h 
    elif cmax == r:  
        h = (60 * ((g - b) / diff) + 360) % 360
  
    # if cmax equal g then compute h 
    elif cmax == g: 
        h = (60 * ((b - r) / diff) + 120) % 360
  
    # if cmax equal b then compute h 
    elif cmax == b: 
        h = (60 * ((r - g) / diff) + 240) % 360
  
    # if cmax equal zero 
    if cmax == 0: 
        s = 0
    else: 
        s = (diff / cmax) * 100
  
    # compute v 
    v = cmax * 100
    return h, s, v 

@app.route('/calculate_distance',methods=['GET','POST'])
def calculate_distance(): # Calculate aggregate distance between points in the input dataset
    df=load_data('temp_pipeline.csv') # Load the input csv
    yorn=request.form['yorn'] # Get what option user selected for distance present- yes or no
    sel_feats=request.form['sel_feat'] # Get the feature names selected by user
    print(" Selected feats from ajax: ",sel_feats)
    print(yorn,type(yorn))
    #sel_feats is already in json format
    feats  = json.loads(sel_feats) # Convert a jsonified object recieved from javascript back to string
    print(feats)
    df=df[feats].copy() # Subset of dataframe with only selected columns
    df=df.dropna() # Drop nan values if present
    df=df.sort_values(by=feats[0]) # Sort values based on just the first column (id)
    df=df.reset_index().drop(['index'],axis=1) # reset index to make the row numbers correct and in sorted order based on id columns
    if(yorn=="no_distance"): # If no distance present - calculate it and make new column in the dataframe 
        east=feats[1] # Get east coordinate
        north=feats[2] # Get north coordinate
        agg=0 # Set aggregate value to 0 
        df['Distance_new']=0 # Create new column named Distance_new and set all rows to 0
        for i in range(1,len(df)): # for every row in the dataframe
            diff=distance_cal(df[east].iloc[i],df[north].iloc[i],df[east].iloc[i-1],df[north].iloc[i-1]) # Calculate distance between current point and previous point 
            agg=agg+diff # Add the distance to get the aggregate value
            df['Distance_new'].iloc[i]=agg # Set the aggregate value to the Distance_new column in teh dataframe, for every row
        print(df.head())
        df=df.round(3) #round the values to 3 decimal and send the first 5 rows back to javascript
        print(df.head())       
        jsonfiles = json.loads(df.head().to_json(orient='records')) # Convert first 5 rows of the dataframe to json object
        
        if(True in list(df.iloc[-1].isnull()[:])): # Sometimes the last row of a dataframe has null values - happened only once, no idea how it came there, but somehow a null row gets added. So in case any such thing exists, get the last row index and remove it.
            drop_ind=df.index[-1] # Get the last row's index value
            print(drop_ind)
            df.drop(drop_ind,axis=0,inplace=True) # Drop the last row by it's index value
        df.sort_index(inplace=True) # And sort the dataframe just to be sure it's in right order after dropping

        colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
        colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    
        r=feats[3] #3 because of east north corrd input  - this line you get R pixel column value
        g=feats[4] # Get G pixel column name
        b=feats[5] # Get B pixel column name 
        # check if 0 value is present in r,g,b, if 0 present, then make those values 0.1
        if(df.describe()[r]['min']==0):    # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator               
            df[r]=np.where(df[r]==0,0.1,df[r])
        if(df.describe()[g]['min']==0):  # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                   
            df[g]=np.where(df[g]==0,0.1,df[g])
        if(df.describe()[b]['min']==0):  # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                   
            df[b]=np.where(df[b]==0,0.1,df[b])
        
        if(df.describe()[r]['min']<0):   # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                
            df[r]=np.where(df[r]<0,0.1,df[r])
        if(df.describe()[g]['min']<0):   # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator              
            df[g]=np.where(df[g]<0,0.1,df[g])
        if(df.describe()[b]['min']<0):   # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                
            df[b]=np.where(df[b]<0,0.1,df[b])

        csp_str=''
        print(colourspaces)
        if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
            print('same dataset - no hsv only rgb')
            csp_str='rgb' # No changes needed, rgb values already there

        elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
            print('same dataset - no hsv only rgb')
            csp_str='rgb' # No changes needed, rgb values already there

        elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
            print('change in dataset - no rgb only hsv')
            # hsv_vals = matplotlib.colors.rgb_to_hsv(df[[r,g,b]]/255)
            hsv_vals = df[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
            df2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
            df[r] = df2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
            df[g] = df2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
            df[b] = df2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

            df = df.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
            for ix, valx in enumerate(feats): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
                if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                    feats[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
                if(valx == g):
                    feats[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
                if(valx == b):
                    feats[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
            
            # check if 0 value is present in r,g,b
            if(df.describe()['H_generated']['min']==0):      # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator             
                df['H_generated']=np.where(df['H_generated']==0,0.1,df['H_generated'])
            if(df.describe()['S_generated']['min']==0):      # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator             
                df['S_generated']=np.where(df['S_generated']==0,0.1,df['S_generated'])
            if(df.describe()['V_generated']['min']==0):      # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator             
                df['V_generated']=np.where(df['V_generated']==0,0.1,df['V_generated'])       

            del df2 # Delete the temporary dataframe
            csp_str='hsv' # Make the value of this string to just hsv

        elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
            print('change in dataset - rgb and hsv')
            hsv_vals = df[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
            df2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
            df.insert(6, 'H_generated', df2['H_generated'].copy()) # When both rgb and hsv are selected, we should not substitue rgb with hsv values. Instead insert new values right after rgb. Order is important in this - so the old order was id, east, north , r,g,b, other fetures, depth. Then new column would be id,east, north, r,g,b, h,s,v,other features , depth.
            df.insert(7, 'S_generated', df2['S_generated'].copy()) # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
            df.insert(8, 'V_generated', df2['V_generated'].copy()) # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

            #insert in feats 
            feats.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
            feats.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
            feats.insert(8, 'V_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

            # check if 0 value is present in r,g,b
            if(df.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                             
                df['H_generated']=np.where(df['H_generated']==0,0.1,df['H_generated'])
            if(df.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                             
                df['S_generated']=np.where(df['S_generated']==0,0.1,df['S_generated'])
            if(df.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                             
                df['V_generated']=np.where(df['V_generated']==0,0.1,df['V_generated'])       

            del df2 # Delete the temporary dataframe
            csp_str='rgbhsv' # Make the value of this string to both rgbhsv

        csp_csv_df=pd.DataFrame([csp_str],columns=['colourspaces'])    # Create a new dataframe with column 'colourspaces' and with value of the string created after creating and finalising colourspace values
        csp_csv_df.to_pickle('model_files/'+'colourspaces.pkl') # Save the colourspace selected option as pkl file to be used later

        print(df)
        df.to_csv('model_files/'+'temp_pipeline_2.csv',index=False) # Save the processed data as csv
        del df # Delete another temporary dataframe
        return (jsonify({ 'jsonfiles': jsonfiles, 'feats': list(feats) }) )
        #return jsonify("parsed")
        
    else:
        return jsonify("Could not parse") # If the no distance was not chosen or some error occured in between computations, then return this error

@app.route('/pre_distance',methods=['GET','POST'])
def pre_calc_distance(): # If distance already present, no need to calculate again, instead just get the distance column name, rename and add to the input dataframe in the correct position
    df=load_data('temp_pipeline.csv') # Load the input csv
    sel_feats=request.form['sel_feat'] # Get the feature names selected by user
    feats  = json.loads(sel_feats) # Convert a jsonified object recieved from javascript back to string
    print(feats)
    cdist_get=request.form['cdist_send'] # Get the column name which has the distance values already present - get the json object of the column name value
    cdist = json.loads(cdist_get) # Convert the json object to string value
    df=df.sort_values(by=feats[0]) # Sort values based on just the first column (id)
    df=df.reset_index().drop(['index'],axis=1) # reset index to make the row numbers correct and in sorted order based on id columns
    if(("Distance_new" in df.columns) & (cdist!="Distance_new")): # If the distance column is present already but it's not named as 'Distance_new' and there is already another column named 'Distance_new', then delete tha 'Distance_new'column 
        df.drop(['Distance_new'],axis=1,inplace=True)  # Drop the 'Distance_new' column
    df.rename(columns={cdist: "Distance_new"},inplace=True) # Rename the already present distance column in the dataframe to 'Distance_new'
    new_df = pd.concat([df[feats].copy(),df['Distance_new']],axis=1)  # Merge the data with jsut the selected features and the newly renamed distance column, to a single new dataframe 
    new_df=new_df.dropna() # Drop nan values if present
    new_df=new_df.reset_index().drop(['index'],axis=1) # reset index to make the row numbers correct and in sorted order based on id columns
    if(True in list(new_df.iloc[-1].isnull()[:])): # Sometimes the last row of a dataframe has null values - happened only once, no idea how it came there, but somehow a null row gets added. So in case any such thing exists, get the last row index and remove it.
        drop_ind=new_df.index[-1] # Get the last row's index value
        new_df.drop(drop_ind,axis=0,inplace=True) # Drop the last row by it's index value
    
    new_df.sort_index(inplace=True) # And sort the dataframe just to be sure it's in right order after dropping

    r=feats[3] #3 because of east north corrd input  - this line you get R pixel column value 
    g=feats[4] # Get G pixel column name
    b=feats[5] # Get B pixel column name  
    # check if 0 value is present in r,g,b
    if(new_df.describe()[r]['min']==0):     # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator               
        new_df[r]=np.where(new_df[r]==0,0.1,new_df[r])
    if(new_df.describe()[g]['min']==0):     # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                            
        new_df[g]=np.where(new_df[g]==0,0.1,new_df[g])
    if(new_df.describe()[b]['min']==0):     # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator              
        new_df[b]=np.where(new_df[b]==0,0.1,new_df[b])

    if(new_df.describe()[r]['min']<0):  # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                 
        new_df[r]=np.where(new_df[r]<0,0.1,new_df[r])
    if(new_df.describe()[g]['min']<0):  # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                 
        new_df[g]=np.where(new_df[g]<0,0.1,new_df[g])
    if(new_df.describe()[b]['min']<0):  # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                
        new_df[b]=np.where(new_df[b]<0,0.1,new_df[b])

    csp_str=''
    colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    print(colourspaces)
    if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
        print('same dataset - no hsv only rgb')
        csp_str='rgb' # No changes needed, rgb values already there

    elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
        print('same dataset - no hsv only rgb')
        csp_str='rgb' # No changes needed, rgb values already there

    elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
        print('change in dataset - no rgb only hsv')
        hsv_vals = new_df[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        df2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
        new_df[r] = df2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        new_df[g] = df2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        new_df[b] = df2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        new_df = new_df.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
        for ix, valx in enumerate(feats): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
            if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                feats[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
            if(valx == g):
                feats[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
            if(valx == b):
                feats[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
        
        # check if 0 value is present in r,g,b
        if(new_df.describe()['H_generated']['min']==0):  # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                  
            new_df['H_generated']=np.where(new_df['H_generated']==0,0.1,new_df['H_generated'])
        if(new_df.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                  
            new_df['S_generated']=np.where(new_df['S_generated']==0,0.1,new_df['S_generated'])
        if(new_df.describe()['V_generated']['min']==0):    # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator               
            new_df['V_generated']=np.where(new_df['V_generated']==0,0.1,new_df['V_generated'])       

        del df2 # Delete the temporary dataframe
        csp_str='hsv' # Make the value of this string to just hsv

    elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
        print('change in dataset - rgb and hsv')
        hsv_vals = new_df[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        df2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
        new_df.insert(6, 'H_generated', df2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        new_df.insert(7, 'S_generated', df2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        new_df.insert(8, 'V_generated', df2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        #insert in feats 
        feats.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        feats.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        feats.insert(8, 'V_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

        # check if 0 value is present in r,g,b
        if(new_df.describe()['H_generated']['min']==0):  # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                    
            new_df['H_generated']=np.where(new_df['H_generated']==0,0.1,new_df['H_generated'])
        if(new_df.describe()['S_generated']['min']==0):  # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                  
            new_df['S_generated']=np.where(new_df['S_generated']==0,0.1,new_df['S_generated'])
        if(new_df.describe()['V_generated']['min']==0):  # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                  
            new_df['V_generated']=np.where(new_df['V_generated']==0,0.1,new_df['V_generated'])       

        del df2 # Delete the temporary dataframe
        csp_str='rgbhsv' # Make the value of this string to both rgbhsv

    csp_csv_df=pd.DataFrame([csp_str],columns=['colourspaces'])    # Create a new dataframe with column 'colourspaces' and with value of the string created after creating and finalising colourspace values
    csp_csv_df.to_pickle('model_files/'+'colourspaces.pkl') # Save the colourspace selected option as pkl file to be used later


    new_df.to_csv('model_files/'+'temp_pipeline_2.csv',index=False) # Save the processed data as csv
    new_df=new_df.round(3)   #round the values to 3 decimal and send the first 5 rows back to javascript
    jsonfiles = json.loads(new_df.head().to_json(orient='records')) # Convert first 5 rows of the dataframe to json object
    del new_df # Delete another temporary dataframe
    return (jsonify({ 'jsonfiles': jsonfiles, 'feats': list(feats) }) )



#### SECTION 6: FUNCTIONS FOR TRAINING CURVE-FITTING OBJECTIVE EQUATIONS AND APPLYING FOUR DIFFERENT VALIDATION STRATEGIES 

###################################################  MODULE 2 - CURVE FITTING FUNCTIONS + SOME MODULE 3 FUNCTIONS ABOVE ####################################################


@app.route('/curve_fitting')
def curve_fitting(): # Load the curve fitting module page
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-04-05 16:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('curve_fitting.html',title='Curve Fitting') # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error

        

def _create_divlogln_features(data,r,g,b,feats): # Creating all log combinations of the features - separately for three options depending on the choice of selected colourspace values 
    print('colorspace',feats)
    if( ('hsv' not in feats and 'rgb' in feats) or ('hsv' not in feats and 'rgb' not in feats)): # Feats variable contains what colourspace has been selected. If none of the two colourspaces is selected, then use only rgb colourspace.  Else use the selected colourspace value
        data['r/g']='' # Initiate empty column
        data['r/b']='' # Initiate empty column
        data['g/r']='' # Initiate empty column
        data['g/b']='' # Initiate empty column
        data['b/r']='' # Initiate empty column
        data['b/g']='' # Initiate empty column
        data['ln(r/g)']='' # Initiate empty column
        data['ln(r/b)']='' # Initiate empty column
        data['ln(g/r)']='' # Initiate empty column
        data['ln(g/b)']='' # Initiate empty column
        data['ln(b/r)']=''
        data['ln(b/g)']='' # Initiate empty column
        data['log10(r/g)']='' # Initiate empty column
        data['log10(r/b)']='' # Initiate empty column
        data['log10(g/r)']='' # Initiate empty column
        data['log10(g/b)']='' # Initiate empty column
        data['log10(b/r)']='' # Initiate empty column
        data['log10(b/g)']='' # Initiate empty column

        data['r/g']=data[r]/data[g] # Quickly calculate r/g for all rows and save the values to the column
        data['r/b']=data[r]/data[b] # Quickly calculate r/b for all rows and save the values to the column
        data['g/r']=data[g]/data[r] # Quickly calculate g/r for all rows and save the values to the column
        data['g/b']=data[g]/data[b] # Quickly calculate g/b for all rows and save the values to the column
        data['b/r']=data[b]/data[r] # Quickly calculate b/r for all rows and save the values to the column
        data['b/g']=data[b]/data[g] # Quickly calculate b/g for all rows and save the values to the column
        data['ln(r/g)']=np.log(data[r]/data[g]) # Quickly calculate ln(r/g) for all rows and save the values to the column
        data['ln(r/b)']=np.log(data[r]/data[b]) # Quickly calculate ln(r/b) for all rows and save the values to the column
        data['ln(g/r)']=np.log(data[g]/data[r]) # Quickly calculate ln(g/r) for all rows and save the values to the column
        data['ln(g/b)']=np.log(data[g]/data[b]) # Quickly calculate ln(g/b) for all rows and save the values to the column
        data['ln(b/r)']=np.log(data[b]/data[r]) # Quickly calculate ln(b/r) for all rows and save the values to the column
        data['ln(b/g)']=np.log(data[b]/data[g]) # Quickly calculate ln(b/g) for all rows and save the values to the column
        data['log10(r/g)']=np.log10(data[r]/data[g]) # Quickly calculate log10(r/g) for all rows and save the values to the column
        data['log10(r/b)']=np.log10(data[r]/data[b]) # Quickly calculate log10(r/b) for all rows and save the values to the column
        data['log10(g/r)']=np.log10(data[g]/data[r]) # Quickly calculate log10(g/r) for all rows and save the values to the column
        data['log10(g/b)']=np.log10(data[g]/data[b]) # Quickly calculate log10(g/b) for all rows and save the values to the column
        data['log10(b/r)']=np.log10(data[b]/data[r]) # Quickly calculate log10(b/r) for all rows and save the values to the column
        data['log10(b/g)']=np.log10(data[b]/data[g]) # Quickly calculate log10(b/g) for all rows and save the values to the column

        ln_cols=['ln(r/g)','ln(r/b)','ln(g/r)','ln(g/b)','ln(b/r)','ln(b/g)','log10(g/r)','log10(b/r)', 'log10(b/g)', 'log10(r/g)', 'log10(r/b)', 'log10(g/b)'] # Store all the newly created column names to a list

    if( 'hsv' in feats and 'rgb' not in feats): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
        data['h/s']='' # Initiate empty column
        data['h/v']='' # Initiate empty column
        data['s/h']='' # Initiate empty column
        data['s/v']='' # Initiate empty column
        data['v/h']='' # Initiate empty column
        data['v/s']='' # Initiate empty column
        data['ln(h/s)']='' # Initiate empty column
        data['ln(h/v)']='' # Initiate empty column
        data['ln(s/h)']='' # Initiate empty column
        data['ln(s/v)']='' # Initiate empty column
        data['ln(v/h)']='' # Initiate empty column
        data['ln(v/s)']='' # Initiate empty column
        data['log10(h/s)']='' # Initiate empty column
        data['log10(h/v)']='' # Initiate empty column
        data['log10(s/h)']='' # Initiate empty column
        data['log10(s/v)']='' # Initiate empty column
        data['log10(v/h)']='' # Initiate empty column
        data['log10(v/s)']='' # Initiate empty column

        data['h/s']=data['H_generated']/data['S_generated'] # Quickly calculate h/s for all rows and save the values to the column
        data['h/v']=data['H_generated']/data['V_generated'] # Quickly calculate h/v for all rows and save the values to the column
        data['s/h']=data['S_generated']/data['H_generated'] # Quickly calculate s/h for all rows and save the values to the column
        data['s/v']=data['S_generated']/data['V_generated'] # Quickly calculate s/v for all rows and save the values to the column
        data['v/h']=data['V_generated']/data['H_generated'] # Quickly calculate v/h for all rows and save the values to the column
        data['v/s']=data['V_generated']/data['S_generated'] # Quickly calculate v/s for all rows and save the values to the column
        data['ln(h/s)']=np.log(data['H_generated']/data['S_generated']) # Quickly calculate ln(h/s) for all rows and save the values to the column
        data['ln(h/v)']=np.log(data['H_generated']/data['V_generated']) # Quickly calculate ln(h/v) for all rows and save the values to the column
        data['ln(s/h)']=np.log(data['S_generated']/data['H_generated']) # Quickly calculate ln(s/h) for all rows and save the values to the column
        data['ln(s/v)']=np.log(data['S_generated']/data['V_generated']) # Quickly calculate ln(s/v) for all rows and save the values to the column
        data['ln(v/h)']=np.log(data['V_generated']/data['H_generated']) # Quickly calculate ln(v/h) for all rows and save the values to the column
        data['ln(v/s)']=np.log(data['V_generated']/data['S_generated']) # Quickly calculate ln(v/s) for all rows and save the values to the column
        data['log10(h/s)']=np.log10(data['H_generated']/data['S_generated']) # Quickly calculate log10(h/s) for all rows and save the values to the column
        data['log10(h/v)']=np.log10(data['H_generated']/data['V_generated']) # Quickly calculate log10(h/v) for all rows and save the values to the column
        data['log10(s/h)']=np.log10(data['S_generated']/data['H_generated']) # Quickly calculate log10(s/h) for all rows and save the values to the column
        data['log10(s/v)']=np.log10(data['S_generated']/data['V_generated']) # Quickly calculate log10(s/v) for all rows and save the values to the column
        data['log10(v/h)']=np.log10(data['V_generated']/data['H_generated']) # Quickly calculate log10(v/h) for all rows and save the values to the column
        data['log10(v/s)']=np.log10(data['V_generated']/data['S_generated']) # Quickly calculate log10(v/s) for all rows and save the values to the column

        ln_cols= ['ln(h/s)','ln(h/v)','ln(s/h)','ln(s/v)','ln(v/h)','ln(v/s)','log10(s/h)','log10(v/h)', 'log10(v/s)', 'log10(h/s)', 'log10(h/v)', 'log10(s/v)'] # Store all the newly created column names to a list
    
    if( 'hsv' in feats and 'rgb' in feats): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
        data['r/g']='' # Initiate empty column
        data['r/b']='' # Initiate empty column
        data['g/r']='' # Initiate empty column
        data['g/b']='' # Initiate empty column
        data['b/r']='' # Initiate empty column
        data['b/g']='' # Initiate empty column
        data['ln(r/g)']='' # Initiate empty column
        data['ln(r/b)']='' # Initiate empty column
        data['ln(g/r)']='' # Initiate empty column
        data['ln(g/b)']='' # Initiate empty column
        data['ln(b/r)']='' # Initiate empty column
        data['ln(b/g)']='' # Initiate empty column
        data['log10(r/g)']='' # Initiate empty column
        data['log10(r/b)']='' # Initiate empty column
        data['log10(g/r)']='' # Initiate empty column
        data['log10(g/b)']='' # Initiate empty column
        data['log10(b/r)']='' # Initiate empty column
        data['log10(b/g)']='' # Initiate empty column

        data['r/g']=data[r]/data[g] # Quickly calculate r/g for all rows and save the values to the column
        data['r/b']=data[r]/data[b] # Quickly calculate r/b for all rows and save the values to the column
        data['g/r']=data[g]/data[r] # Quickly calculate g/r for all rows and save the values to the column
        data['g/b']=data[g]/data[b] # Quickly calculate g/b for all rows and save the values to the column
        data['b/r']=data[b]/data[r] # Quickly calculate b/r for all rows and save the values to the column
        data['b/g']=data[b]/data[g] # Quickly calculate b/g for all rows and save the values to the column
        data['ln(r/g)']=np.log(data[r]/data[g]) # Quickly calculate ln(r/g) for all rows and save the values to the column
        data['ln(r/b)']=np.log(data[r]/data[b]) # Quickly calculate ln(r/b) for all rows and save the values to the column
        data['ln(g/r)']=np.log(data[g]/data[r]) # Quickly calculate ln(g/r) for all rows and save the values to the column
        data['ln(g/b)']=np.log(data[g]/data[b]) # Quickly calculate ln(g/b) for all rows and save the values to the column
        data['ln(b/r)']=np.log(data[b]/data[r]) # Quickly calculate ln(b/r) for all rows and save the values to the column
        data['ln(b/g)']=np.log(data[b]/data[g]) # Quickly calculate ln(b/g) for all rows and save the values to the column
        data['log10(r/g)']=np.log10(data[r]/data[g]) # Quickly calculate log10(r/g) for all rows and save the values to the column
        data['log10(r/b)']=np.log10(data[r]/data[b]) # Quickly calculate log10(r/b) for all rows and save the values to the column
        data['log10(g/r)']=np.log10(data[g]/data[r]) # Quickly calculate log10(g/r) for all rows and save the values to the column
        data['log10(g/b)']=np.log10(data[g]/data[b]) # Quickly calculate log10(g/b) for all rows and save the values to the column
        data['log10(b/r)']=np.log10(data[b]/data[r]) # Quickly calculate log10(b/r) for all rows and save the values to the column
        data['log10(b/g)']=np.log10(data[b]/data[g]) # Quickly calculate log10(b/g) for all rows and save the values to the column

        ln_cols=['ln(r/g)','ln(r/b)','ln(g/r)','ln(g/b)','ln(b/r)','ln(b/g)','log10(g/r)','log10(b/r)', 'log10(b/g)', 'log10(r/g)', 'log10(r/b)', 'log10(g/b)'] # Store all the newly created rgb based log combination column names to a list

        data['h/s']='' # Initiate empty column
        data['h/v']='' # Initiate empty column
        data['s/h']='' # Initiate empty column
        data['s/v']='' # Initiate empty column
        data['v/h']='' # Initiate empty column
        data['v/s']='' # Initiate empty column
        data['ln(h/s)']='' # Initiate empty column
        data['ln(h/v)']='' # Initiate empty column
        data['ln(s/h)']='' # Initiate empty column
        data['ln(s/v)']='' # Initiate empty column
        data['ln(v/h)']='' # Initiate empty column
        data['ln(v/s)']='' # Initiate empty column
        data['log10(h/s)']='' # Initiate empty column
        data['log10(h/v)']='' # Initiate empty column
        data['log10(s/h)']='' # Initiate empty column
        data['log10(s/v)']='' # Initiate empty column
        data['log10(v/h)']='' # Initiate empty column
        data['log10(v/s)']='' # Initiate empty column

        data['h/s']=data['H_generated']/data['S_generated'] # Quickly calculate h/s for all rows and save the values to the column
        data['h/v']=data['H_generated']/data['V_generated'] # Quickly calculate h/v for all rows and save the values to the column
        data['s/h']=data['S_generated']/data['H_generated'] # Quickly calculate s/h for all rows and save the values to the column
        data['s/v']=data['S_generated']/data['V_generated'] # Quickly calculate s/v for all rows and save the values to the column
        data['v/h']=data['V_generated']/data['H_generated'] # Quickly calculate v/h for all rows and save the values to the column
        data['v/s']=data['V_generated']/data['S_generated'] # Quickly calculate v/s for all rows and save the values to the column
        data['ln(h/s)']=np.log(data['H_generated']/data['S_generated']) # Quickly calculate ln(h/s) for all rows and save the values to the column
        data['ln(h/v)']=np.log(data['H_generated']/data['V_generated']) # Quickly calculate ln(h/v) for all rows and save the values to the column
        data['ln(s/h)']=np.log(data['S_generated']/data['H_generated']) # Quickly calculate ln(s/h) for all rows and save the values to the column
        data['ln(s/v)']=np.log(data['S_generated']/data['V_generated']) # Quickly calculate ln(s/v) for all rows and save the values to the column
        data['ln(v/h)']=np.log(data['V_generated']/data['H_generated']) # Quickly calculate ln(v/h) for all rows and save the values to the column
        data['ln(v/s)']=np.log(data['V_generated']/data['S_generated']) # Quickly calculate ln(v/s) for all rows and save the values to the column
        data['log10(h/s)']=np.log10(data['H_generated']/data['S_generated']) # Quickly calculate log10(h/s) for all rows and save the values to the column
        data['log10(h/v)']=np.log10(data['H_generated']/data['V_generated']) # Quickly calculate log10(h/v) for all rows and save the values to the column
        data['log10(s/h)']=np.log10(data['S_generated']/data['H_generated']) # Quickly calculate log10(s/h) for all rows and save the values to the column
        data['log10(s/v)']=np.log10(data['S_generated']/data['V_generated']) # Quickly calculate log10(s/v) for all rows and save the values to the column
        data['log10(v/h)']=np.log10(data['V_generated']/data['H_generated']) # Quickly calculate log10(v/h) for all rows and save the values to the column
        data['log10(v/s)']=np.log10(data['V_generated']/data['S_generated']) # Quickly calculate log10(v/s) for all rows and save the values to the column

        ln_cols2= ['ln(h/s)','ln(h/v)','ln(s/h)','ln(s/v)','ln(v/h)','ln(v/s)','log10(s/h)','log10(v/h)', 'log10(v/s)', 'log10(h/s)', 'log10(h/v)', 'log10(s/v)'] # Store all the newly created hsv based log combination column names to a list
        for i in ln_cols2: # For all column names in hsv based log combination column names list
            ln_cols.append(i) # Append the column names to the rgb based log combination column names list
    
    
    return data, ln_cols # Return updated dataset and combination column names - based on the selected colourspace option

## Define different curve fitting objective functions
#objective function
def objective_1(x,a,b):
	return (a*x) + b

def objective_2(x,a,b,c):
	return (a*x**2) + (b*x) + c

def objective_3(x,a,b,c):
	return a + ( b * np.exp(-c*x) )

# def objective_4(x, a, b, c, d, e):
# 	return a + ( ( b-c )/( 1 + (x/d)**e ) )

# def objective_5(x, a, b, c):
# 	return a + ( b * sin(c - x) )

def objective_4(x, a, b):
	return a + (b * np.log(x)) 

def r2_score_cus(y1,y2):
	y1=pd.DataFrame(y1,columns=['col'])
	y2=pd.DataFrame(y2,columns=['col'])

	return y1['col'].corr(y2['col']) 


## Training and validation metrics function for curve-fitting functions

def train_metrics_cf(y_array,y_line_array, column_name): # Inputs the model variable, train data, train target values, and the name of the model to calculate and return training metric results as a dataframe

    y= pd.DataFrame(y_array,columns=['col'])    # Save the actual training-target array values to dataframe
    y_line= pd.DataFrame(y_line_array, columns=['col']) # Save the prediction array values to dataframe
    col1="col" # Column name

    r2s= r2_score_cus( y_array, y_line_array ) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse( y[col1],y_line[col1] )) #calculate rmse metric score
    maes=mean_absolute_error( y[col1],y_line[col1] ) #calculate mae metric score
    evars=explained_variance_score( y[col1],y_line[col1] ) #calculate variance metric score
    maxs=max_error( y[col1],y_line[col1] ) #calculate max error metric score
    tmp=[] # Create an empty list
    tmp.append([column_name,rmses,maes,r2s,maxs,evars]) # Join all metric values into a list


    final_scores_train=pd.DataFrame(tmp,columns=['1. Feature Name ','2. Root Mean Squared Error(RMSE)','3. Mean Absolute Error(MAE)','4. Pearson Correlation Coefficient','5. Maximum Residual Error','6. Explained variance Score'])    # Convert the metric values list to a dataframe with proper column names  
    return final_scores_train # Return the training metric table

def validation_metrics_cf(y_array,y_line_array, column_name): # Inputs the model variable, validation data, validation target values, and the name of the model to calculate and return validation metric results as a dataframe

    y= pd.DataFrame(y_array,columns=['col'])      # Save the actual validation-target array values to dataframe
    y_line= pd.DataFrame(y_line_array, columns=['col']) # Save the prediction array values to dataframe
    col1="col"

    r2s= r2_score_cus( y_array, y_line_array ) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse( y[col1],y_line[col1] )) #calculate rmse metric score
    maes=mean_absolute_error( y[col1],y_line[col1] ) #calculate mae metric score
    evars=explained_variance_score( y[col1],y_line[col1] ) #calculate variance metric score
    maxs=max_error( y[col1],y_line[col1] ) #calculate max error metric score
    tmp=[] # Create an empty list
    tmp.append([column_name,rmses,maes,r2s,maxs,evars]) # Join all metric values into a list


    final_scores_train=pd.DataFrame(tmp,columns=['1. Feature Name ','2. Validation_Root Mean Squared Error(RMSE)','3. Validation_Mean Absolute Error(MAE)','4. Validation_Pearson Correlation Coefficient','5. Validation_Maximum Residual Error','6. Validation_Explained variance Score'])  # Convert the metric values list to a dataframe with proper column names     
    return final_scores_train # Return the training metric table


def test_metrics_cf(model,x_test,y_test,model_name): ## Non-functional 
    predictions_all=pd.DataFrame(y_test)     
    y_pred_model=model.predict(x_test)
    col1=predictions_all.columns.tolist()[0]
    predictions_all['Model_full_Pred']=y_pred_model
    r2s=predictions_all[col1].corr(predictions_all['Model_full_Pred']) # coeff of correlation only - (r) - gives better insight to negative correlations. And do not use r2_score of sklearn - i use .corr() - which is linear fit of squares method and not residual variance and input data variance comparison
    rmses=np.sqrt(mse(predictions_all[col1],predictions_all['Model_full_Pred']))
    maes=mean_absolute_error(predictions_all[col1],predictions_all['Model_full_Pred'])
    evars=explained_variance_score(predictions_all[col1],predictions_all['Model_full_Pred'])
    maxs=max_error(predictions_all[col1],predictions_all['Model_full_Pred'])
    tmp=[]
   # print("test",tmp)
    tmp.append([model_name,rmses,maes,r2s,maxs,evars])
    final_scores_test=pd.DataFrame(tmp,columns=['1. Model Name','2. Validation_Root Mean Squared Error(RMSE)','3. Validation_Mean Absolute Error(MAE)','4. Validation_Pearson Correlation Coefficient','5. Validation_Maximum Residual Error','6. Validation_Explained variance Score'])    
    return final_scores_test

#curve fit

## Curve fit functions with different four different validation strategies

# 1. Random validation points
@app.route('/ml_algorithms_random_vali_cf',methods=['GET','POST'])
def ml_algorithms_random_vali_cf():

    feats=request.form['sel_feat'] # Get selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to string

    tts_value_obj=request.form['tts_value'] # Get the train test split value as json object
    tr_split=json.loads(tts_value_obj) # Convert the json object to string
    print(tr_split)
    
    original_feats_train=[]  # Initialize empty list to save the original features used for training - access later in full-data part
    # Make sure full river also has same column names
    for i in range(len(sel_cols1)): # For all values in the sel_cols1
        original_feats_train.append([str(sel_cols1[i])]) # Append every column name to the list
    original_feats_train_df=pd.DataFrame(original_feats_train,columns=['feats'])    # Create a dataframe with column name 'feats' and store the original_feats_train list values 
    original_feats_train_df.to_csv('model_files/'+'original_feats_train.csv',index=False) # Save the dataframe as csv selected to use later
    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    data=load_data('x_y.csv') # Load this csv which was stored aftr input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe
    spl_data=pd.read_csv('model_files/temp_pipeline_2.csv') # Read the processed input data
    x=spl_data[sel_cols1[3:-1]].copy() # Get the processed data without target values - (last column name in the list is the target column name)

    colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    # create log, ln features
    x_data, ln_cols = _create_divlogln_features(x,r,g,b,colourspaces) # Create log combinations - call this function and get updated dataset and also get the new column names that are added
    x_data.to_csv('model_files/x_data_logln.csv',index=False) # Save this updated input data as csv to use later
    # Split the revised dataset
    test_split=(100-int(tr_split))/100 # Calculate the validation split value
    x_train,x_test,y_train,y_test=tts(x_data,y,test_split) # Since this is random validation, we split the data into train and validation sets on random indices

    x_train4,x_test4,y_train4,y_test4=tts(spl_data,y,test_split) # We split the non-updated input data (spl_data is the dataset which was read from csv ) - we split and store them as training and validation datasets. We save the spl_data instead of latest updated x_data because the latter consists of all log combinations which is not needed while storing csv file.

    #Save train and validation csv files - not processed files, but the same indices from original dataset (just with selected columns)
    x_train4.to_csv('model_files/Curve-fit_Training-data_file.csv', index=False) # Save as training data file
    x_test4.to_csv('model_files/Curve-fit_Validation-data_file.csv', index=False) # Save as validation data file

    col_max=[] # Initialize empty list
    y_train_arr= np.array(y_train.values, dtype=float) # Convert the training target values from dataframe to an array of float type
    y_test_arr= np.array(y_test.values, dtype=float ) # Convert the validation target values from dataframe to an array of float type
    
    for col in x_train.columns: # For each feature in the processed dataset, fit on all 4 objectives and store their r2 score 
        print(col)
        r2_score_list=[]
        obj_score={}
        x_train_arr=np.array(x_train[col].values,dtype=float) # Convert the training dataframe to an array of float type - curve fitting works only on numpy arrays
        # x_train_arr = x_train_arr[(np.isnan(x_train_arr) == False) & (np.isinf(x_train_arr) == False)]
        paras, unkn=curve_fit(objective_1,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
        a,b=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_1(x_train_arr, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_2,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 2 to get an equation for training data and target values 
        a,b,c=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_2(x_train_arr, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_3,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 3 to get an equation for training data and target values 
        a,b,c=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_3(x_train_arr, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        # if(col not in ln_cols):
        # paras, unkn=curve_fit(objective_4,x,y, maxfev=100000)
        # a,b,c,d,e=paras
        # r2_score_list.append( r2_score_cus( y, objective_4(x, a, b, c, d, e)) )

        # paras, unkn=curve_fit(objective_5,x,y, maxfev=100000)
        # a,b,c=paras
        # r2_score_list.append( r2_score_cus( y, objective_5(x, a, b, c)) )

        if(col not in ln_cols): # Do this objective 4 only if the column is not a log-combination column. Because objective 4 takes log of input values, and giving already computed log values to this function does not suit well - also you get error. So do this function only for non-log columns 
            paras, unkn=curve_fit(objective_4,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 4 to get an equation for training data and target values 
            a,b=paras # Get the parameters / coeefficients
            r2_score_list.append( r2_score_cus( y_train_arr, objective_4(x_train_arr, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values


        max_indx=r2_score_list.index(max(r2_score_list))+1 # Get the index of the best r2 score from the stored r2 scores list for every feaure

        max_obj_str="objective_"+str(max_indx) # Get the name of the objective that gave highest r2 score 

        col_max.append({'column':str(col), 'objective':max_obj_str, 'score': max(r2_score_list)}) # Append the feaure name, its max r2 score objective name, max r2 score into a dict

    print(len(col_max))

    # top 5 curve fit for TRAIN data
    top_5 = sorted(enumerate(col_max), key=lambda x: x[1]['score'], reverse=True)[:5] # Out of all max r2 scores for every feature, get the top 5 r2 scores and its feature name - by ordering the dict in descending order based on r2 scores
    
    r2_top_5=[] # Initialize empty list
    eq_top_5=[] # Initialize empty list
    col_top_5=[] # Initialize empty list
    obj_top_5=[] # Initialize empty list
    paras_l=[] # Initialize empty list
    x_full=[] # Initialize empty list
    y_full=[] # Initialize empty list

    count=0
    for i in top_5: # For this descending order dict, i is the objective name, fit that objective function to training data again - and this time use that trained equation to predict on training data, whole input data and on validation data
        count+=1 # Keep counter - to run 5 objectives next
        # for x_train and x_test , get predictions and scores
        x_train_arr = np.array( x_train[i[1]['column']].values,dtype=float ) # Get the ith column from the training dataset and conver to float array
        x_test_arr = np.array( x_test[i[1]['column']].values,dtype=float ) # Get the ith column from the validation dataset and conver to float array
        x_full= np.array( x_data[i[1]['column']].values,dtype=float ) # Get the ith column from the full-input dataset and conver to float array
        
        col_name=str(i[1]['column']) # Get the ith column name from the dict 
        if( i[1]['objective']=="objective_1" ): # If it is objective 1 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_1,x_train_arr,y_train_arr, maxfev=100000)
            a,b=paras
            y_line = objective_1(x_train_arr, a, b)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 1 is ", r2_score_now )
            eq= str(round(a,3))+ '*x' + ' + ' + str(round(b,3)) # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_1(x_test_arr, a, b)
            y_full= objective_1( x_full, a, b)
            paras_l.append( [a,b] )
            

        if( i[1]['objective']=="objective_2" ): # If it is objective 2 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_2,x_train_arr,y_train_arr, maxfev=100000)
            a,b,c=paras
            y_line = objective_2(x_train_arr, a, b, c)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], "2 is ", r2_score_now )
            eq= ( str(round(a,3)) + '*x^2 ') + ' +'  + ( str(round(b,3)) + '*x') + ' + ' + str(round(c,3)) # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_2(x_test_arr, a, b, c)
            y_full= objective_2( x_full, a, b, c)
            paras_l.append( [a,b,c] )

        if( i[1]['objective']=="objective_3" ): # If it is objective 3 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_3,x_train_arr,y_train_arr, maxfev=100000)
            a,b,c=paras
            y_line = objective_3(x_train_arr, a, b, c)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 3 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + ( str(round(b,3)) + '* e^(' + str(-round(c,3)) + '*x)' ) + ' )' # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_3(x_test_arr, a, b, c)
            y_full= objective_3( x_full, a, b, c)
            paras_l.append( [a,b,c] )

        # if( i[1]['objective']=="objective_4" ): 
        #     paras, unkn=curve_fit(objective_4,x,y, maxfev=100000)
        #     a,b,c,d,e=paras
        #     y_line = objective_4(x, a, b, c, d, e)
        #     r2_score_now= r2_score_cus( y, y_line)
        #     print(" r2 score for ",i[1], " 4 is ", r2_score_now )
        #     eq= str(round(a,3)) + ' + ' + '(' + str(b-c) + ')' + '/' + '( 1 + (x'  + '/' + str(d) + ')^' + str(e) + ') )'   
        #     paras_l.append( [a,b,c,d,e] )
        #     print(y_line)

        # if( i[1]['objective']=="objective_5" ):
        #     paras, unkn=curve_fit(objective_5,x,y, maxfev=100000)
        #     a,b,c=paras
        #     y_line = objective_5(x, a, b, c)
        #     r2_score_now= r2_score_cus( y, y_line)
        #     print(" r2 score for ",i[1], " 5 is ", r2_score_now )
        #     eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + '*' + 'sin(' + str(round(c,3)) + '-x) )'
        #     paras_l.append( [a,b,c] )

        if( i[1]['objective'] == "objective_4" ): # If it is objective 4 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_4,x_train_arr,y_train_arr, maxfev=100000)
            a,b=paras
            y_line = objective_4(x_train_arr, a, b)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 4 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + ' * ' + 'log(x) )' # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_4(x_test_arr, a, b)
            y_full= objective_4( x_full, a, b)
            paras_l.append( [a,b] )
		
        if(count==1): # for the count value
            x_1=x_full
            y_1=y_full # Save the full input data predictions
            y_train_1=y_line # Save the training data predictions
            y_test_1=y_line_test # Save the validation data predictions
        if(count==2): # for the count value
            x_2=x_full
            y_2=y_full # Save the full input data predictions
            y_train_2=y_line # Save the training data predictions
            y_test_2=y_line_test # Save the validation data predictions
        if(count==3): # for the count value
            x_3=x_full
            y_3=y_full # Save the full input data predictions
            y_train_3=y_line # Save the training data predictions
            y_test_3=y_line_test # Save the validation data predictions
        if(count==4): # for the count value
            x_4=x_full
            y_4=y_full # Save the full input data predictions
            y_train_4=y_line # Save the training data predictions
            y_test_4=y_line_test # Save the validation data predictions
        if(count==5):  # for the count value
            x_5=x_full
            y_5=y_full # Save the full input data predictions
            y_train_5=y_line # Save the training data predictions
            y_test_5=y_line_test # Save the validation data predictions

        r2_top_5.append(round(r2_score_now, 3)) # Round the r2 metric score and append to the top 5 r2 scores list
        eq_top_5.append(str(eq)) # Round the equation string and append to the top 5 equations list
        col_top_5.append(col_name) # Append the column name to the top 5 column names list
        obj_top_5.append(str(i[1]['objective'])) # Append the objective name to the top 5 objective names list
        print(col_top_5)
    
    print(paras_l)
    #save top5 col name and objective name to csv
    save_top_5=pd.DataFrame( columns=['Column_name','Objective_name','Paras'], index=range(5)) # After the all the top 5 objective and scores are computed, store them to a dataframe
    for i in range(0,5): # For 5 loops, store each of the top-5 column name, objective name and their parameter values to dataframe for later use
        save_top_5['Column_name'][i]=col_top_5[i]
        save_top_5['Objective_name'][i]=obj_top_5[i]
        save_top_5['Paras'][i]=paras_l[i]
    print(save_top_5)

    save_top_5.to_csv('model_files/'+'Save_top_5_cf.csv',index=False)       # Save the top-5 equation details dataframe as csv   
    
    # Metrics for curve-fit train data 
    tm1=train_metrics_cf( y_train_arr, y_train_1, save_top_5['Column_name'][0] )
    tm2=train_metrics_cf( y_train_arr, y_train_2, save_top_5['Column_name'][1] )
    tm3=train_metrics_cf( y_train_arr, y_train_3, save_top_5['Column_name'][2] )
    tm4=train_metrics_cf( y_train_arr, y_train_4, save_top_5['Column_name'][3] )
    tm5=train_metrics_cf( y_train_arr, y_train_5, save_top_5['Column_name'][4] )

    # Metrics for curve-fit valkdation data 
    te1=validation_metrics_cf( y_test_arr, y_test_1, save_top_5['Column_name'][0] )
    te2=validation_metrics_cf( y_test_arr, y_test_2, save_top_5['Column_name'][1] )
    te3=validation_metrics_cf( y_test_arr, y_test_3, save_top_5['Column_name'][2] )
    te4=validation_metrics_cf( y_test_arr, y_test_4, save_top_5['Column_name'][3] )
    te5=validation_metrics_cf( y_test_arr, y_test_5, save_top_5['Column_name'][4] )

    tms=[tm1,tm2,tm3,tm4,tm5] # Append all training metric scores to list
    final_train_metrics=pd.concat(tms,axis=0) # Concat all list values into a single training metric score datafame
    final_train_metrics=final_train_metrics.round(3)  # Round the metrics to 3 decimals
    jsonfiles_tms = json.loads(final_train_metrics[tm1.columns].to_json(orient='records')) # Save the training metrics as json object to show to the user using javascript 
    final_train_metrics.to_csv('model_files/'+'curve_fitting_train_statistics.txt',index=False) # Save the training metrics scores as txt file
    
    tes=[te1,te2,te3,te4,te5] # Append all validation metric scores to list
    final_test_metrics=pd.concat(tes,axis=0) # Concat all list values into a single validation metric score datafame
    final_test_metrics=final_test_metrics.round(3)  # Round the metrics to 3 decimals
    jsonfiles_tes = json.loads(final_test_metrics[te1.columns].to_json(orient='records'))  # Save the validation metrics as json object to show to the user using javascript 
    final_test_metrics.to_csv('model_files/'+'curve_fitting_validation_statistics.txt',index=False) # Save the validation metrics scores as txt file

    # Only train preds
    train_predictions=pd.DataFrame()
    train_predictions['Curve_1']=y_train_1 # Save the first curve training data's predictions
    train_predictions['Curve_2']=y_train_2 # Save the second curve training data's predictions
    train_predictions['Curve_3']=y_train_3 # Save the third curve training data's predictions
    train_predictions['Curve_4']=y_train_4 # Save the fourth curve training data's predictions
    train_predictions['Curve_5']=y_train_5 # Save the fifth curve training data's predictions
    train_predictions.to_csv('model_files/'+'Train_predictions.csv',index=False) # Save the dataframe as csv

    # predictions for vali and input
    vali_predictions_all=pd.DataFrame()
    vali_predictions_all[sel_cols1[-1]]=y_test # Save the actual validation target values to the dataframe
    vali_predictions_all[east]=x_test4[east].copy() # Then save the east coordinate values in the validation data
    vali_predictions_all[north]=x_test4[north].copy() # Then save the north coordinate values in the validation data
    vali_predictions_all['Curve_1']=y_test_1 # Save the first curve validation data's predictions
    vali_predictions_all['Curve_2']=y_test_2 # Save the second curve validation data's predictions
    vali_predictions_all['Curve_3']=y_test_3 # Save the third curve validation data's predictions
    vali_predictions_all['Curve_4']=y_test_4 # Save the fourth curve validation data's predictions
    vali_predictions_all['Curve_5']=y_test_5 # Save the fifth curve validation data's predictions
    vali_predictions_all=vali_predictions_all.round(3) # Round values in dataframe to 3 decimal places
    vali_predictions_all.to_csv('model_files/'+'all_models_vali_pred.csv',index=False) # Save the dataframe as csv
    # vali_predictions_all['Distance_new']=x_test4['Distance_new'].copy()

    predictions_all=pd.DataFrame()
    predictions_all[sel_cols1[-1]]=y # Save the actual target values of full-input data to the dataframe
    predictions_all['Curve_1']=y_1 # Save the first curve full-input data's predictions
    predictions_all['Curve_2']=y_2 # Save the second curve full-input data's predictions
    predictions_all['Curve_3']=y_3 # Save the third curve full-input data's predictions
    predictions_all['Curve_4']=y_4 # Save the fourth curve full-input data's predictions
    predictions_all['Curve_5']=y_5 # Save the fifth curve full-input data's predictions
    predictions_all=predictions_all.round(3) # Round values in dataframe to 3 decimal places
    predictions_all.to_csv('model_files/all_models_preds.csv',index=False) # Save the dataframe as csv
    print(predictions_all.head())
    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the full-input data's prediction dataframe
    return jsonify({ 'eq_top_5': list(eq_top_5), 'col_top_5': list(col_top_5), 'train_metric_scores' : jsonfiles_tms, 'test_metric_scores':jsonfiles_tes , 'sample_preds':jsonfiles_predictions_all })


##################### AUTO SELECT VALI CF #########################


@app.route('/load_splitfile',methods=['POST','GET']) #Load the splitfile to extract info and use it to split the dataset into train and validation
def load_splitfile():
    #first copy from downloads to current directory folder
   
    splitfile_name_obj=request.form['splitfile_name']
    model_files_name=json.loads(splitfile_name_obj)

    if( len(model_files_name) >0 ): # If there is at least one input file
        dst=str(model_files_name[0]) # Get the first file's path ( only one file needed) 
        extension=str(str(dst).split('.')[-1])     # Get the extension of the file
        print(extension)
        
        if(extension=="pkl"): # If extension match with pkl then proceed 
            df = pd.read_pickle(model_files_name[0]) # Read pkl files using read_pickle function
            id_name = str(df['ID_name'].iloc[0]) # Get the id on which the data was sorted - this is important since we have to sort the data before splitting and we should sort on same id as it was sorted when the split file was saved
            tr_split = str(df['Splitvalue'].iloc[0]) # Get the train split value to use in the next function

            print(id_name, tr_split)
            return (jsonify({ 'id_name':id_name, 'tr_split':tr_split  }))


## 2. Auto-group validation points

@app.route('/ml_algorithms_auto_select_vali_cf',methods=['GET','POST'])
def ml_algorithms_auto_select_vali_cf():

    feats=request.form['sel_feat'] # Get selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to string

    tts_value_obj=request.form['tts_value'] # Get the train test split value as json object
    tr_split=json.loads(tts_value_obj) # Convert the json object to string
    print(tr_split)
    
    original_feats_train=[] # Initialize empty list to save the original features used for training - access later in full-data part
    # Make sure full river also has same column names
    for i in range(len(sel_cols1)): # For all values in the sel_cols1
        original_feats_train.append([str(sel_cols1[i])]) # Append every column name to the list
    original_feats_train_df=pd.DataFrame(original_feats_train,columns=['feats'])  # Create a dataframe with column name 'feats' and store the original_feats_train list values    
    original_feats_train_df.to_csv('model_files/'+'original_feats_train.csv',index=False) # Save the dataframe as csv selected to use later

    # test_dataset, train_data =get_Groups_Validation(test_split, sel_cols1)
    # For identical data
    split_strat_obj=request.form['split_strat'] # Get json objec of what split strategy is used - if it is to use the same split from previously saved split file or to split into new groups 
    split_strat=json.loads(split_strat_obj) # Convert the json object to string value

    identical_inx_obj=request.form['identical_inx']  # This is the json object,  to get if the user chooses to use an identical split to an earlier run done just before the current run. User can select any one of the two options - either this or using a splitfile info
    identical_inx=json.loads(identical_inx_obj) # Convert the json object to string value

    if(split_strat == 0): # When the split strategy is not to use any old files and to make new groups
        if( str(identical_inx) == "0"): # When a fresh split needs to be done and not select the previous run's split strategy
            print("not identical")
            test_split=(100-int(tr_split)) # Calculate the validation split
            test_dataset, train_data =get_Groups_Validation(test_split, sel_cols1) # Go to its function to see how the groups are split


        elif( str(identical_inx) == "1"): # to use the previous run's split strategy and create same groups and train, validation values
            print("identical")
            #read indices from file and create data with those indices
            prev_df_train = pd.read_csv('model_files/Curve-fit_Training-data_file.csv') # Read this train-data csv file, if you are running this second time, surely there should be this file present because we save this file after every run
            prev_df_train = prev_df_train.set_index('Unnamed: 0') # Set_index creates a new column with the name 'Unnamed:0' and copies all the index values - need these index values to separate validation and training data
            prev_df_vali = pd.read_csv('model_files/Curve-fit_Validation-data_file.csv') # Read this validation-data csv file, if you are running this second time, surely there should be this file present because we save this file after every run
            current_df = load_data('temp_pipeline_2.csv') # Read the processed input data
            train_data = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( prev_df_train[sel_cols1[0]] ) ) ] # Get exact train data by matching the ids of the previous train split data and the get all those rows from input processed data having same id values
            # train_data=train_data.reset_index().drop(['index'],axis=1) 
            test_dataset = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( prev_df_vali[sel_cols1[0]] ) ) ] # Get exact validation data by matching the ids of the previous validation split data and the get all those rows from input processed data having same id values
            test_dataset = test_dataset.reset_index().drop(['index'],axis=1) 
            test_dataset['Groups'] = prev_df_vali['Groups'].copy() # Copy the group values from previous run's dataset to current dataset

    elif(split_strat == 1): # When the split strategy is to use an old file and recreate the exact groups and values
        splitfile_name_obj=request.form['splitfile_name'] # Load the json object containing the name of the split-info file
        splitfile_name=json.loads(splitfile_name_obj) # Convert the json object to string value

        df = pd.read_pickle(splitfile_name[0]) # Read pkl files using read_pickle function
        # print(list(df['Train_id'].iloc[0])[:10])
        # print(list(df['Validation_id'].iloc[0]))
        current_df = load_data('temp_pipeline_2.csv') # Read the processed input data
        train_data = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( df.iloc[0]['Train_id'] ) ) ] # Get exact train data by matching the ids of the previous train split data and the get all those rows from input processed data having same id values
        # train_data = train_data.set_index( list(df.iloc[0]['Unnamed: 0']) )
        # train_data=train_data.reset_index().drop(['index'],axis=1) 
        test_dataset = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( df.iloc[0]['Validation_id'] ) ) ] # Get exact validation data by matching the ids of the previous validation split data and the get all those rows from input processed data having same id values
        test_dataset = test_dataset.reset_index().drop(['index'],axis=1) 
        test_dataset['Groups'] = list(df.iloc[0]['Groups'].copy()) # Copy the group values from previous run's dataset to current dataset
        del df # Delete temp dataframe


    #Save splitinfo for user if he decides to save locally
    splitinfo_df = pd.DataFrame(columns=['Unnamed: 0','Train_id','Validation_id','Groups','Splitvalue','ID_name']) # Create a dataframe to store the new splitinfo details as pkl file - this new splitinfo can be an import splitfile itself. Save this to use for next iteration
    splitinfo_df['Unnamed: 0'] = [np.array(train_data.index)] # Create a temporary column to save the index of the training data - index is important to get correct data from correct rows
    splitinfo_df['Train_id'] = [np.array(train_data[sel_cols1[0]])] # This column has one single row with an array of all train id values
    splitinfo_df['Validation_id'] = [np.array(test_dataset[sel_cols1[0]])] # This column has one single row with an array of all validation id values
    splitinfo_df['Groups'] = [np.array(test_dataset['Groups'])] # This column has one single row with an array of all groups - only validation has group values
    splitinfo_df['Splitvalue'] = tr_split # Store the training split value
    splitinfo_df['ID_name'] = str(sel_cols1[0]) # Store the ID name
    splitinfo_df.to_pickle('model_files/Curve-fit_SplitInfo_file.pkl') # Save the dataframe as pickle file
    del splitinfo_df # Delete temp dataframe

    #Save train and validation csv files - not processed files, but the same indices from original dataset (just with selected columns)
    train_data.to_csv('model_files/Curve-fit_Training-data_file.csv', index=True) # Save as training data file
    test_dataset.to_csv('model_files/Curve-fit_Validation-data_file.csv', index=True) # Save as validation data file

    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    sel_cols3=sel_cols1[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
    print(sel_cols1)
    data=load_data('x_y.csv') # Load this csv which was stored after input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe

    x_test=test_dataset[sel_cols3].copy() # Make the dataset only with selected features and remove other columns in it
    y_test=test_dataset[sel_cols1[-1]].copy() # The last column is the target variable values

    vali_predictions_all=pd.DataFrame(y_test.copy()) # Create a dataframe with actual validation target values 
    vali_predictions_all['Groups_generated']=test_dataset['Groups'].copy() # Copy the groups generated for validation
    vali_predictions_all[east]=test_dataset[east].copy() # Copy the east coordinate values
    vali_predictions_all[north]=test_dataset[north].copy() # Copy the north coordinate values
    vali_predictions_all['Distance_new']=test_dataset['Distance_new'].copy() # Copy the distance column values

    y_train=train_data[sel_cols1[-1]].copy() # Get just the training data's target values - The last column name in sel_cols1 is the target variable
    x_train=train_data[sel_cols3].copy() # Make the dataset only with selected features and remove other columns in it

    # for graphing full input data
    spl_data=pd.read_csv('model_files/temp_pipeline_2.csv') # Read the processed input data
    x=spl_data[sel_cols1[3:-1]].copy() # Get the processed data without target values - (last column name in the list is the target column name)
    # create log, ln features
    colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    x_data, ln_cols = _create_divlogln_features(x,r,g,b,colourspaces) # Create log combinations - call this function and get updated dataset and also get the new column names that are added
    x_data.to_csv('model_files/x_data_logln.csv',index=False) # Save this updated input data as csv to use later

    x_train, ln_cols = _create_divlogln_features(x_train,r,g,b,colourspaces) # Create log combinations for training data separately- call this function and get updated dataset and also get the new column names that are added
    x_test, ln_cols = _create_divlogln_features(x_test,r,g,b,colourspaces) # Create log combinations for validation data separately- call this function and get updated dataset and also get the new column names that are added

    col_max=[] # Initialize empty list
    y_train_arr= np.array(y_train.values, dtype=float) # Convert the training target values from dataframe to an array of float type
    y_test_arr= np.array(y_test.values, dtype=float ) # Convert the validation target values from dataframe to an array of float type
    print(x_train.head())
    for col in x_train.columns: # For each feature in the processed dataset, fit on all 4 objectives and store their r2 score 
        print(col)
        r2_score_list=[]
        obj_score={}
        x_train_arr=np.array(x_train[col].values,dtype=float) # Convert the training dataframe to an array of float type - curve fitting works only on numpy arrays
        
        paras, unkn=curve_fit(objective_1,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
        a,b=paras  # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_1(x_train_arr, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_2,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 2 to get an equation for training data and target values 
        a,b,c=paras  # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_2(x_train_arr, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_3,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 3 to get an equation for training data and target values 
        a,b,c=paras  # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_3(x_train_arr, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        if(col not in ln_cols): # Do this objective 4 only if the column is not a log-combination column. Because objective 4 takes log of input values, and giving already computed log values to this function does not suit well - also you get error. So do this function only for non-log columns 
            paras, unkn=curve_fit(objective_4,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 4 to get an equation for training data and target values 
            a,b=paras  # Get the parameters / coeefficients
            r2_score_list.append( r2_score_cus( y_train_arr, objective_4(x_train_arr, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values


        max_indx=r2_score_list.index(max(r2_score_list))+1 # Get the index of the best r2 score from the stored r2 scores list for every feaure

        max_obj_str="objective_"+str(max_indx) # Get the name of the objective that gave highest r2 score 

        col_max.append({'column':str(col), 'objective':max_obj_str, 'score': max(r2_score_list)}) # Append the feaure name, its max r2 score objective name, max r2 score into a dict

    print(len(col_max))

    # top 5 curve fit for TRAIN data
    top_5 = sorted(enumerate(col_max), key=lambda x: x[1]['score'], reverse=True)[:5] # Out of all max r2 scores for every feature, get the top 5 r2 scores and its feature name - by ordering the dict in descending order based on r2 scores
    
    r2_top_5=[] # Initialize empty list
    eq_top_5=[] # Initialize empty list
    col_top_5=[] # Initialize empty list
    obj_top_5=[] # Initialize empty list
    paras_l=[] # Initialize empty list
    x_full=[] # Initialize empty list
    y_full=[] # Initialize empty list

    count=0
    for i in top_5: # For this descending order dict, i is the objective name, fit that objective function to training data again - and this time use that trained equation to predict on training data, whole input data and on validation data
        count+=1 # Keep counter - to run 5 objectives next
        # for x_train and x_test , get predictions and scores
        x_train_arr = np.array( x_train[i[1]['column']].values,dtype=float ) # Get the ith column from the training dataset and conver to float array
        x_test_arr = np.array( x_test[i[1]['column']].values,dtype=float ) # Get the ith column from the validation dataset and conver to float array
        x_full= np.array( x_data[i[1]['column']].values,dtype=float ) # Get the ith column from the full-input dataset and conver to float array
        
        col_name=str(i[1]['column']) # Get the ith column name from the dict 
        if( i[1]['objective']=="objective_1" ): # If it is objective 1 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_1,x_train_arr,y_train_arr, maxfev=100000)
            a,b=paras
            y_line = objective_1(x_train_arr, a, b)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 1 is ", r2_score_now )
            eq= str(round(a,3))+ '*x' + ' + ' + str(round(b,3)) # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_1(x_test_arr, a, b)
            y_full= objective_1( x_full, a, b)
            paras_l.append( [a,b] )
            

        if( i[1]['objective']=="objective_2" ): # If it is objective 2 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_2,x_train_arr,y_train_arr, maxfev=100000)
            a,b,c=paras
            y_line = objective_2(x_train_arr, a, b, c)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], "2 is ", r2_score_now )
            eq= ( str(round(a,3)) + '*x^2 ') + ' +'  + ( str(round(b,3)) + '*x') + ' + ' + str(round(c,3)) # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_2(x_test_arr, a, b, c)
            y_full= objective_2( x_full, a, b, c)
            paras_l.append( [a,b,c] )

        if( i[1]['objective']=="objective_3" ): # If it is objective 3 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_3,x_train_arr,y_train_arr, maxfev=100000)
            a,b,c=paras
            y_line = objective_3(x_train_arr, a, b, c)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 3 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + ( str(round(b,3)) + '* e^(' + str(-round(c,3)) + '*x)' ) + ' )' # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_3(x_test_arr, a, b, c)
            y_full= objective_3( x_full, a, b, c)
            paras_l.append( [a,b,c] )

        if( i[1]['objective'] == "objective_4" ): # If it is objective 4 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_4,x_train_arr,y_train_arr, maxfev=100000)
            a,b=paras
            y_line = objective_4(x_train_arr, a, b)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 4 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + ' * ' + 'log(x) )' # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_4(x_test_arr, a, b)
            y_full= objective_4( x_full, a, b)
            paras_l.append( [a,b] )
		
        if(count==1): # for the count value
            x_1=x_full
            y_1=y_full # Save the full input data predictions
            y_train_1=y_line # Save the training data predictions
            y_test_1=y_line_test # Save the validation data predictions
        if(count==2): # for the count value
            x_2=x_full
            y_2=y_full # Save the full input data predictions
            y_train_2=y_line # Save the training data predictions
            y_test_2=y_line_test # Save the validation data predictions
        if(count==3): # for the count value
            x_3=x_full
            y_3=y_full # Save the full input data predictions
            y_train_3=y_line # Save the training data predictions
            y_test_3=y_line_test # Save the validation data predictions
        if(count==4): # for the count value
            x_4=x_full
            y_4=y_full # Save the full input data predictions
            y_train_4=y_line # Save the training data predictions
            y_test_4=y_line_test # Save the validation data predictions
        if(count==5): # for the count value
            x_5=x_full
            y_5=y_full # Save the full input data predictions
            y_train_5=y_line # Save the training data predictions
            y_test_5=y_line_test # Save the validation data predictions

        r2_top_5.append(round(r2_score_now, 3)) # Round the r2 metric score and append to the top 5 r2 scores list
        eq_top_5.append(str(eq)) # Round the equation string and append to the top 5 equations list
        col_top_5.append(col_name) # Append the column name to the top 5 column names list
        obj_top_5.append(str(i[1]['objective'])) # Append the objective name to the top 5 objective names list
        print(col_top_5)
    
    print(paras_l)
    #save top5 col name and objective name to csv
    save_top_5=pd.DataFrame( columns=['Column_name','Objective_name','Paras'], index=range(5)) # After the all the top 5 objective and scores are computed, store them to a dataframe
    for i in range(0,5): # For 5 loops, store each of the top-5 column name, objective name and their parameter values to dataframe for later use
        save_top_5['Column_name'][i]=col_top_5[i]
        save_top_5['Objective_name'][i]=obj_top_5[i]
        save_top_5['Paras'][i]=paras_l[i]
    print(save_top_5)

    save_top_5.to_csv('model_files/'+'Save_top_5_cf.csv',index=False) # Save the top-5 equation details dataframe as csv           
    
    # Metrics for curve-fit train data
    tm1=train_metrics_cf( y_train_arr, y_train_1, save_top_5['Column_name'][0] )
    tm2=train_metrics_cf( y_train_arr, y_train_2, save_top_5['Column_name'][1] )
    tm3=train_metrics_cf( y_train_arr, y_train_3, save_top_5['Column_name'][2] )
    tm4=train_metrics_cf( y_train_arr, y_train_4, save_top_5['Column_name'][3] )
    tm5=train_metrics_cf( y_train_arr, y_train_5, save_top_5['Column_name'][4] )

    # Metrics for curve-fit validation data 
    te1=validation_metrics_cf( y_test_arr, y_test_1, save_top_5['Column_name'][0] )
    te2=validation_metrics_cf( y_test_arr, y_test_2, save_top_5['Column_name'][1] )
    te3=validation_metrics_cf( y_test_arr, y_test_3, save_top_5['Column_name'][2] )
    te4=validation_metrics_cf( y_test_arr, y_test_4, save_top_5['Column_name'][3] )
    te5=validation_metrics_cf( y_test_arr, y_test_5, save_top_5['Column_name'][4] )

    tms=[tm1,tm2,tm3,tm4,tm5] # Append all training metric scores to list
    final_train_metrics=pd.concat(tms,axis=0) # Concat all list values into a single training metric score datafame
    final_train_metrics=final_train_metrics.round(3) # Round the metrics to 3 decimals
    jsonfiles_tms = json.loads(final_train_metrics[tm1.columns].to_json(orient='records')) # Save the training metrics as json object to show to the user using javascript 
    final_train_metrics.to_csv('model_files/'+'curve_fitting_train_statistics.txt',index=False) # Save the training metrics scores as txt file
    
    tes=[te1,te2,te3,te4,te5] # Append all validation metric scores to list
    final_test_metrics=pd.concat(tes,axis=0) # Concat all list values into a single validation metric score datafame
    final_test_metrics=final_test_metrics.round(3)  # Round the metrics to 3 decimals
    jsonfiles_tes = json.loads(final_test_metrics[te1.columns].to_json(orient='records')) # Save the validation metrics as json object to show to the user using javascript 
    final_test_metrics.to_csv('model_files/'+'curve_fitting_validation_statistics.txt',index=False) # Save the validation metrics scores as txt file

    # Only train preds
    train_predictions=pd.DataFrame()
    train_predictions['Curve_1']=y_train_1 # Save the first curve training data's predictions
    train_predictions['Curve_2']=y_train_2 # Save the second curve training data's predictions
    train_predictions['Curve_3']=y_train_3 # Save the third curve training data's predictions
    train_predictions['Curve_4']=y_train_4 # Save the fourth curve training data's predictions
    train_predictions['Curve_5']=y_train_5 # Save the fifth curve training data's predictions
    train_predictions.to_csv('model_files/'+'Train_predictions.csv',index=False) # Save the dataframe as csv

    vali_predictions_all['Curve_1']=y_test_1 # Save the first curve validation data's predictions
    vali_predictions_all['Curve_2']=y_test_2 # Save the second curve validation data's predictions
    vali_predictions_all['Curve_3']=y_test_3 # Save the third curve validation data's predictions
    vali_predictions_all['Curve_4']=y_test_4 # Save the fourth curve validation data's predictions
    vali_predictions_all['Curve_5']=y_test_5 # Save the fifth curve validation data's predictions
    print(vali_predictions_all.head())
    vali_predictions_all=vali_predictions_all.round(3) # Round values in dataframe to 3 decimal places
    print(vali_predictions_all.shape)
    vali_predictions_all.to_csv('model_files/'+'all_models_vali_pred.csv',index=False) # Save the dataframe as csv

    predictions_all=pd.DataFrame()
    predictions_all[sel_cols1[-1]]=y # Save the actual target values of full-input data to the dataframe
    predictions_all['Curve_1']=y_1 # Save the first curve full-input data's predictions
    predictions_all['Curve_2']=y_2 # Save the second curve full-input data's predictions
    predictions_all['Curve_3']=y_3 # Save the third curve full-input data's predictions
    predictions_all['Curve_4']=y_4 # Save the fourth curve full-input data's predictions
    predictions_all['Curve_5']=y_5 # Save the fifth curve full-input data's predictions
    print(predictions_all.head())
    predictions_all=predictions_all.round(3) # Round values in dataframe to 3 decimal places
    predictions_all.to_csv('model_files/all_models_preds.csv',index=False) # Save the dataframe as csv

    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the full-input data's prediction dataframe
    return jsonify({ 'eq_top_5': list(eq_top_5), 'col_top_5': list(col_top_5), 'train_metric_scores' : jsonfiles_tms, 'test_metric_scores':jsonfiles_tes , 'sample_preds':jsonfiles_predictions_all })



##################### UPLOAD VALI CF #########################
@app.route('/upload_vali_csv_file', methods=['POST'])
def upload_vali_csv_file(): # Save the uploaded validation csv to local path and rename to 'vali_data_software_generated.csv'
    if( request.method=="POST" ):
        if( request.files ):
            file=request.files["file"] # request variable is the flask variable which contains all the information sent from javascript side to python for processing, we access values by request.files[ name_of_variable_given_in_javascript_side ]
            print(file)
            file.save(os.path.join(app.config['FILE_SAVE'], file.filename)) # Same as input csv file, get the validation file from user and save it to model_files folder
            print(" file saved ")
            #f"{file.filename}
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'],"vali_data_software_generated.csv") )): # Remove any existing files with name vali_data_software_generated.csv. Note that the original location of the file is unchanged - meaning the original file untouched, but a copy of the file is stored in model_files folder and that file is renamed to temp_pipeline.csv which will be used almost everywhere in the code
                os.remove( os.path.join(app.config['FILE_SAVE'],"vali_data_software_generated.csv") )  # If another file named vali_data_software_generated.csv already exists, then delete that and rename the new file to vali_data_software_generated.csv
            os.rename( os.path.join(app.config['FILE_SAVE'], file.filename), os.path.join(app.config['FILE_SAVE'],"vali_data_software_generated.csv") ) # Rename the copied file to vali_data_software_generated.csv
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'], file.filename)) ): # Remove the initial copy of the user's copied file - we only rename it, so this line won't probably run, but still if multiple copies of same file are created somehow, then delete them
                os.remove( os.path.join(app.config['FILE_SAVE'], file.filename) ) # Remove the duplicate files
            
            res = make_response(jsonify({"message":  f" Validation data uploaded successfully "}),200) # Send back response to user saying files are uploaded

            return res
        return render_template('curve_fitting.html/upload_vali_csv_file')


## 3. Use user's uploaded validation csv

@app.route('/ml_algorithms_upload_vali_csv_cf',methods=['GET','POST'])
def ml_algorithms_upload_vali_csv_cf():
    feats=request.form['sel_feat'] # Get selected features ( FOR THE NEW UPLOADED CSV ) as json object
    sel_cols1  = json.loads(feats) # Convert the json object to string
    
    selected_feats_b4_json=request.form['selected_feats_b4'] # Get json object of what features were used for the input data-processing.
    feats_temp  = json.loads(selected_feats_b4_json) # Convert the json object to string

    colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    
    test_dataset=pd.read_csv('model_files/'+'vali_data_software_generated.csv') # Use the input validation csv as the validation dataset and the whole input data as training data. The rest of the codes are same as above functions
    # edit test_dataset and add required colourspaces
    r=feats_temp[3] #3 because of east north corrd input. Get R column name
    g=feats_temp[4] # Get G column name
    b=feats_temp[5] # Get B column name  
    
    ## Now for the uploaded csv - do all the pre-processing we did over the input dataset

    # check if 0 value is present in r,g,b
    if(test_dataset.describe()[r]['min']==0):  # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                
        test_dataset[r]=np.where(test_dataset[r]==0,0.1,test_dataset[r])
    if(test_dataset.describe()[g]['min']==0):  # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                
        test_dataset[g]=np.where(test_dataset[g]==0,0.1,test_dataset[g])
    if(test_dataset.describe()[b]['min']==0):  # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                
        test_dataset[b]=np.where(test_dataset[b]==0,0.1,test_dataset[b])
    
    if(test_dataset.describe()[r]['min']<0):   # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                 
        test_dataset[r]=np.where(test_dataset[r]<0,0.1,test_dataset[r])
    if(test_dataset.describe()[g]['min']<0):   # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                 
        test_dataset[g]=np.where(test_dataset[g]<0,0.1,test_dataset[g])
    if(test_dataset.describe()[b]['min']<0):   # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                
        test_dataset[b]=np.where(test_dataset[b]<0,0.1,test_dataset[b])

    print(colourspaces)
    if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
        print('same dataset - no hsv only rgb')

    elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
        print('same dataset - no hsv only rgb')

    elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
        print('change in dataset - no rgb only hsv')
        # hsv_vals = matplotlib.colors.rgb_to_hsv(test_dataset[[r,g,b]]/255)
        hsv_vals = test_dataset[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        test_dataset2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
        test_dataset[r] = test_dataset2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset[g] = test_dataset2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset[b] = test_dataset2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        test_dataset = test_dataset.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
        for ix, valx in enumerate(feats_temp): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
            if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                feats_temp[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
            if(valx == g):
                feats_temp[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
            if(valx == b):
                feats_temp[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
        
        # check if 0 value is present in r,g,b
        if(test_dataset.describe()['H_generated']['min']==0):     # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                
            test_dataset['H_generated']=np.where(test_dataset['H_generated']==0,0.1,test_dataset['H_generated'])
        if(test_dataset.describe()['S_generated']['min']==0):     # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
            test_dataset['S_generated']=np.where(test_dataset['S_generated']==0,0.1,test_dataset['S_generated'])
        if(test_dataset.describe()['V_generated']['min']==0):     # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                            
            test_dataset['V_generated']=np.where(test_dataset['V_generated']==0,0.1,test_dataset['V_generated'])       

        del test_dataset2 # Delete the temporary dataframe

    elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
        print('change in dataset - rgb and hsv')
        hsv_vals = test_dataset[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        test_dataset2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
        test_dataset.insert(6, 'H_generated', test_dataset2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset.insert(7, 'S_generated', test_dataset2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset.insert(8, 'V_generated', test_dataset2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        #insert in feats_temp 
        feats_temp.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        feats_temp.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        feats_temp.insert(8, 'V_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

        # check if 0 value is present in r,g,b
        if(test_dataset.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                    
            test_dataset['H_generated']=np.where(test_dataset['H_generated']==0,0.1,test_dataset['H_generated'])
        if(test_dataset.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                    
            test_dataset['S_generated']=np.where(test_dataset['S_generated']==0,0.1,test_dataset['S_generated'])
        if(test_dataset.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                    
            test_dataset['V_generated']=np.where(test_dataset['V_generated']==0,0.1,test_dataset['V_generated'])       

        del test_dataset2 # Delete the temporary dataframe
    
    original_feats_train=[]  # Initialize empty list to save the original features used for training - access later in full-data part
    # Make sure full river also has same column names
    for i in range(len(sel_cols1)): # For all values in the sel_cols1
        original_feats_train.append([str(sel_cols1[i])]) # Append every column name to the list
    original_feats_train_df=pd.DataFrame(original_feats_train,columns=['feats'])   # Create a dataframe with column name 'feats' and store the original_feats_train list values      
    original_feats_train_df.to_csv('model_files/'+'original_feats_train.csv',index=False) # Save the dataframe as csv selected to use later

    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    sel_cols3=sel_cols1[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
    print(sel_cols1)
    data=load_data('x_y.csv') # Load this csv which was stored after input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe

    x_test=test_dataset[sel_cols3].copy() # Make the dataset only with selected features and remove other columns in it
    y_test=test_dataset[sel_cols1[-1]].copy() # The last column is the target variable values

    vali_predictions_all=pd.DataFrame(y_test.copy()) # Create a dataframe with actual validation target values 
    vali_predictions_all[east]=test_dataset[east].copy() # Copy the east coordinate values
    vali_predictions_all[north]=test_dataset[north].copy() # Copy the north coordinate values
    vali_predictions_all['Distance_new']=0 # Calculate distance for this new uploaded validation csv
    agg=0
    for i in range(1,len(vali_predictions_all)):
        diff=distance_cal(vali_predictions_all[vali_predictions_all.columns[1]].iloc[i],vali_predictions_all[vali_predictions_all.columns[2]].iloc[i],vali_predictions_all[vali_predictions_all.columns[1]].iloc[i-1],vali_predictions_all[vali_predictions_all.columns[2]].iloc[i-1])
        agg=agg+diff
        vali_predictions_all['Distance_new'].iloc[i]=agg
       

    y_train=data[sel_cols1[-1]].copy() # Get just the training data's target values - The last column name in sel_cols1 is the target variable
    x_train=data[sel_cols3].copy() # Make the dataset only with selected features and remove other columns in it

    spl_data=pd.read_csv('model_files/temp_pipeline_2.csv') # Read the processed input data
    x=spl_data[sel_cols1[3:-1]].copy() # Get the processed data without target values - (last column name in the list is the target column name)
    
    # create log, ln features
    x_data, ln_cols = _create_divlogln_features(x,r,g,b,colourspaces) # Create log combinations - call this function and get updated dataset and also get the new column names that are added
    x_data.to_csv('model_files/x_data_logln.csv',index=False) # Save this updated input data as csv to use later

    x_train, ln_cols = _create_divlogln_features(x_train,r,g,b,colourspaces) # Create log combinations for training data separately- call this function and get updated dataset and also get the new column names that are added
    x_test, ln_cols = _create_divlogln_features(x_test,r,g,b,colourspaces) # Create log combinations for validation data separately- call this function and get updated dataset and also get the new column names that are added

    col_max=[] # Initialize empty list
    y_train_arr= np.array(y_train.values, dtype=float) # Convert the training target values from dataframe to an array of float type
    y_test_arr= np.array(y_test.values, dtype=float ) # Convert the validation target values from dataframe to an array of float type
    
    for col in x_train.columns: # For each feature in the processed dataset, fit on all 4 objectives and store their r2 score 
        print(col)
        r2_score_list=[]
        obj_score={}
        x_train_arr=np.array(x_train[col].values,dtype=float) # Convert the training dataframe to an array of float type - curve fitting works only on numpy arrays
        
        paras, unkn=curve_fit(objective_1,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
        a,b=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_1(x_train_arr, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_2,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 2 to get an equation for training data and target values 
        a,b,c=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_2(x_train_arr, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_3,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 3 to get an equation for training data and target values 
        a,b,c=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y_train_arr, objective_3(x_train_arr, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        if(col not in ln_cols): # Do this objective 4 only if the column is not a log-combination column. Because objective 4 takes log of input values, and giving already computed log values to this function does not suit well - also you get error. So do this function only for non-log columns 
            paras, unkn=curve_fit(objective_4,x_train_arr,y_train_arr, maxfev=100000) # Curve-fit objective 4 to get an equation for training data and target values 
            a,b=paras # Get the parameters / coeefficients
            r2_score_list.append( r2_score_cus( y_train_arr, objective_4(x_train_arr, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values


        max_indx=r2_score_list.index(max(r2_score_list))+1 # Get the index of the best r2 score from the stored r2 scores list for every feaure

        max_obj_str="objective_"+str(max_indx) # Get the name of the objective that gave highest r2 score 

        col_max.append({'column':str(col), 'objective':max_obj_str, 'score': max(r2_score_list)}) # Append the feaure name, its max r2 score objective name, max r2 score into a dict

    print(len(col_max))

    # top 5 curve fit for TRAIN data
    top_5 = sorted(enumerate(col_max), key=lambda x: x[1]['score'], reverse=True)[:5] # Out of all max r2 scores for every feature, get the top 5 r2 scores and its feature name - by ordering the dict in descending order based on r2 scores
    
    r2_top_5=[] # Initialize empty list
    eq_top_5=[] # Initialize empty list
    col_top_5=[] # Initialize empty list
    obj_top_5=[] # Initialize empty list
    paras_l=[] # Initialize empty list
    x_full=[] # Initialize empty list
    y_full=[] # Initialize empty list

    count=0
    for i in top_5: # For this descending order dict, i is the objective name, fit that objective function to training data again - and this time use that trained equation to predict on training data, whole input data and on validation data
        count+=1 # Keep counter - to run 5 objectives next
        # for x_train and x_test , get predictions and scores
        x_train_arr = np.array( x_train[i[1]['column']].values,dtype=float ) # Get the ith column from the training dataset and conver to float array
        x_test_arr = np.array( x_test[i[1]['column']].values,dtype=float ) # Get the ith column from the validation dataset and conver to float array
        x_full= np.array( x_data[i[1]['column']].values,dtype=float ) # Get the ith column from the full-input dataset and conver to float array
        
        col_name=str(i[1]['column']) # Get the ith column name from the dict 
        if( i[1]['objective']=="objective_1" ): # If it is objective 1 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_1,x_train_arr,y_train_arr, maxfev=100000)
            a,b=paras
            y_line = objective_1(x_train_arr, a, b)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 1 is ", r2_score_now )
            eq= str(round(a,3))+ '*x' + ' + ' + str(round(b,3)) # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_1(x_test_arr, a, b)
            y_full= objective_1( x_full, a, b)
            paras_l.append( [a,b] )
            

        if( i[1]['objective']=="objective_2" ): # If it is objective 2 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_2,x_train_arr,y_train_arr, maxfev=100000)
            a,b,c=paras
            y_line = objective_2(x_train_arr, a, b, c)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], "2 is ", r2_score_now )
            eq= ( str(round(a,3)) + '*x^2 ') + ' +'  + ( str(round(b,3)) + '*x') + ' + ' + str(round(c,3)) # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_2(x_test_arr, a, b, c)
            y_full= objective_2( x_full, a, b, c)
            paras_l.append( [a,b,c] )

        if( i[1]['objective']=="objective_3" ): # If it is objective 3 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_3,x_train_arr,y_train_arr, maxfev=100000)
            a,b,c=paras
            y_line = objective_3(x_train_arr, a, b, c)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 3 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + ( str(round(b,3)) + '* e^(' + str(-round(c,3)) + '*x)' ) + ' )' # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_3(x_test_arr, a, b, c)
            y_full= objective_3( x_full, a, b, c)
            paras_l.append( [a,b,c] )

        if( i[1]['objective'] == "objective_4" ): # If it is objective 4 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_4,x_train_arr,y_train_arr, maxfev=100000)
            a,b=paras
            y_line = objective_4(x_train_arr, a, b)
            r2_score_now= r2_score_cus( y_train_arr, y_line)
            print(" r2 score for ",i[1], " 4 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + ' * ' + 'log(x) )' # Create and store a string equation and round the coefficients to 3 decimals
            y_line_test= objective_4(x_test_arr, a, b)
            y_full= objective_4( x_full, a, b)
            paras_l.append( [a,b] )
		
        if(count==1): # for the count value
            x_1=x_full
            y_1=y_full # Save the full input data predictions
            y_train_1=y_line # Save the training data predictions
            y_test_1=y_line_test # Save the validation data predictions
        if(count==2): # for the count value
            x_2=x_full
            y_2=y_full # Save the full input data predictions
            y_train_2=y_line # Save the training data predictions
            y_test_2=y_line_test # Save the validation data predictions
        if(count==3): # for the count value
            x_3=x_full
            y_3=y_full # Save the full input data predictions
            y_train_3=y_line # Save the training data predictions
            y_test_3=y_line_test # Save the validation data predictions
        if(count==4): # for the count value
            x_4=x_full
            y_4=y_full # Save the full input data predictions
            y_train_4=y_line # Save the training data predictions
            y_test_4=y_line_test # Save the validation data predictions
        if(count==5): # for the count value
            x_5=x_full
            y_5=y_full # Save the full input data predictions
            y_train_5=y_line # Save the training data predictions
            y_test_5=y_line_test # Save the validation data predictions

        r2_top_5.append(round(r2_score_now, 3)) # Round the r2 metric score and append to the top 5 r2 scores list
        eq_top_5.append(str(eq)) # Round the equation string and append to the top 5 equations list
        col_top_5.append(col_name) # Append the column name to the top 5 column names list
        obj_top_5.append(str(i[1]['objective'])) # Append the objective name to the top 5 objective names list
        print(col_top_5)
    
    print(paras_l)
    #save top5 col name and objective name to csv
    save_top_5=pd.DataFrame( columns=['Column_name','Objective_name','Paras'], index=range(5)) # After the all the top 5 objective and scores are computed, store them to a dataframe
    for i in range(0,5): # For 5 loops, store each of the top-5 column name, objective name and their parameter values to dataframe for later use
        save_top_5['Column_name'][i]=col_top_5[i]
        save_top_5['Objective_name'][i]=obj_top_5[i]
        save_top_5['Paras'][i]=paras_l[i]
    print(save_top_5)

    save_top_5.to_csv('model_files/'+'Save_top_5_cf.csv',index=False)    # Save the top-5 equation details dataframe as csv                
    
    # Metrics for curve-fit train data
    tm1=train_metrics_cf( y_train_arr, y_train_1, save_top_5['Column_name'][0] )
    tm2=train_metrics_cf( y_train_arr, y_train_2, save_top_5['Column_name'][1] )
    tm3=train_metrics_cf( y_train_arr, y_train_3, save_top_5['Column_name'][2] )
    tm4=train_metrics_cf( y_train_arr, y_train_4, save_top_5['Column_name'][3] )
    tm5=train_metrics_cf( y_train_arr, y_train_5, save_top_5['Column_name'][4] )

    # Metrics for curve-fit validation data 
    te1=validation_metrics_cf( y_test_arr, y_test_1, save_top_5['Column_name'][0] )
    te2=validation_metrics_cf( y_test_arr, y_test_2, save_top_5['Column_name'][1] )
    te3=validation_metrics_cf( y_test_arr, y_test_3, save_top_5['Column_name'][2] )
    te4=validation_metrics_cf( y_test_arr, y_test_4, save_top_5['Column_name'][3] )
    te5=validation_metrics_cf( y_test_arr, y_test_5, save_top_5['Column_name'][4] )

    tms=[tm1,tm2,tm3,tm4,tm5] # Append all training metric scores to list
    final_train_metrics=pd.concat(tms,axis=0) # Concat all list values into a single training metric score datafame
    final_train_metrics=final_train_metrics.round(3)  # Round the metrics to 3 decimals
    jsonfiles_tms = json.loads(final_train_metrics[tm1.columns].to_json(orient='records')) # Save the training metrics as json object to show to the user using javascript 
    final_train_metrics.to_csv('model_files/'+'curve_fitting_train_statistics.txt',index=False) # Save the training metrics scores as txt file
    
    tes=[te1,te2,te3,te4,te5] # Append all validation metric scores to list
    final_test_metrics=pd.concat(tes,axis=0) # Concat all list values into a single validation metric score datafame
    final_test_metrics=final_test_metrics.round(3)  # Round the metrics to 3 decimals
    jsonfiles_tes = json.loads(final_test_metrics[te1.columns].to_json(orient='records')) # Save the validation metrics as json object to show to the user using javascript 
    final_test_metrics.to_csv('model_files/'+'curve_fitting_validation_statistics.txt',index=False) # Save the validation metrics scores as txt file

    # Only train preds
    train_predictions=pd.DataFrame()
    train_predictions['Curve_1']=y_train_1 # Save the first curve training data's predictions
    train_predictions['Curve_2']=y_train_2 # Save the second curve training data's predictions
    train_predictions['Curve_3']=y_train_3 # Save the third curve training data's predictions
    train_predictions['Curve_4']=y_train_4 # Save the fourth curve training data's predictions
    train_predictions['Curve_5']=y_train_5 # Save the fifth curve training data's predictions
    train_predictions.to_csv('model_files/'+'Train_predictions.csv',index=False) # Save the dataframe as csv

    vali_predictions_all['Curve_1']=y_test_1 # Save the first curve validation data's predictions
    vali_predictions_all['Curve_2']=y_test_2 # Save the second curve validation data's predictions
    vali_predictions_all['Curve_3']=y_test_3 # Save the third curve validation data's predictions
    vali_predictions_all['Curve_4']=y_test_4 # Save the fourth curve validation data's predictions
    vali_predictions_all['Curve_5']=y_test_5 # Save the fifth curve validation data's predictions
    print(vali_predictions_all.head())
    vali_predictions_all=vali_predictions_all.round(3) # Round values in dataframe to 3 decimal places
    print(vali_predictions_all.shape)
    vali_predictions_all.to_csv('model_files/'+'all_models_vali_pred.csv',index=False) # Save the dataframe as csv

    predictions_all=pd.DataFrame()
    predictions_all[sel_cols1[-1]]=y # Save the actual target values of full-input data to the dataframe
    predictions_all['Curve_1']=y_1 # Save the first curve full-input data's predictions
    predictions_all['Curve_2']=y_2 # Save the second curve full-input data's predictions
    predictions_all['Curve_3']=y_3 # Save the third curve full-input data's predictions
    predictions_all['Curve_4']=y_4 # Save the fourth curve full-input data's predictions
    predictions_all['Curve_5']=y_5 # Save the fifth curve full-input data's predictions
    print(predictions_all.head())
    predictions_all=predictions_all.round(3) # Round values in dataframe to 3 decimal places
    predictions_all.to_csv('model_files/all_models_preds.csv',index=False) # Save the dataframe as csv

    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the full-input data's prediction dataframe
    return jsonify({ 'eq_top_5': list(eq_top_5), 'col_top_5': list(col_top_5), 'train_metric_scores' : jsonfiles_tms, 'test_metric_scores':jsonfiles_tes , 'sample_preds':jsonfiles_predictions_all })


## 4. No validation at all

@app.route('/ml_algorithms_no_vali_cf',methods=['GET','POST'])
def ml_algorithms_no_vali_cf():
    feats=request.form['sel_feat'] # Get selected features as json object - for the uploaded csv
    sel_cols1  = json.loads(feats) # Convert the json object to string

    original_feats_train=[]  # Initialize empty list to save the original features used for training - access later in full-data part
    # Make sure full river also has same column names
    for i in range(len(sel_cols1)): # For all values in the sel_cols1
        original_feats_train.append([str(sel_cols1[i])]) # Append every column name to the list
    original_feats_train_df=pd.DataFrame(original_feats_train,columns=['feats'])     # Create a dataframe with column name 'feats' and store the original_feats_train list values      
    original_feats_train_df.to_csv('model_files/'+'original_feats_train.csv',index=False) # Save the dataframe as csv selected to use later
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    data=load_data('x_y.csv') # Load this csv which was stored after input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    x=data[sel_cols1[3:-1]].copy() # Get the processed data without target values - (last column name in the list is the target column name)
    y=np.array(data[sel_cols1[-1]].values,dtype=float) # Get the data's target values and store as separate dataframe

    # create log, ln features
    colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    data, ln_cols = _create_divlogln_features(x,r,g,b,colourspaces) # Create log combinations - call this function and get updated dataset and also get the new column names that are added
    data.to_csv('model_files/x_data_logln.csv',index=False) # Save this updated input data as csv to use later. We do not create or import any validation data because the option selected by the user is to use all of the input data as training data and no validation at all

    # #save transformed data
    # to_send_data=data.copy()
    # to_send_data['Depth']=y
    # to_send_data.to_csv('model_files/transformed_curve_fit.csv',index=False)
    
    col_max=[] # Initialize empty list

    for col in data.columns: # For each feature in the processed dataset, fit on all 4 objectives and store their r2 score 
        print(col)
        r2_score_list=[]
        obj_score={}
        x=np.array(data[col].values,dtype=float) # Convert the training dataframe to an array of float type - curve fitting works only on numpy arrays
        
        paras, unkn=curve_fit(objective_1,x,y, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
        a,b=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y, objective_1(x, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_2,x,y, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
        a,b,c=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y, objective_2(x, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        paras, unkn=curve_fit(objective_3,x,y, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
        a,b,c=paras # Get the parameters / coeefficients
        r2_score_list.append( r2_score_cus( y, objective_3(x, a, b, c)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values

        # if(col not in ln_cols):
        # paras, unkn=curve_fit(objective_4,x,y, maxfev=100000)
        # a,b,c,d,e=paras
        # r2_score_list.append( r2_score_cus( y, objective_4(x, a, b, c, d, e)) )

        # paras, unkn=curve_fit(objective_5,x,y, maxfev=100000)
        # a,b,c=paras
        # r2_score_list.append( r2_score_cus( y, objective_5(x, a, b, c)) )

        if(col not in ln_cols): # Do this objective 4 only if the column is not a log-combination column. Because objective 4 takes log of input values, and giving already computed log values to this function does not suit well - also you get error. So do this function only for non-log columns 
            paras, unkn=curve_fit(objective_4,x,y, maxfev=100000) # Curve-fit objective 1 to get an equation for training data and target values 
            a,b=paras # Get the parameters / coeefficients
            r2_score_list.append( r2_score_cus( y, objective_4(x, a, b)) ) # Calculate r2 score for the predicted values (from equation) and actaual target values


        max_indx=r2_score_list.index(max(r2_score_list))+1 # Get the index of the best r2 score from the stored r2 scores list for every feaure

        max_obj_str="objective_"+str(max_indx) # Get the name of the objective that gave highest r2 score 

        col_max.append({'column':str(col), 'objective':max_obj_str, 'score': max(r2_score_list)}) # Append the feaure name, its max r2 score objective name, max r2 score into a dict

    print(len(col_max))

    # top 5 curve fit for TRAIN data
    top_5 = sorted(enumerate(col_max), key=lambda x: x[1]['score'], reverse=True)[:5] # Out of all max r2 scores for every feature, get the top 5 r2 scores and its feature name - by ordering the dict in descending order based on r2 scores
    
    r2_top_5=[] # Initialize empty list
    eq_top_5=[] # Initialize empty list
    col_top_5=[] # Initialize empty list
    obj_top_5=[] # Initialize empty list
    paras_l=[] # Initialize empty list

    count=0
    for i in top_5: # For this descending order dict, i is the objective name, fit that objective function to training data again - and this time use that trained equation to predict on training data - which is the whole input data in this case
        count+=1 # Keep counter - to run 5 objectives next
        x=np.array(data[i[1]['column']].values,dtype=float) # Get the ith column from the training dataset and conver to float array
        col_name=str(i[1]['column']) # Get the ith column name from the dict 
        if( i[1]['objective']=="objective_1" ): # If it is objective 1 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_1,x,y, maxfev=100000)
            a,b=paras
            y_line = objective_1(x, a, b)
            r2_score_now= r2_score_cus( y, y_line)
            print(" r2 score for ",i[1], " 1 is ", r2_score_now )
            eq= str(round(a,3))+ '*x' + ' + ' + str(round(b,3)) # Create and store a string equation and round the coefficients to 3 decimals
            paras_l.append( [a,b] )
            

        if( i[1]['objective']=="objective_2" ): # If it is objective 2 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_2,x,y, maxfev=100000)
            a,b,c=paras
            y_line = objective_2(x, a, b, c)
            r2_score_now= r2_score_cus( y, y_line)
            print(" r2 score for ",i[1], "2 is ", r2_score_now )
            eq= ( str(round(a,3)) + '*x^2 ') + ' +'  + ( str(round(b,3)) + '*x') + ' + ' + str(round(c,3)) # Create and store a string equation and round the coefficients to 3 decimals
            paras_l.append( [a,b,c] )

        if( i[1]['objective']=="objective_3" ): # If it is objective 3 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_3,x,y, maxfev=100000)
            a,b,c=paras
            y_line = objective_3(x, a, b, c)
            r2_score_now= r2_score_cus( y, y_line)
            print(" r2 score for ",i[1], " 3 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + ( str(round(b,3)) + '* e^(' + str(-round(c,3)) + '*x)' ) + ' )' # Create and store a string equation and round the coefficients to 3 decimals
            paras_l.append( [a,b,c] )

        # if( i[1]['objective']=="objective_4" ): 
        #     paras, unkn=curve_fit(objective_4,x,y, maxfev=100000)
        #     a,b,c,d,e=paras
        #     y_line = objective_4(x, a, b, c, d, e)
        #     r2_score_now= r2_score_cus( y, y_line)
        #     print(" r2 score for ",i[1], " 4 is ", r2_score_now )
        #     eq= str(round(a,3)) + ' + ' + '(' + str(b-c) + ')' + '/' + '( 1 + (x'  + '/' + str(d) + ')^' + str(e) + ') )'   
        #     paras_l.append( [a,b,c,d,e] )
        #     print(y_line)

        # if( i[1]['objective']=="objective_5" ):
        #     paras, unkn=curve_fit(objective_5,x,y, maxfev=100000)
        #     a,b,c=paras
        #     y_line = objective_5(x, a, b, c)
        #     r2_score_now= r2_score_cus( y, y_line)
        #     print(" r2 score for ",i[1], " 5 is ", r2_score_now )
        #     eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + '*' + 'sin(' + str(round(c,3)) + '-x) )'
        #     paras_l.append( [a,b,c] )

        if( i[1]['objective'] == "objective_4" ): # If it is objective 4 , run the curve-fit again , get the parameters / coefficients and calculate metric scores
            paras, unkn=curve_fit(objective_4,x,y, maxfev=100000)
            a,b=paras
            y_line = objective_4(x, a, b)
            r2_score_now= r2_score_cus( y, y_line)
            print(" r2 score for ",i[1], " 4 is ", r2_score_now )
            eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + ' * ' + 'log(x) )' # Create and store a string equation and round the coefficients to 3 decimals
            paras_l.append( [a,b] )
		
        if(count==1): # for the count value
            x_1=x
            y_1=y_line # Save the training data predictions
        if(count==2): # for the count value
            x_2=x
            y_2=y_line # Save the training data predictions
        if(count==3): # for the count value
            x_3=x
            y_3=y_line # Save the training data predictions
        if(count==4): # for the count value
            x_4=x
            y_4=y_line # Save the training data predictions
        if(count==5): # for the count value
            x_5=x
            y_5=y_line # Save the training data predictions

        r2_top_5.append(round(r2_score_now, 3)) # Round the r2 metric score and append to the top 5 r2 scores list
        eq_top_5.append(str(eq)) # Round the equation string and append to the top 5 equations list
        col_top_5.append(col_name) # Append the column name to the top 5 column names list
        obj_top_5.append(str(i[1]['objective'])) # Append the objective name to the top 5 objective names list
        
        print(col_top_5)
    print(paras_l)
    #save top5 col name and objective name to csv
    save_top_5=pd.DataFrame( columns=['Column_name','Objective_name','Paras'], index=range(5)) # After the all the top 5 objective and scores are computed, store them to a dataframe
    # save_top_5['Column_name']=''
    # save_top_5['Objective_name']=''
    # save_top_5['Paras']=''
    for i in range(0,5): # For 5 loops, store each of the top-5 column name, objective name and their parameter values to dataframe for later use
        save_top_5['Column_name'][i]=col_top_5[i]
        save_top_5['Objective_name'][i]=obj_top_5[i]
        save_top_5['Paras'][i]=paras_l[i]
    print(save_top_5)

    save_top_5.to_csv('model_files/'+'Save_top_5_cf.csv',index=False)  # Save the top-5 equation details dataframe as csv                       

# Metrics for curve-fit train data
    tm1=train_metrics_cf( y, y_1, save_top_5['Column_name'][0] ) # Only training metrics - no validation data so no validation metrics
    tm2=train_metrics_cf( y, y_2, save_top_5['Column_name'][1] )
    tm3=train_metrics_cf( y, y_3, save_top_5['Column_name'][2] )
    tm4=train_metrics_cf( y, y_4, save_top_5['Column_name'][3] )
    tm5=train_metrics_cf( y, y_5, save_top_5['Column_name'][4] )

    tms=[tm1,tm2,tm3,tm4,tm5] # Append all training metric scores to list
    final_train_metrics=pd.concat(tms,axis=0) # Concat all list values into a single training metric score datafame
    final_train_metrics=final_train_metrics.round(3)  # Round the metrics to 3 decimals
    jsonfiles_tms = json.loads(final_train_metrics[tm1.columns].to_json(orient='records')) # Save the training metrics as json object to show to the user using javascript 
    final_train_metrics.to_csv('model_files/'+'curve_fitting_statistics.txt',index=False) # Save the training metrics scores as txt file

    predictions_all=pd.DataFrame()
    predictions_all[sel_cols1[-1]]=y # Save the actual target values of full-input data to the dataframe
    predictions_all['Curve_1']=y_1 # Save the first curve full-input data's predictions
    predictions_all['Curve_2']=y_2 # Save the second curve full-input data's predictions
    predictions_all['Curve_3']=y_3 # Save the third curve full-input data's predictions
    predictions_all['Curve_4']=y_4 # Save the fourth curve full-input data's predictions
    predictions_all['Curve_5']=y_5 # Save the fifth curve full-input data's predictions
    print(predictions_all.head())
    predictions_all=predictions_all.round(3) # Round values in dataframe to 3 decimal places
    predictions_all.to_csv('model_files/all_models_preds.csv',index=False) # Save the dataframe as csv

    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the full-input data's prediction dataframe
    return jsonify({ 'eq_top_5': list(eq_top_5), 'col_top_5': list(col_top_5), 'train_metric_scores' : jsonfiles_tms, 'sample_preds':jsonfiles_predictions_all })



#### SECTION 7: GRAPHING FUNCTIONS FOR CURVE-FITTING MODULE AND SAVE PLOTS AS PDF

# The next few sections consists of functions for graph plotting for curve-fit module

@app.route('/provide_group_values_all_cf', methods=['POST','GET'])
def provide_group_values_all_cf(): # This function is to send back values for every group change in graph
    group_numb_obj=request.form['group_numb'] # Get the json object of the group number
    group_numb=json.loads(group_numb_obj) # Get the selected group number by converting json object to string
    
    vali_preds=load_data('all_models_vali_pred.csv') # Load the validation predictions with actual target values, we saved this file earlier while training the curve-fitting equations 
    time.sleep(0.1)    
    return jsonify({ 'vali_x':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new']), 'vali_y1':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[0]]), 'vali_c1':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-5]]), 'vali_c2':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-4]]), 'vali_c3':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-3]]), 'vali_c4':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-2]]), 'vali_c5':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-1]]) }) 


@app.route('/graphs_predictions_all_curves', methods=['POST','GET'])
def graphs_predictions_all_curves(): # This function is to provide values for the software to plot and show graphs - only for curve-fitting module
    show_graphs_check_obj=request.form['show_graphs_check'] # Get json object of show_graphs_check - the values can be yes or no
    show_graphs_check=json.loads(show_graphs_check_obj) # Convert the json object to string value
    vali_mode_obj=request.form['vali_mode'] # Get the validation mode json object
    vali_mode=json.loads(vali_mode_obj) # Convert the json object to string value
    col_top_5_obj=request.form['col_top_5'] # Get the top 5 columns dict - this also has its r2 scores and objective names - get this whole info as a json object
    col_top_5=json.loads(col_top_5_obj) # Convert the json object to string value
    print("inside")
    if(show_graphs_check=="Yes"): # If show_graphs_check is yes then compute graph values and send back
        preds=load_data('all_models_preds.csv') # File with predictions of full input data also has the actual target variable values
        data=load_data('temp_pipeline_2.csv') # Load the processed input data - we get the distance values for every graph from this dataset

        pre=preds.copy() # Make a copy of this dataframe - to not lose original if any changes are made
        col0=pre.columns.tolist()[0] # Get the ID (first) column name
        pre=pre.sort_values(by=col0) # Sort the dataframe by the ID column
        pre=pre.reset_index() # Reset the index just to be sure all the indices are in order
        pre.drop(['index'],axis=1,inplace=True) # When you reset index, an extra column called 'index' is created, so remove that

        x_data=pd.read_csv('model_files/x_data_logln.csv') # This dataset we stored during model training/ curve-training phase - load this processed data with all log combinations features
        x_1=x_data[col_top_5[0]] # First best performing curve-fitting column's feature values
        x_2=x_data[col_top_5[1]] # Second best performing curve-fitting column's feature values
        x_3=x_data[col_top_5[2]] # Third best performing curve-fitting column's feature values
        x_4=x_data[col_top_5[3]] # Fourth best performing curve-fitting column's feature values
        x_5=x_data[col_top_5[4]] # Fifth best performing curve-fitting column's feature values

        if((vali_mode=="random_vali") | (vali_mode=="no_vali")): # When the vali mode is random vali, return the below values
            return jsonify({'c1_preds': list(preds['Curve_1']), 'c2_preds':list(preds['Curve_2']), 'c3_preds':list(preds['Curve_3']), 'c4_preds':list(preds['Curve_4']), 'c5_preds':list(preds['Curve_5']),'actual':list(preds[preds.columns[0]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_c1_preds': list(pre['Curve_1']), 'sorted_c2_preds':list(pre['Curve_2']), 'sorted_c3_preds':list(pre['Curve_3']), 'sorted_c4_preds':list(pre['Curve_4']), 'sorted_c5_preds':list(pre['Curve_5']),'sorted_y1':list(pre[pre.columns[0]]), 'x_1':list(x_1),'x_2':list(x_2),'x_3':list(x_3),'x_4':list(x_4),'x_5':list(x_5), 'col_top_5':list(col_top_5) }) 
        
        if(vali_mode=="auto_select_vali"): # When the vali mode is auto grouping vali, return the below values
            vali_preds=load_data('all_models_vali_pred.csv') # Get all curve's validation prediction file - saved after curve-training phase
            group_count=int(len(vali_preds['Groups_generated'].unique())) # Count the total number of unqiue groups in the dataset
            return jsonify( {'c1_preds': list(preds['Curve_1']), 'c2_preds':list(preds['Curve_2']), 'c3_preds':list(preds['Curve_3']), 'c4_preds':list(preds['Curve_4']), 'c5_preds':list(preds['Curve_5']), 'actual':list(preds[preds.columns[0]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_c1_preds': list(pre['Curve_1']), 'sorted_c2_preds':list(pre['Curve_2']), 'sorted_c3_preds':list(pre['Curve_3']), 'sorted_c4_preds':list(pre['Curve_4']), 'sorted_c5_preds':list(pre['Curve_5']),'sorted_y1':list(pre[pre.columns[0]]), 'group_count': group_count, 'vali_x':list(vali_preds[vali_preds['Groups_generated']=='Group_1']['Distance_new']), 'vali_y1':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[0]]), 'vali_c1':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-5]]), 'vali_c2':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-4]]), 'vali_c3':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-3]]), 'vali_c4':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-2]]), 'vali_c5':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-1]]), 'x_all':list(data[data.columns[1]]), 'y_all':list(data[data.columns[2]]), 'x_vali':list(vali_preds[vali_preds.columns[2]]), 'y_vali': list(vali_preds[vali_preds.columns[3]]), 'x_1':list(x_1),'x_2':list(x_2),'x_3':list(x_3),'x_4':list(x_4),'x_5':list(x_5), 'col_top_5':list(col_top_5)  }) # We return values which are needed to plot graphs - most of the values are pretty straightforward. For some of these values, the reason we took only group 1's values is when the page or plot is refreshed - the first thing to load should be the group 1, this would allow the user to then change to any group number they want to view
        
        if(vali_mode=="upload_vali_csv"): # When the vali mode is upload external csv vali, return the below values
            vali_preds=load_data('all_models_vali_pred.csv') # Get all curve's validation prediction file - saved after curve-training phase
            return jsonify({ 'c1_preds': list(preds['Curve_1']), 'c2_preds':list(preds['Curve_2']), 'c3_preds':list(preds['Curve_3']), 'c4_preds':list(preds['Curve_4']), 'c5_preds':list(preds['Curve_5']),'actual':list(preds[preds.columns[0]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_c1_preds': list(pre['Curve_1']), 'sorted_c2_preds':list(pre['Curve_2']), 'sorted_c3_preds':list(pre['Curve_3']), 'sorted_c4_preds':list(pre['Curve_4']), 'sorted_c5_preds':list(pre['Curve_5']),'sorted_y1':list(pre[pre.columns[0]]), 'vali_x':list(vali_preds['Distance_new']), 'vali_y1':list(vali_preds[vali_preds.columns[0]]), 'vali_c1':list(vali_preds[vali_preds.columns[-5]]), 'vali_c2':list(vali_preds[vali_preds.columns[-4]]), 'vali_c3':list(vali_preds[vali_preds.columns[-3]]), 'vali_c4':list(vali_preds[vali_preds.columns[-2]]), 'vali_c5':list(vali_preds[vali_preds.columns[-1]]), 'x_1':list(x_1),'x_2':list(x_2),'x_3':list(x_3),'x_4':list(x_4),'x_5':list(x_5), 'col_top_5':list(col_top_5) })


#### SECTION 8: PREDICTION ON FULL-DATA, FUNCTIONS FOR CURVE-FITTING MODULE 

## BELOW FUNCTION IS NON-FUNTIONAL - WE ENTIRELY MOVE AWAY FROM THE METHOD OF COPYING FULL-DATA FILES WHICH ARE USUALLY LARGE IN SIZE, INSTEAD OPT TO A REMOTE ACCESS BASED METHOD
@app.route('/upload_fulldata_file_cf',methods=['POST','GET']) ## NON-FUNCTIONAL
def upload_fulldata_file_cf(): # Upload the input full-data csv from user, and copy to local file path - NEED TO MAKE PROCESSING CHANGES TO DATASET, SO DO NOT JUST EDIT THE FILE FROM THE ORIGINAL LOCATION OF THE FILE
    print("in file present")
    if( request.method=="POST" ): # If the request is POST
        if( request.files ): # If there are input files from the user
            file=request.files["file"] # request variable is the flask variable which contains all the information sent from javascript side to python for processing, we access values by request.files[ name_of_variable_given_in_javascript_side ]
            print(file)
            file.save(os.path.join(app.config['FILE_SAVE'], file.filename)) # We have already set app config to model_files. This will allow the flask app to store input data files directly into the model_files path. In this line we do file.save to save files with names file.filename into the recently set app.config value - which is model_files
            print(" file saved ")
            #f"{file.filename}
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'],"full_river_data_software_generated_cf.csv"))): # Rename the input data to full_river_data_software_generated_cf.csv. Note that the original location of the file is unchanged - meaning the original file untouched, but a copy of the file is stored in model_files folder and that file is renamed to full_river_data_software_generated_cf.csv which will be used almost everywhere in the code
                os.remove( os.path.join(app.config['FILE_SAVE'],"full_river_data_software_generated_cf.csv")) # If another file named full_river_data_software_generated_cf.csv already exists, then delete that and rename the new file to full_river_data_software_generated_cf.csv
            os.rename( os.path.join(app.config['FILE_SAVE'], file.filename), os.path.join(app.config['FILE_SAVE'],"full_river_data_software_generated_cf.csv") ) # rename the new file to full_river_data_software_generated_cf.csv
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'], file.filename)) ): # Remove the initial copy of the user's copied file - we only rename it, so this line won't probably run, but still if multiple copies of same file are created somehow, then delete them
                os.remove( os.path.join(app.config['FILE_SAVE'], file.filename) ) # Remove the duplicate files
            res = make_response(jsonify({"message":  f"File uploaded successfully "}),200) # Send back response to user saying files are uploaded

            return res
        return render_template('simplified_workflow.html/upload_fulldata_file_cf.html')



# full river input and process
@app.route('/full_river_input_cf',methods=['POST']) # This is a full-data part where the software uses trained models to make predictions on a whole large dataset
def full_river_input_cf():
    print("Merging")
    full_files_name_obj=request.form['full_files_name']
    full_files_name=json.loads(full_files_name_obj) # get the file name - location of the full-data csv file. We don't copy the file to the local 'model_files' folder because full-data is usually very large and we do not have to consume extra space, we can just get the file location path, and access/run functions on the dataset without having to copy the csv

    df=pd.read_csv(full_files_name[0], nrows=10)  # read first 10 rows and send to javascript to view in app page
    full_columns=df.columns # Get all the column names of the dataframe
    df_jsonfiles = json.loads(df.head().to_json(orient='records')) # Always load as json and then jsonify the data (next two lines) before sending - only way to communicate between python and javascript
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))


@app.route('/full_river_predictions_cf',methods=['POST','GET'])
def full_river_predictions_cf(): # This function takes in the full-data csv, processes it with the same functions and feature engineering methods that were used on the training dataset. After processing, load the trained model / trained curve-fit parameters, make predictions and store the output csv to the user's selected location
    print('in')
    feature_eq_obj=request.form['feature_eq']
    feature_eq=json.loads(feature_eq_obj) # Get what curve equation the user has selected to apply on full-data

    full_files_name_obj=request.form['full_files_name']
    full_files_name=json.loads(full_files_name_obj) # Get the file path

    sel_feats=request.form['feats']
    sel_cols3 = json.loads(sel_feats) # Columns selected for full-data

    actual_status=request.form['actual_status'] # If an actual target variable is present or not
    # actual_status=json.loads(actual_status_obj) 
    
    target_json=request.form['actual_variable']
    target=json.loads(target_json) # If actual target variable is present, then which variable is it

    send_feats_train_obj=request.form['send_feats_train']
    send_feats_train=json.loads(send_feats_train_obj) # The feature names which were used while training - it is important to use these names and rename the same columns selected for full-data (which might have different names ex: in training it can be 'R' column while in full-data that column might have the name 'R_05m') so rename them to apply the feature processing methods
    new_cols=send_feats_train[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)


    processed_data=pd.read_csv(full_files_name[0]) # Read dataset and store as dataframe
    processed_data=processed_data.dropna() # Drop the nan values
    processed_data=processed_data.reset_index().drop(['index'],axis=1) # Sort and drop index values

    colourspaces_json=request.form['colourspaces'] # Get what colourspace was used in training - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value
    # check if 0 value is present in r,g,b
    # edit test_dataset and add required colourspaces
    r=sel_cols3[3] #3 because of east north corrd input. Get R column name 
    g=sel_cols3[4] # Get G column name
    b=sel_cols3[5] # Get B column name 

    # Create proper rgb or hsv values as per the selected option
    if(processed_data.describe()[r]['min']==0):   # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                 
        processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
    if(processed_data.describe()[g]['min']==0):   # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
        processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
    if(processed_data.describe()[b]['min']==0):   # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
        processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

    if(processed_data.describe()[r]['min']<0):   # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                 
        processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
    if(processed_data.describe()[g]['min']<0):   # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                 
        processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
    if(processed_data.describe()[b]['min']<0):   # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                 
        processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

    if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
        print('same dataset - no hsv only rgb')

    elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
        print('same dataset - no hsv only rgb')

    elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
        print('change in dataset - no rgb only hsv')
        hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
        processed_data[r] = processed_data2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        processed_data[g] = processed_data2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        processed_data[b] = processed_data2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
        for ix, valx in enumerate(sel_cols3): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
            if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                sel_cols3[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
            if(valx == g):
                sel_cols3[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
            if(valx == b):
                sel_cols3[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
        
        # check if 0 value is present in r,g,b
        if(processed_data.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                 
            processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
        if(processed_data.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
            processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
        if(processed_data.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
            processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

        del processed_data2 # Delete the temporary dataframe

    elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
        print('change in dataset - rgb and hsv')
        hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
        processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        #insert in sel_cols3 
        sel_cols3.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        sel_cols3.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        sel_cols3.insert(8, 'V_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

        # check if 0 value is present in r,g,b
        if(processed_data.describe()['H_generated']['min']==0): # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                      
            processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
        if(processed_data.describe()['S_generated']['min']==0): # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                      
            processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
        if(processed_data.describe()['V_generated']['min']==0): # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                      
            processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

        del processed_data2 # Delete the temporary dataframe

    pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols3[1] # Get east coordinate column name
    north=sel_cols3[2] # Get north coordinate column name
    
    r=send_feats_train[3] # Get R column name
    g=send_feats_train[4] # Get G column name
    b=send_feats_train[5] # Get B column name
    sel_cols2=sel_cols3[3:] # Get all the column names from r,g,b, till the last selected feature - note that there is no target variable here
    
    if(actual_status=="Yes"): # Check if there is an actual target variable
        sel_tar=str(target) # Get the name of the target column
        print(" Target " , target)
    if(actual_status=="No"): # Check if there is no actual target variable
        sel_tar="none"  # If no target variable, make this string to none
        target="none"

    print("Actual status",actual_status)
    
    print(" Feature_eq ",feature_eq)
    print("features",sel_cols3)

    save_top_5=pd.read_csv('model_files/Save_top_5_cf.csv') # Load this csv - stored after input-training data processing. This csv contains info on the top 5 curve-fitting equations, their feature columns and metric scores 
    # orig_data=pd.read_csv('model_files/transformed_curve_fit.csv')

    # Now process the data in the exact way the training data was processed - and use this processed data to make predictions
    if(actual_status=="Yes"): # Check if there is an actual target variable
        predi=pd.DataFrame() # Initiate empty dataframe
        predi[sel_tar]=processed_data[sel_tar].copy() # If actual variable is present and selected by the user, then copy the target values and keep this as a separate dataframe and merge along with the predictions dataframe at the end
    elif(actual_status=="No"): # Check if there is no actual target variable
        predi=pd.DataFrame() # If no actual target variable, then just initiate empty dataframe

    to_merge=pd.DataFrame() # Empty dataframe to merge later
    merged=pd.DataFrame() # Empty dataframe to merge predictions of every batch processing iteration and the id, east and north of every batch-processing iteration - there is no batch processing of full-data in curve-fitting, but there is batch processing in simplified workflow since its computation load is huge
    to_merge=pd.DataFrame(processed_data[[pid,east,north]]) # Copy the id, east and north columns from the main dataframe to be processed, into the 'to_merge' dataframe - use this dataframe to merge with the  predictions dataframe at the end
    processed_data=processed_data[sel_cols2].copy() # Get the main dataframe - sel_cols2 is r,g,b and other main features - note that there is no target variable here
    old_cols=sel_cols3[3:] # To rename the full-data column names to be same as the input training-data column names, get all the current full-data column names from r,g,b, till the last selected feature
    print(old_cols, new_cols)
    for j in range(len(old_cols)): # For every current full-data column name
        processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True) # Like mentioned before, need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
        
    print("start",processed_data.shape)
    obj=save_top_5[save_top_5['Column_name']==feature_eq ]['Objective_name'].values[0] # Extract the objective number for the function which the user selected - to apply over the full-data (user selects the feature column )
    print( " Objective extracted : ", str(obj))
    
    paras_str= save_top_5[save_top_5['Column_name']==feature_eq ]['Paras'].values[0][1:-1].split(',') # This is to get the parameter / weights / coefficient values which we stored in the dataframe after the training phase in curve-fitting module 
    paras=[] # Initialze empty list for parameter values
    for i in range(len(paras_str)): # For all the string parameter values in the 'paras_str' list, convert to float values and store to another list
        paras.append( float(paras_str[i]) ) # Append the float values to new list
    print(paras)
        # create feats

    print(processed_data.head())
    data, ln_cols = _create_divlogln_features(processed_data,r,g,b,colourspaces) # Create the log and ln combinations of rgb or hsv values - based on colourspace values

    # choose only that column which is selected by user
    x=np.array( data[str(feature_eq)].values,dtype=float) # Convert the dataset of the selected feature column, to an array of float type - curve fitting works only on numpy arrays
    # orig_x=np.array( orig_data[str(feature_eq)].values, dtype=float )
    # orig_y=np.array( orig_data["Depth"].values, dtype=float )

    # Based on the obj value apply the proper objective function and get predictions on the data
    if( obj=="objective_1" ):
        a,b=paras # Get the parameters / coeefficients
        y_line = objective_1(x, a, b) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
        eq= str(round(a,3))+ '*x' + ' + ' + str(round(b,3))
        

    if( obj=="objective_2" ):
        a,b,c=paras # Get the parameters / coeefficients
        y_line = objective_2(x, a, b, c) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
        eq= ( str(round(a,3)) + '*x^2 ') + ' +'  + ( str(round(b,3)) + '*x') + ' + ' + str(round(c,3))

    if( obj=="objective_3" ):  
        a,b,c=paras # Get the parameters / coeefficients
        y_line = objective_3(x, a, b, c) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
        eq= str(round(a,3)) + ' + ' + '( ' + ( str(round(b,3)) + '* e^(' + str(-round(c,3)) + '*x)' ) + ' )'

    # if( obj=="objective_4" ): 
    #     a,b,c,d,e=paras
    #     y_line = objective_4(x, a, b, c, d, e)
    #     eq= str(round(a,3)) + ' + ' + '(' + str(b-c) + ')' + '/' + '( 1 + (x'  + '/' + str(d) + ')^' + str(e) + ') )'   

    # if( obj=="objective_5" ):
    #     a,b,c=paras
    #     y_line = objective_5(x, a, b, c)
    #     eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + '*' + 'sin(' + str(round(c,3)) + '-x) )'

    if( obj== "objective_4" ):
        a,b=paras # Get the parameters / coeefficients
        y_line = objective_4(x, a, b) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
        eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + ' * ' + 'log(x) )'

    predi['Curve_fit_Depth']=y_line # Store the final prediction values to the dataframe under the column name 'Curve_fit_Depth'
    merged=pd.concat([to_merge,predi],axis=1) # Merge all columns and predicted values into one dataframe
    full_river=merged.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
    del merged

    full_river=full_river.sort_values(by=pid) # Sort the values of ID column
    full_river=full_river.reset_index().drop(['index'],axis=1) # Reset the sorted dataframe and remove the index column
    print("Sample of the output")
    print(full_river.head())
    full_river.to_csv('model_files/'+'Curve_fit_Depth-Predictions.csv',index=False) # Save the output prediction csv

    # metrics
    jsonfiles_tms=''
    if(actual_status=="Yes"): # If actual target is present calculate the metrics between predicted and the actual target variables
        tm=train_metrics_cf( np.array( predi[sel_tar].values,dtype=float), y_line, str(feature_eq) ) # Get the metric scores by passing in the prediction values and the actual target values
        
        print(tm)
        
        full_river_preds_metrics_temp= tm.copy() # Save a copy of the metric scores
        full_river_preds_metrics_temp.to_csv('model_files/'+'Curve_fitting_fulldata_statistics.txt',index=False) # Save the final full-data metric scores as text file
        final_preds_full_concat=pd.concat([tm,full_river_preds_metrics_temp],axis=0) # Sometimes json object has problems sending data with just one row, so we add another row with same values as the first row
        final_preds_full_concat=final_preds_full_concat.round(3) # Round the metrics to 3 decimals
        jsonfiles_tms = json.loads( final_preds_full_concat[tm.columns].to_json(orient='records') ) # Now create and send a json object for the metric scores - since there is an extra row with same values as first row, just send them both - and in javascript we show only one row

    del predi
    jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records')) # Save the first 5 rows of the full-data predictions, as json object to show to the user using javascript 

    return jsonify({ 'preds': jsonfiles_full_river_preds, 'test_metrics': jsonfiles_tms })


@app.route('/save_model_files_cf', methods=['POST','GET'])
def save_model_files_cf(): # To save the output full-data prediction csv file to the user's preferred location

    file_path_obj= request.form['file_path'] # Get the file path object from javascript
    file_path=json.loads(file_path_obj) # Get the file path string from the object value
    print(file_path)
    
    shutil.copy('model_files/Curve_fit_Depth-Predictions.csv',file_path) # Copy the output prediction csv file to the selected location
    
    return jsonify("Done")












########## FOR TIF BASED INPUT FILE

# full river input and process
@app.route('/full_tif_input_cf',methods=['POST'])
def full_tif_input_cf(): # This function is to get the input raster files and send json object with details about raster band names, number of bands etc
    print("tif files mode")
    
    full_files_name_obj=request.form['full_files_name'] # Json object containing the tif file path name - it is a list of names - the first value is the rgb raster, the subsequent values are extra raster files
    full_files_name=json.loads(full_files_name_obj) # Convert the json list object to list of values
    count_raster = 0 # Initialize counter for number of raster files
    tif_files = [] # Initialize empty list to store all tif files
    rasterfile_bands_dict = {} # Initialize dict to store tif files info

    if os.path.exists('tif_processing'): # If this folder already exists
        shutil.rmtree('tif_processing', ignore_errors=True) # Delete that folder
    if not os.path.exists('tif_processing'): # If the folder doesn't exist
        os.mkdir('tif_processing') # Then create it - this is the folder where all the tif files and its split parts will be stored

    #full_files_name = ['C:/Users/PAVEETHRAN/Desktop/Bathymetry Tool/Electron Bathymetry/csv_to_tif/LakeColeridge_Chainage_1m.tif']
    for file in full_files_name: # For every file in this list
        file_ext=str(str(file).split('.')[-1])   # Get its extension
        if(file_ext=="tif"): # Check if the extension is tif, if yes then continue
            count_raster+=1  # Keeps count of number of tif files imported
            tif_files.append( file ) # Append the tif file to the list
            #for each raster, get number of bands present
            if(len(tif_files)>0 ):
                raster=rasterio.open( file ) # Use rasterio to open and read a tif file
                bands= raster.count # get the band count of the tif file

                rasterfile_bands_dict[file] = int(bands) # Store the band count as the value and the filename as the key, to this dict


    return jsonify( { 'count_raster':count_raster, 'rasterfile_bands_dict':rasterfile_bands_dict, 'tif_files':tif_files } )

# input - df: a Dataframe, chunkSize: the chunk size
# output - a list of DataFrame
# purpose - splits the DataFrame into smaller chunks 
# FOR TIF BASED FILES
# def split_dataframe(df, chunk_size = 1000000): 
#     chunks = list()
#     num_chunks = len(df) // chunk_size + 1
#     for i in range(num_chunks):
#         chunks.append(df[i*chunk_size:(i+1)*chunk_size])
#     return chunks


@app.route('/full_river_predictions_cf_tif',methods=['POST','GET'])
def full_river_predictions_cf_tif(): # This function takes in the full-data tif, splits into several tif files, batch processes them with the same functions and feature engineering methods that were used on the training dataset. After processing, load the trained model / trained curve-fit parameters, make predictions on each tif, and after all batches, merge all separate output tif files into one tif file and store that output tif to the user's selected location
    
    # Remove all output tifs if present
    import glob,os
    demList1 = glob.glob("tif_processing/output_[0-9]_part.tif") # Get all the list of files with [0-9] (1-10) names
    demList2 = glob.glob("tif_processing/output_[0-9][0-9]_part.tif") # Get all the list of files with [0-9][0-9] (10-100) names
    demList3 = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Get all the list of files with [0-9][0-9][0-9] (100-400) names

    demList=[] # Initialize empty list
    for i in demList1: # For all file names in this list
        demList.append(i) # Append to the main list
    for i in demList2: # For all file names in this list
        demList.append(i) # Append to the main list
    for i in demList3: # For all file names in this list
        demList.append(i) # Append to the main list

    for i in demList: # For all file names in the final list
        os.remove(i) # If there are any files with the names present in this list, remove them files
        
    feature_eq_obj=request.form['feature_eq'] # Get what curve equation the user has selected to apply on full-data - this is a json object
    feature_eq=json.loads(feature_eq_obj) # Convert the json object to string
    
    full_files_name_obj=request.form['tif_files'] # Get the tif files path as json list object
    full_files_name=json.loads(full_files_name_obj) # Convert the json object to list
    
    sel_feats=request.form['feats'] # Columns selected for full-data
    sel_cols3 = json.loads(sel_feats)
    
    actual_status=request.form['actual_status'] # If an actual target variable is present or not - for TIF based full-data, we have kept no actual status value by default
    # actual_status=json.loads(actual_status_obj)

    send_feats_train_obj=request.form['send_feats_train'] # The feature names which were used while training - it is important to use these names and rename the same columns selected for full-data (which might have different names ex: in training it can be 'R' column while in full-data that column might have the name 'R_05m') so rename them to apply the feature processing methods
    send_feats_train=json.loads(send_feats_train_obj) # Convert the json object to list
    new_cols=send_feats_train[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)

    pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols3[1] # Get east coordinate column name
    north=sel_cols3[2] # Get north coordinate column name
    
    r=send_feats_train[3] # Get R column name
    g=send_feats_train[4] # Get G column name
    b=send_feats_train[5] # Get B column name
    
    if(actual_status=="No"):  # Check if there is no actual target variable
        sel_tar="none" # If no target variable, make this string to none

    colourspaces_json=request.form['colourspaces'] # Get what colourspace was used in training - this is a json object
    colourspaces  = json.loads(colourspaces_json)  # Convert the json object to string value

    print("Actual status",actual_status)
    print(" Feature eqn",feature_eq)
    print("features",sel_cols3)

    # Get band names
    band_name_number_json=request.form['band_name_number'] # Get json object for the band details dict that contains the band names and band number in the order of input files - first would be the rgb raster band names and then band name and its number for other rasters
    band_name_number  = json.loads(band_name_number_json) # Convert the json object to dict

    print(band_name_number)

    rasterfile_bands_dict_json=request.form['rasterfile_bands_dict']
    rasterfile_bands_dict  = json.loads(rasterfile_bands_dict_json) # This variable has all the details of each band of every raster file

    print(rasterfile_bands_dict)

    
    # full_data=pd.read_csv(full_files_name[0], chunksize=1000000)
    #### Create csv from tif file
    import os,gc
    import gdal
    import geopandas as gpd
    from osgeo import gdal
    import rasterio

    # Read only RGB raster first
    raster_filename = full_files_name[0]
    print(raster_filename)
    
    output_count=0 # Initialze this to 0 - this is a counter to keep count of the output file number for every split


    # Warp all other rasters except the first (rgb) raster
    from osgeo import gdal
    dem = gdal.Open(raster_filename) # get raster file dem
    gt = dem.GetGeoTransform() # get extents
    print(gt)

    # get coordinates of upper left corner
    xmin = gt[0]
    ymax = gt[3]
    res_all = gt[1]

    ulx, xres, xskew, uly, yskew, yres = dem.GetGeoTransform()
    lrx = ulx + (dem.RasterXSize*xres) # Calculate lower right and bottom coordinates
    lry = uly + (dem.RasterYSize*yres)

    rastfile_warp = {}
    # Save the raster object in a dict with filename as key
    for rast_file in full_files_name: # For all files in the list
        if(rast_file!=raster_filename): # Proceed only if the file name is not the first raster file - which is the rgb raster
            print(" Warping raster file ", rast_file)
            # Warp the chainage raster
            # We are warping the chainage raster to match its extents and projection to that of the main rgb raster - only if they match, we can extract band values on those matching coordinates
            ds = gdal.Open(rast_file,1) # Use gdal to open and read the raster file
            src=None # Make this to none first
            src = gdal.Warp("", ds,format="vrt",
                outputBounds=[ ulx, lry, lrx, uly ], 
                xRes=res_all, yRes=-res_all,
                resampleAlg=gdal.GRA_NearestNeighbour, options=['COMPRESS=DEFLATE']) # This function warps the raster to match with the given output bounds and resolution. 
            print(src.GetGeoTransform())

            rastfile_warp[str(rast_file)] = src # Save each dem object of each raster file to a list and use it later
            src = None # Make it none and use this for next raster file in next loop
            ds = None # Make it none and use this for next raster file in next loop

    # determine total length of raster
    xlen = res_all * dem.RasterXSize
    ylen = res_all * dem.RasterYSize

    # number of tiles in x and y direction
    div = 20 # Time taken for the whole process till end:  For large data (500mb input raster file) - 30 div = 3590 sec, 15 div = 3000 sec, 10 div = 3300 sec. For small data (50mb input raster file) - 5 div = 343 sec, 15 div = 299 sec, 50 div = 817 sec, 30 div = 400 sec
    # ydiv = 2

    # size of a single tile
    xsize = xlen/div
    ysize = ylen/div

    # create lists of x and y coordinates
    xsteps = [xmin + xsize * iq for iq in range(div+1)]
    ysteps = [ymax - ysize * iq for iq in range(div+1)]
    
    
    # loop over min, max x and y coordinates
    for ii in range(div):
        for jj in range(div):
            xmin_all = xsteps[ii] # Get initial end of the current split-square x coordinate (left-top corner)
            xmax_all = xsteps[ii+1] # Get far end of the current split-square x coordinate (right-top corner)
            ymax_all = ysteps[jj] # Get initial end of the current split-square y coordinate (left-bottom corner)
            ymin_all = ysteps[jj+1] # Get far end of the current split-square y coordinate (right-bottom corner)
            
            print("xmin: "+str(xmin_all))
            print("xmax: "+str(xmax_all))
            print("ymin: "+str(ymin_all))
            print("ymax: "+str(ymax_all))
            print("\n")

            # This line is unique to curve fitting module only
            sel_cols3 = json.loads(sel_feats) # Load the column names selected for full-data

            output_count += 1 # For every square (split) or for every loop, increment this counter - represents the output number of each tif split part
            main_raster_obj = None # Main raster - rgb raster
            other_raster_obj = None # Other rasters - chainage, weight etc.  rasters

            main_raster_obj = gdal.Translate("tif_processing/input_"+ str(output_count) +"_part.tif", dem, projWin = (xmin_all, ymax_all, xmax_all, ymin_all), xRes = res_all, yRes = -res_all, creationOptions=["COMPRESS=DEFLATE"]) # Split into a small part
            # First get RGB values from main raster (first input raster is always the main raster)
                
            nodata_val = main_raster_obj.GetRasterBand(1).GetNoDataValue() # Get the nodata value of the rgb raster
            print(nodata_val)
            if(nodata_val==None): # If nodata value is none then make it 256 for rgb raster
                print('None nodata value')
                nodata_val =  np.float(256) # 256 - nodatavalue for rgb raster
                print(nodata_val)
            band1=main_raster_obj.GetRasterBand(1).ReadAsArray() # Read the first band of the main rgb raster as array
            print(band1.shape)
            xmin = main_raster_obj.GetGeoTransform()[0] # Get lower extents
            ymax = main_raster_obj.GetGeoTransform()[3] # Get top extents
            xsize = main_raster_obj.RasterXSize # Get total length in X axis
            ysize = main_raster_obj.RasterYSize # Get total length in Y axis
            xstart = xmin +main_raster_obj.GetGeoTransform()[1]/2 # Get the start point - this should be the mid-point of the left bottom most grid - that is why we add ( for X-axis go right side from min X ) half of the resolution value to the initial start point - to get the mid-point 
            ystart = ymax - main_raster_obj.GetGeoTransform()[1]/2 # Get the start point - this should be the mid-point of the right topmost grid - that is why we subtract ( for Y-axis go down side from max Y ) half of the resolution value to the initial start point - to get the mid-point 
            res = main_raster_obj.GetGeoTransform()[1] # Save the resolution value

            x = np.arange(xstart, xstart+xsize*res, res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the initial grid's mid-point and add half resolution values all the way till topmost grid
            y = np.arange(ystart, ystart-ysize*res, -res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the topmost grid's mid-point and subtract half resolution values all the way till bottom most grid
            
            # The X values should in ascending order and Y values in descending order
            
            # Sometimes translating a raster leaves some pixels in the corners behind and so the shape of the raster slightly changes. To match with the original shape, we compare the original shape and translated raster and add/remove pixel values to match them both
            print(x.shape,y.shape)
            print(y[-1])
            if(x.shape[0]>band1.shape[1]): # If the created x array has more values than the band's x values
                x_diff = x.shape[0]-band1.shape[1] # Then remove the extra values. This line gets the count of extra values
                x=x[:-x_diff] # Slice the array by removing the extra value's indices

            if(x.shape[0]<band1.shape[1]): # If the created x array has less values than the band's x values
                x_diff = band1.shape[1] - x.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                for idf in range(x_diff): # For each extra value needed
                    x = np.insert(x, x[-1]+res) # Adding one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

            if(y.shape[0]>band1.shape[0]): # If the created x array has more values than the band's y values
                y_diff = y.shape[0]-band1.shape[0] # Then remove the extra values. This line gets the count of extra values
                y=y[:-y_diff] # Slice the array by removing the extra value's indices

            if(y.shape[0]<band1.shape[0]):  # If the created x array has less values than the band's y values
                y_diff = band1.shape[0] - y.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                for idf in range(y_diff):  # For each extra value needed
                    y = np.insert(y, y[-1]-res) # Subtracting one more coordinate value with value equal to the previous value subtracted by the resolution value - this will be the center point of next below grid

            print(y[-1])

            X1,Y1 = np.meshgrid(x,y,copy=False) # Create a meshgrid  - this will output two variables which have same format as the band values from raster - look up meshgrid to know more. Important - meshgrid should be in this format : X values should in ascending order and Y values in descending order

            del x,y # Delete this x and y
            gc.collect()
            save_this_df = pd.DataFrame(columns=['East_generated','North_generated']) # Create a new dataframe with east and north column names as specified
            save_this_df['East_generated'] = X1.flatten() # Flattening will convert to 1-d array from n-d array
            save_this_df['North_generated'] = Y1.flatten() # Flattening will convert to 1-d array from n-d array
            print('With all nan and values ', save_this_df.shape)
            gc.collect()   

            # # here get nodata values and remove
            mr = np.ma.masked_equal(band1,nodata_val) ## Try to get new mask spcific to each band/file - because a lot of times they might not be the same as the rgb raster and using the same maks as the rgb raster might give nodata values 
            new_x = np.ma.masked_array(X1, mr.mask) # Apply the mask to x and y meshgrid values
            new_y = np.ma.masked_array(Y1, mr.mask) # Apply the mask to x and y meshgrid values
            del X1,Y1
            gc.collect()
            new_x = np.ma.compressed(new_x) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
            new_y = np.ma.compressed(new_y) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
            gc.collect()
            print(new_x.shape)
            print(new_y.shape)

            df = pd.DataFrame(columns=['East_generated','North_generated']) # Create another dataframe with these east and north column names
            df['East_generated']=np.array(new_x) # Copy the compressed east coordinate array values into the east column
            df['North_generated']=np.array(new_y)  # Copy the compressed north coordinate array values into the north column # Store x and y to dataframe- these do not have any nan or no-data values
            del new_x,new_y
            gc.collect()

            for i in sel_cols3[3:6]: # Now using that mask to apply on three bands of the raster - to get r,g,b. Also, find the band number of r,g,b instead of taking first three bands directly - they might be in different order
                band_number = band_name_number[i] # Get band number of each feature (r,g,b) from this list
                band1 = main_raster_obj.GetRasterBand(band_number).ReadAsArray() # Get the band values as array
                new_band1 = np.ma.masked_array(band1, mr.mask) #Apply mask for band values
                del band1
                new_band1 = np.ma.compressed(new_band1) # Compress to get only values without nodata values
                df[i]=np.array(new_band1) # Copy the compressed band values into a new column named after the Ith feature name # Store to dataframe

            del new_band1
            gc.collect()


            df = df.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values
            df=df.reset_index().drop(['index'],axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
            print( " Main raster df ")
            print(df.shape)
            print(df.describe())
            print(df.head())

            print("  THESE MANY ROWS ARE AVAILABLE",len(df))

            if(len(df) == 0 ): # If there are no proper values - when we split tif files into many parts, some parts may not have any band values - just nodata values. So once we remove all no-data values, there will be no other points left - in that case just write this tif file back to output tif and merge it at the end with all other output split parts
                save_this_df['NODATA']=np.float32(nodata_val) # Get the nodata values and store to this dataframe
                mdf = np.array(save_this_df['NODATA'].values, dtype=np.float32) # Convert to array and change data type to float32 - uses less memory to store files
                import rasterio as rio   
                with rio.open("tif_processing/input_"+ str(output_count) +"_part.tif") as src2: # Read the input split file and copy its metadata to store it to output tif file of this specific split part
                    ras_data = src2.read() # Read the raster data
                    ras_meta = src2.profile # Read the meta data of raster
                # make any necessary changes to raster properties, e.g.:
                
                ras_meta['dtype'] = "float32" # Since all values are nodata values, we can just use a float 32 value to store them - higher the precision- more storage the tif file would consume
                ras_meta['nodata'] = np.float32(nodata_val)
                ras_meta['count'] = 1 # Specify only one band in output tif file
                ras_meta['options']=['COMPRESS=DEFLATE']
                # inDs = gdal.Open("input_"+ str(output_count) +"_part.tif")
                ras_meta['crs'] = main_raster_obj.GetProjection() # Get CRS on main rgb raster file and save it as the meta data crs of the output raster file
                
                if(output_count<10): # Store the file in format 001,002 upto 400 - since we divide the main raster into 20x20 filter, there will be 400 parts
                    with rio.open("tif_processing/output_00"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst: 
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 001 to 010
                elif(output_count>10 and output_count<100):
                    with rio.open("tif_processing/output_0"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 010 to 099
                else:
                    with rio.open("tif_processing/output_"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1)   # Store for output number values between 100 to 400

                dst = None
                src2= None
                main_raster_obj =None
                inDs =None

                # gdal.Translate('output_' + str(output_count) +'_part.tif', dem, projWin = (xmin, ymax, xmax, ymin), xRes = res, yRes = -res, dtype='float32' ,options=['COMPRESS=DEFLATE'] )
                # main_raster_obj = None
                import os
                os.remove("tif_processing/input_"+ str(output_count)+"_part.tif") # Remove the input split file of this iteration

            # Now if there are actual values present, which are not nodata values - then get other raster values like chainage etc. by matching the coordiante x and y values 
            if(len(df)>0):
                for i in sel_cols3[6:]: # For rest of the columns other than r,g,b
                    # for multiple files
                    band_numb = band_name_number[i] # Get the band number of the Ith feature name
                    total_count = 0
                    determined_band_numb = 0
                    loop_done=0

                    # determine which file it belongs to
                    # For every file do the same as done earlier - get the specific split part( we already computed xmin xmax ymin ymax above), get this small part, check for proper values and merge with dataframe
                    for fl in full_files_name: # Go through all files and check which file has the band values / band name of the particular iteration (Ith feature name)
                        if(loop_done==0): # As long as loop is active
                            fl_count = rasterfile_bands_dict[fl] # For every file , get the total band count from this dict
                            total_count = total_count + fl_count # Add the total count to band count
                            print(fl_count, fl,i, band_numb, total_count )
                            if( (band_numb > total_count) == False ): # Compare with the values from javascript - if the band number of this feature is not greater than the total count - it means this band must be in this file. So we search more in this particular file 
                                file_name_val = fl # We set the file name to the current iteration's filename

                                # determine the band number in that file name
                                total_count = total_count - fl_count # Subtract the total count from the file's total band count - this is to reset the count to the start value of this band. So now, we can have a loop over the total band count of the file, keep adding 1 to the total count and see if the values match. If they match - that Kth value is the band number of the Ith feature in that raster file
                                for k in range( fl_count ): # K is the iteraction count for the total band count of this raster file 'fl'
                                    if( band_numb == ( total_count + (k+1) ) ): # If the band number from the list matches with the total count + Kth value+1 - then K+1 is the  band number
                                        src = None
                                        determined_band_numb = k + 1 # Set band number of the Ith feature
                                        print(determined_band_numb,file_name_val)
                                        
                                        # Get the raster object of the file
                                        src = rastfile_warp[str(file_name_val)] # Get the dem object saved earlier, for the raster filename 'file_name_val' 

                                        # For every tif split (tile), get the same split from other files 
                                        other_raster_obj= gdal.Translate("", src, format='vrt', projWin = (xmin_all, ymax_all, xmax_all, ymin_all), xRes = res_all, yRes = -res_all)

                                        nodata_val_ch = other_raster_obj.GetRasterBand(determined_band_numb).GetNoDataValue() # Get the nodata value of the raster
                                        print(nodata_val_ch)
                                        if(nodata_val_ch==None): # If nodata value is none then make it -3.402823e+38 for non-rgb raster
                                            print('None nodata')
                                            nodata_val_ch= np.float(-3.402823e+38) # nodatavalue for non-rgb raster
                                            print(nodata_val_ch)
                                        
                                        band1 = other_raster_obj.GetRasterBand(determined_band_numb).ReadAsArray() # Read the first band of the main rgb raster as array
                                        xmin = other_raster_obj.GetGeoTransform()[0] # Get lower extents
                                        ymax = other_raster_obj.GetGeoTransform()[3] # Get top extents
                                        xsize = other_raster_obj.RasterXSize # Get total length in X axis
                                        ysize = other_raster_obj.RasterYSize # Get total length in Y axis
                                        xstart = xmin + other_raster_obj.GetGeoTransform()[1] / 2 # Get the start point - this should be the mid-point of the left bottom most grid - that is why we add ( for X-axis go right side from min X ) half of the resolution value to the initial start point - to get the mid-point 
                                        ystart = ymax - other_raster_obj.GetGeoTransform()[1] / 2 # Get the start point - this should be the mid-point of the right topmost grid - that is why we subtract ( for Y-axis go down side from max Y ) half of the resolution value to the initial start point - to get the mid-point 
                                        res = other_raster_obj.GetGeoTransform()[1] # Save the resolution value

                                        x = np.arange(xstart, xstart + xsize * res, res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the initial grid's mid-point and add half resolution values all the way till topmost grid
                                        y = np.arange(ystart, ystart - ysize * res, -res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the topmost grid's mid-point and subtract half resolution values all the way till bottom most grid

                                        # The X values should in ascending order and Y values in descending order
                                        
                                        # Sometimes translating a raster leaves some pixels in the corners behind and so the shape of the raster slightly changes. To match with the original shape, we compare the original shape and translated raster and add/remove pixel values to match them both
                                        print(band1.shape)
                                        print(x.shape,y.shape)
                                        if(x.shape[0]>band1.shape[1]): # If the created x array has more values than the band's x values
                                            x_diff = x.shape[0]-band1.shape[1] # Then remove the extra values. This line gets the count of extra values
                                            x=x[:-x_diff] # Slice the array by removing the extra value's indices

                                        if(x.shape[0]<band1.shape[1]): # If the created x array has less values than the band's x values
                                            x_diff = band1.shape[1] - x.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                                            for i in range(x_diff): # For each extra value needed
                                                x = np.insert(x, x[-1]+res) # Adding one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

                                        if(y.shape[0]>band1.shape[0]): # If the created x array has more values than the band's y values
                                            y_diff = y.shape[0]-band1.shape[0] # Then remove the extra values. This line gets the count of extra values
                                            y=y[:-y_diff] # Slice the array by removing the extra value's indices

                                        if(y.shape[0]<band1.shape[0]): # If the created x array has less values than the band's y values
                                            y_diff = band1.shape[0] - y.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                                            for i in range(y_diff): # For each extra value needed
                                                y = np.insert(y, y[-1]-res) # Subtracting one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

                                        X1, Y1 = np.meshgrid(x, y, copy=False) # Create a meshgrid  - this will output two variables which have same format as the band values from raster - look up meshgrid to know more. Important - meshgrid should be in this format : X values should in ascending order and Y values in descending order
                                        del x
                                        del y
                                        gc.collect()

                                        
                                        mr = np.ma.masked_equal(band1, nodata_val_ch)  ## Try to get new mask spcific to each band/file - because a lot of times they might not be the same as the rgb raster and using the same maks as the rgb raster might give nodata values 
                                        new_x = np.ma.masked_array(X1, mr.mask) # Apply the mask to x and y meshgrid values
                                        new_y = np.ma.masked_array(Y1, mr.mask) # Apply the mask to x and y meshgrid values

                                        del X1,Y1
                                        gc.collect()
                                        new_x = np.ma.compressed(new_x) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
                                        new_y = np.ma.compressed(new_y) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 

                                        temp_df = pd.DataFrame(columns=['East_generated', 'North_generated', i]) # Create another dataframe with these east and north column names
                                        temp_df['East_generated'] = np.array(new_x) # Copy the compressed east coordinate array values into the east column
                                        temp_df['North_generated'] = np.array(new_y) # Copy the compressed north coordinate array values into the north column

                                        del new_x
                                        del new_y
                                        gc.collect()

                                        new_band1 = np.ma.masked_array(band1, mr.mask) #Apply mask for band values
                                        del band1
                                        new_band1 = np.ma.compressed(new_band1) # Compress to get only values without nodata values

                                        temp_df[i] = np.array(new_band1) # Copy the compressed band values into a new column named after the Ith feature name # Store to dataframe
                                        temp_df = temp_df.drop_duplicates(subset=['East_generated','North_generated'], keep='first')  # More often there are duplicates - so reduce computation load by removing duplicate values
                                        temp_df = temp_df.reset_index().drop(['index'], axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                                        df = pd.merge(df, temp_df, how='inner', on=['East_generated','North_generated']) # Now merge on common values from df and temp_df - this appends all feature columns from different iteration of features, into one single dataframe
                                        print (' Adding other raster df ')
                                        print(df.head())
                                        print(df.shape)


                gc.collect()

                processed_data=pd.DataFrame()
                processed_data=df.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                processed_data['ID'] = np.array(processed_data.index) # Add an ID column
                print('After all bands')
                print(processed_data.head())
                print(processed_data.shape)
    
    
                ## This df is the final full-data for processing
                # full_data = split_dataframe(df)
                r=sel_cols3[3] #3 because of east north corrd input. Get R column name 
                g=sel_cols3[4] # Get G column name
                b=sel_cols3[5] # Get B column name
                
                # Create proper rgb or hsv values as per the selected option
                if(processed_data.describe()[r]['min']==0):     # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                
                    processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
                if(processed_data.describe()[g]['min']==0):     # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                             
                    processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
                if(processed_data.describe()[b]['min']==0):     # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                              
                    processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

                if(processed_data.describe()[r]['min']<0):   # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                  
                    processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
                if(processed_data.describe()[g]['min']<0):   # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                 
                    processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
                if(processed_data.describe()[b]['min']<0):   # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                 
                    processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

                if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
                    print('same dataset - no hsv only rgb')

                elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
                    print('same dataset - no hsv only rgb')

                elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
                    print('change in dataset - no rgb only hsv')
                    hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                    processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                    processed_data[r] = processed_data2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                    processed_data[g] = processed_data2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                    processed_data[b] = processed_data2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                    processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
                    for ix, valx in enumerate(sel_cols3): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
                        if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                            sel_cols3[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
                        if(valx == g):
                            sel_cols3[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
                        if(valx == b):
                            sel_cols3[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
                    
                    # check if 0 value is present in r,g,b
                    if(processed_data.describe()['H_generated']['min']==0):    # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                 
                        processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                    if(processed_data.describe()['S_generated']['min']==0):    # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                               
                        processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                    if(processed_data.describe()['V_generated']['min']==0):    # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                               
                        processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                    del processed_data2 # Delete the temporary dataframe

                elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
                    print('change in dataset - rgb and hsv')
                    hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                    processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                    processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                    processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                    processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                    #insert in sel_cols3 
                    sel_cols3.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                    sel_cols3.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                    sel_cols3.insert(8, 'V_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

                    # check if 0 value is present in r,g,b
                    if(processed_data.describe()['H_generated']['min']==0):  # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                       
                        processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                    if(processed_data.describe()['S_generated']['min']==0):  # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                       
                        processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                    if(processed_data.describe()['V_generated']['min']==0):  # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                       
                        processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                    del processed_data2 # Delete the temporary dataframe

                pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
                east=sel_cols3[1] # Get east coordinate column name
                north=sel_cols3[2] # Get north coordinate column name
                
                r=send_feats_train[3] # Get R column name
                g=send_feats_train[4] # Get G column name
                b=send_feats_train[5] # Get B column name
                sel_cols2=sel_cols3[3:] # Get all the column names from r,g,b, till the last selected feature - note that there is no target variable here
                
                if(actual_status=="Yes"): # Check if there is an actual target variable
                    sel_tar=str(target) # Get the name of the target column
                    print(" Target " , target)
                if(actual_status=="No"): # Check if there is no actual target variable
                    sel_tar="none"  # If no target variable, make this string to none
                    target="none"

                print("Actual status",actual_status)
                
                print(" Feature_eq ",feature_eq)
                print("features",sel_cols3)

                save_top_5=pd.read_csv('model_files/Save_top_5_cf.csv') # Load this csv - stored after input-training data processing. This csv contains info on the top 5 curve-fitting equations, their feature columns and metric scores 
                # orig_data=pd.read_csv('model_files/transformed_curve_fit.csv')

                # Now process the data in the exact way the training data was processed - and use this processed data to make predictions
                if(actual_status=="Yes"): # Check if there is an actual target variable
                    predi=pd.DataFrame()  # Initiate empty dataframe
                    predi[sel_tar]=processed_data[sel_tar].copy() # If actual variable is present and selected by the user, then copy the target values and keep this as a separate dataframe and merge along with the predictions dataframe at the end
                elif(actual_status=="No"): # Check if there is no actual target variable
                    predi=pd.DataFrame() # If no actual target variable, then just initiate empty dataframe

                to_merge=pd.DataFrame() # Empty dataframe to merge later
                merged=pd.DataFrame() # Empty dataframe to merge predictions of every batch processing iteration and the id, east and north of every batch-processing iteration - there is no batch processing of full-data in curve-fitting, but there is batch processing in simplified workflow since its computation load is huge
                to_merge=pd.DataFrame(processed_data[[pid,east,north]]) # Copy the id, east and north columns from the main dataframe to be processed, into the 'to_merge' dataframe - use this dataframe to merge with the  predictions dataframe at the end
                processed_data=processed_data[sel_cols2].copy() # Get the main dataframe - sel_cols2 is r,g,b and other main features - note that there is no target variable here
                old_cols=sel_cols3[3:] # To rename the full-data column names to be same as the input training-data column names, get all the current full-data column names from r,g,b, till the last selected feature
                print(old_cols, new_cols)
                for j in range(len(old_cols)): # For every current full-data column name
                    processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True) # Like mentioned before, need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
                    
                print("start",processed_data.shape)
                obj=save_top_5[save_top_5['Column_name']==feature_eq ]['Objective_name'].values[0] # Extract the objective number for the function which the user selected - to apply over the full-data (user selects the feature column )
                print( " Objective extracted : ", str(obj))
                
                paras_str= save_top_5[save_top_5['Column_name']==feature_eq ]['Paras'].values[0][1:-1].split(',') # This is to get the parameter / weights / coefficient values which we stored in the dataframe after the training phase in curve-fitting module 
                paras=[] # Initialze empty list for parameter values
                for i in range(len(paras_str)): # For all the string parameter values in the 'paras_str' list, convert to float values and store to another list
                    paras.append( float(paras_str[i]) ) # Append the float values to new list
                print(paras)
                    # create feats

                print(processed_data.head())
                data, ln_cols = _create_divlogln_features(processed_data,r,g,b,colourspaces) # Create the log and ln combinations of rgb or hsv values - based on colourspace values

                # choose only that column which is selected by user
                x=np.array( data[str(feature_eq)].values,dtype=float) # Convert the dataset of the selected feature column, to an array of float type - curve fitting works only on numpy arrays
                
                # Based on the obj value apply the proper objective function and get predictions on the data
                if( obj=="objective_1" ):
                    a,b=paras # Get the parameters / coeefficients
                    y_line = objective_1(x, a, b) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
                    eq= str(round(a,3))+ '*x' + ' + ' + str(round(b,3))
                    

                if( obj=="objective_2" ):
                    a,b,c=paras # Get the parameters / coeefficients
                    y_line = objective_2(x, a, b, c) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
                    eq= ( str(round(a,3)) + '*x^2 ') + ' +'  + ( str(round(b,3)) + '*x') + ' + ' + str(round(c,3))

                if( obj=="objective_3" ):  
                    a,b,c=paras # Get the parameters / coeefficients
                    y_line = objective_3(x, a, b, c) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
                    eq= str(round(a,3)) + ' + ' + '( ' + ( str(round(b,3)) + '* e^(' + str(-round(c,3)) + '*x)' ) + ' )'

                # if( obj=="objective_4" ): 
                #     a,b,c,d,e=paras
                #     y_line = objective_4(x, a, b, c, d, e)
                #     eq= str(round(a,3)) + ' + ' + '(' + str(b-c) + ')' + '/' + '( 1 + (x'  + '/' + str(d) + ')^' + str(e) + ') )'   

                # if( obj=="objective_5" ):
                #     a,b,c=paras
                #     y_line = objective_5(x, a, b, c)
                #     eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + '*' + 'sin(' + str(round(c,3)) + '-x) )'

                if( obj== "objective_4" ):
                    a,b=paras # Get the parameters / coeefficients
                    y_line = objective_4(x, a, b) # Use the trained parameters to create the equation and use it to predict on the processed full-data values
                    eq= str(round(a,3)) + ' + ' + '( ' + str(round(b,3)) + ' * ' + 'log(x) )'

                predi['Curve_fit_Depth']=y_line # Store the final prediction values to the dataframe under the column name 'Curve_fit_Depth'
                merged=pd.concat([to_merge,predi],axis=1) # Merge all columns and predicted values into one dataframe
                full_river=merged.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
                del merged,predi,processed_data

                col_name="Curve_fit_Depth"
                full_river=full_river.sort_values(by=pid) # Sort the values of ID column
                full_river=full_river.reset_index().drop(['index'],axis=1) # Reset the sorted dataframe and remove the index column
                
                print("Sample of the output")
                print(full_river.head())
                
                # full_river.to_csv('model_files/'+'Curve_fit_Depth-Predictions.csv',index=False)
                jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records')) # Save the first 5 rows of the full-data predictions, as json object to show to the user using javascript  
                preds_file_name='Predictions_Equation'+str( ( np.array(save_top_5[save_top_5['Column_name']==feature_eq ].index)[0]+ 1 )  ).lower()+'.tif' # get the selected curve-feature to copy that name to the output file's name

                ## Convert back to raster tif file
                full_river = full_river.drop(['ID'],axis=1) # Drop the ID column which was created during the process
                
                full_river = full_river.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values 
                full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                
                #save_this_df = pd.merge(save_this_df, full_river,  how='left', on=['East_generated','North_generated']).fillna(float(nodata_val))

                mdf=pd.DataFrame()
                # Now we have the output predictions and east,north coordinates in a dataframe. We need to merge with the save_this_df dataframe - this dataframe contains all east, north coordinates. All coordinates includes the ones we removed because they had no-data values.
                # The reason to merge it is when we create output tif file, the array values should match the exact number of values as was the input band. After removing nodata values, there will be change in number of values, so we need to add those nodata values back and also make sure we add coordinates at the right indices - should not be random.
                chks = split_dataframe(save_this_df) # We split the dataframe because merge function is computationally very heavy and splitting into smaller chunks and merging each chunk based on the east and north coordinates will be efficient
                del save_this_df

                for i in chks: # For each chunk
                    print(i.shape)
                    mdf1 = pd.merge(i, full_river,  how='left', on=['East_generated',
                                    'North_generated']).fillna(float(nodata_val)) # Merge this chunk of predictions with the east, north data 
                    mdf = pd.concat([mdf, mdf1 ],axis=0) # Merge the updated chunk with the previously merged rows
                    del i
                    del mdf1
                    gc.collect()
                    mdf = mdf.reset_index().drop(['index'],axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                del chks,full_river
                gc.collect()
                print(mdf.shape)
                print(mdf.head())

                print('merge done')
                # Now we have complete dataframe with number of values matching this particular split part's total raster values, so convert to tif and store with output number
                mdf = mdf.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                mdf = np.array( mdf[ col_name ].values, dtype=np.float32 ) # Get the prediction column values and change data type to float32 - uses less memory to store files
                
                # Read main rgb file and get its extents to output tif
                xsize = main_raster_obj.RasterXSize # Get total length in X axis
                ysize = main_raster_obj.RasterYSize # Get total length in Y axis

                import rasterio as rio   
                with rio.open("tif_processing/input_"+ str(output_count) +"_part.tif") as src2: # Read the input split file and copy its metadata to store it to output tif file of this specific split part
                    ras_data = src2.read() # Read the raster data
                    ras_meta = src2.profile # Read the meta data of raster

                # make any necessary changes to raster properties, e.g.:
                ras_meta['dtype'] = "float32" # Since all values are nodata values, we can just use a float 32 value to store them - higher the precision- more storage the tif file would consume
                ras_meta['nodata'] = np.float32(nodata_val)
                ras_meta['count'] = 1 # Specify only one band in output tif file
                ras_meta['options']=['COMPRESS=DEFLATE']
                # inDs = gdal.Open("input_"+ str(output_count) +"_part.tif")
                ras_meta['crs'] = main_raster_obj.GetProjection() # Get CRS on main rgb raster file and save it as the meta data crs of the output raster file
                
                print(ras_meta)

                if(output_count<10): # Store the file in format 001,002 upto 400 - since we divide the main raster into 20x20 filter, there will be 400 parts
                    with rio.open("tif_processing/output_00"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 001 to 010
                elif(output_count>10 and output_count<100):
                    with rio.open("tif_processing/output_0"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 010 to 099
                else:
                    with rio.open("tif_processing/output_"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 100 to 400

                dst = None

                src2= None
                main_raster_obj = None
                
                import os
                os.remove("tif_processing/input_"+ str(output_count)+"_part.tif") # Remove the input split file of this iteration

                #if you get errors with proj.db or with crs - then use this:
                # inDs = gdal.Open(raster_filename)
                # ras_meta['crs'] = inDs.GetProjection()
                del mdf
                gc.collect()               
    
    src = None
    dem = None
    del rastfile_warp
    gc.collect()

    # Merge all tif into a single output file
    import glob
    demList = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Use glob to get all files name with matching names

    print(demList)

    vrt = gdal.BuildVRT("merged.vrt",demList) # Build a VRT and pass in the list of all output tif file names to be merged into one single tif file
    gdal.Translate("tif_processing/"+ preds_file_name, vrt,  xRes= res_all, yRes=-res_all, creationOptions=["COMPRESS=DEFLATE"] ) # It will merge all files into one file and compress the size using deflate algorithm
    vrt = None               

    # Remove all output tifs if present
    import glob
    demList1 = glob.glob("tif_processing/output_[0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file
    demList2 = glob.glob("tif_processing/output_[0-9][0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file
    demList3 = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file

    demList=[] # Initialize empty list
    for i in demList1: # For all files in this list of separate output tif files - this is for 0-10 output numbered files
        demList.append(i) # Append to the new list
    for i in demList2: # For all files in this list of separate output tif files - this is for 10-100 output numbered files
        demList.append(i) # Append to the new list
    for i in demList3: # For all files in this list of separate output tif files - this is for 100-400 output numbered files
        demList.append(i) # Append to the new list

    for i in demList: # For all files in the fully appened list of output tif files
        os.remove(i) # Remove them

    return jsonify( { 'data':jsonfiles_full_river_preds, 'output_file_name':preds_file_name } )

# This is non-functional          
@app.route('/save_model_files_tif_cf', methods=['POST','GET']) # This is non-functional
def save_model_files_tif_cf(): # This is non-functional
    model_loc_obj= request.form['model_save_loc']
    model_loc=json.loads(model_loc_obj)

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    algo_choice_obj= request.form['ml_algo_name']
    algo_choice=json.loads(algo_choice_obj)
    print(algo_choice)

    if os.path.exists(model_loc):
        shutil.rmtree(model_loc)
        
    # shutil.copytree('model_files',model_loc)

    file_path=file_path+model_loc
    shutil.copytree('model_files',file_path)
    
    return jsonify("Done")


@app.route('/save_files_tif_cf', methods=['POST','GET'])
def save_files_tif_cf(): # Save the output tif file to the user's selected location

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj) # Get the file path - of the user's selected location
    print(file_path)
    
    file_name_obj= request.form['file_name']
    file_name=json.loads(file_name_obj) # Get the current output tif file name

    shutil.copy('tif_processing/'+ file_name,file_path) # Copy the current generated output tif file to the user's selected location
    #os.remove( 'model_files/'+ file_name )

    return jsonify("Done")






































































##### FOR MOD 2,3
@app.route('/save_statistics', methods=['POST','GET'])
def save_statistics(): # Save the statistics file to the user's location

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj) # Get the file path - of the user's selected location
    print(file_path)

    vali_mode_obj= request.form['vali_mode']
    vali_mode=json.loads(vali_mode_obj) # Get the validation mode
    
    if( vali_mode=="no_vali" ): # If the validation mode is 'no validation mode' - then save only training statistics (there is no validation statistic)
        shutil.copy('model_files/training_statistics.txt',file_path)

    else: # For any other modes, save both validation and training statistic files
        shutil.copy('model_files/training_statistics.txt',file_path)
        shutil.copy('model_files/validation_statistics.txt',file_path)
    
    return jsonify("Done")

@app.route('/save_statistics_cf', methods=['POST','GET'])
def save_statistics_cf():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj) # Get the file path - of the user's selected location
    print(file_path)

    vali_mode_obj= request.form['vali_mode']
    vali_mode=json.loads(vali_mode_obj)  # Get the validation mode
    
    if( vali_mode=="no_vali" ):  # If the validation mode is 'no validation mode' - then save only training statistics (there is no validation statistic)
        shutil.copy('model_files/curve_fitting_train_statistics.txt',file_path)
    
    else: # For any other modes, save both validation and training statistic files
        shutil.copy('model_files/curve_fitting_train_statistics.txt',file_path)
        shutil.copy('model_files/curve_fitting_validation_statistics.txt',file_path)
    
    return jsonify("Done")

#### Save training and validation csv files for ml module

@app.route('/save_train_validation_csv', methods=['POST','GET'])
def save_train_validation_csv():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj) # Get the file path - of the user's selected location
    print(file_path)
    
    vali_mode_obj= request.form['vali_mode']
    vali_mode=json.loads(vali_mode_obj) # Get the validation mode

    ml_algo_name_obj= request.form['ml_algo_name']
    ml_algo_name=json.loads(ml_algo_name_obj) # Get the selected ml algorithm

    if( vali_mode=="random_vali" or vali_mode=="auto_select_vali" ): # This function saves training and validation prediction for random vali mode or auto select grouping mode. No need to save files for other two modes
        if(ml_algo_name == "Try all Techniques"): # If all techiniques option was selected
            
            # Training file
            preds = pd.read_csv('model_files/all_models_preds.csv') # Read this file which was stored during the model training phase - this file contains all predictions of 5 models
            data = pd.read_csv('model_files/Training-data_file.csv') # Read this file which was stored during the model training phase - this file just contains all training data - no training data predictions
            t1 = pd.read_csv('model_files/Technique1_Training_preds.csv') # Read all separately stored training predictions for each model
            t2 = pd.read_csv('model_files/Technique2_Training_preds.csv') # Read all separately stored training predictions for each model
            t3 = pd.read_csv('model_files/Technique3_Training_preds.csv') # Read all separately stored training predictions for each model
            t4 = pd.read_csv('model_files/Technique4_Training_preds.csv') # Read all separately stored training predictions for each model
            t5 = pd.read_csv('model_files/Technique5_Training_preds.csv') # Read all separately stored training predictions for each model
            
            data['Technique1_train_Pred'] = t1['Technique1_train_Pred'].copy() # Copy the correct technique's training predictions to the training data file - additional columns
            data['Technique2_train_Pred'] = t2['Technique2_train_Pred'].copy() # Copy the correct technique's training predictions to the training data file - additional columns
            data['Technique3_train_Pred'] = t3['Technique3_train_Pred'].copy() # Copy the correct technique's training predictions to the training data file - additional columns
            data['Technique4_train_Pred'] = t4['Technique4_train_Pred'].copy() # Copy the correct technique's training predictions to the training data file - additional columns
            data['Technique5_train_Pred'] = t5['Technique5_train_Pred'].copy() # Copy the correct technique's training predictions to the training data file - additional columns

            data.to_csv(file_path + 'Training-data_file_all.csv') # Save the final training file to the user's selected location


            # Validation file
            preds = pd.read_csv('model_files/all_models_vali_pred.csv') # Read this file which was stored during the model training phase - this file contains all validation data predictions of 5 models
            data = pd.read_csv('model_files/Validation-data_file.csv') # Read this file which was stored during the model training phase - this file just contains all validation data - no validation data predictions
            data['Technique1_vali_pred'] = preds['Technique1_vali_pred'].copy() # Copy the correct technique's validation predictions to the validation data file - additional columns
            data['Technique2_vali_pred'] = preds['Technique2_vali_pred'].copy() # Copy the correct technique's validation predictions to the validation data file - additional columns
            data['Technique3_vali_pred'] = preds['Technique3_vali_pred'].copy() # Copy the correct technique's validation predictions to the validation data file - additional columns
            data['Technique4_vali_pred'] = preds['Technique4_vali_pred'].copy() # Copy the correct technique's validation predictions to the validation data file - additional columns
            data['Technique5_vali_pred'] = preds['Technique5_vali_pred'].copy() # Copy the correct technique's validation predictions to the validation data file - additional columns
            data.to_csv(file_path + 'Validation-data_file_all.csv') # Save the final validation file to the user's selected location

            # shutil.copy('model_files/Training-data_file.csv',file_path)
            # shutil.copy('model_files/Validation-data_file.csv',file_path)
        
        else:      # If a single model is selected
            preds = pd.read_csv('model_files/' + ml_algo_name + '_pred_temp.csv') # Find and read the file with the selected algorithm's name - this file contains the predictions of full input data
            data = pd.read_csv('model_files/Training-data_file.csv') # Read this file which was stored during the model training phase - this file just contains all training data - no training data predictions
            t1 = pd.read_csv('model_files/' + ml_algo_name + '_Training_preds.csv') # Find and read the file with the selected algorithm's name - this file contains the predictions of just the training data
            
            data[ ml_algo_name + '_train_Pred'] = t1[ ml_algo_name + '_train_Pred'].copy() # Copy the correct technique's training predictions to the training data file - additional columns
            data.to_csv(file_path + 'Training-data_file_all.csv') # Save the final training file to the user's selected location

            preds = pd.read_csv('model_files/' + ml_algo_name + '_vali_pred.csv') # Copy the correct technique's validation predictions to the validation data file - additional columns
            data = pd.read_csv('model_files/Validation-data_file.csv') # Save the final validation file to the user's selected location
            data[ ml_algo_name + '_vali_pred' ] = preds[ ml_algo_name + '_vali_pred' ].copy() # Copy the correct technique's validation predictions to the validation data file - additional columns
            data.to_csv(file_path + 'Validation-data_file_all.csv') # Save the final validation file to the user's selected location

    return jsonify("Done")

#### Save training and validation csv files for curve-fit module

@app.route('/save_train_validation_csv_cf', methods=['POST','GET'])
def save_train_validation_csv_cf():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj) # Get the file path - of the user's selected location
    print(file_path)
    
    vali_mode_obj= request.form['vali_mode']
    vali_mode=json.loads(vali_mode_obj) # Get the validation mode

    if( vali_mode=="random_vali" or vali_mode=="auto_select_vali" ): # This function saves training and validation prediction for random vali mode or auto select grouping mode. No need to save files for other two modes
        preds = pd.read_csv('model_files/Train_predictions.csv') # Read the stored training predictions of all curves
        data = pd.read_csv('model_files/Curve-fit_Training-data_file.csv') # Read this file which was stored during the curve fitting phase - this file just contains all training data - no training data predictions
        data['Curve_1'] = preds['Curve_1'].copy() # Copy the correct curve-fitted equation's training predictions to the training data file - additional columns
        data['Curve_2'] = preds['Curve_2'].copy() # Copy the correct curve-fitted equation's training predictions to the training data file - additional columns
        data['Curve_3'] = preds['Curve_3'].copy() # Copy the correct curve-fitted equation's training predictions to the training data file - additional columns
        data['Curve_4'] = preds['Curve_4'].copy() # Copy the correct curve-fitted equation's training predictions to the training data file - additional columns
        data['Curve_5'] = preds['Curve_5'].copy() # Copy the correct curve-fitted equation's training predictions to the training data file - additional columns

        data.to_csv(file_path + 'Curve-fit_Training-data_file_all.csv') # Save the final training file to the user's selected location


        # Validation file
        preds = pd.read_csv('model_files/all_models_vali_pred.csv') # Read this file which was stored during the curve fitting phase - this file contains all validation data predictions of 5 models
        data = pd.read_csv('model_files/Curve-fit_Validation-data_file.csv') # Read this file which was stored during the curve fitting phase - this file just contains all validation data - no validation data predictions
        data['Curve_1'] = preds['Curve_1'].copy() # Copy the correct curve-fitted equation's validation predictions to the validation data file - additional columns
        data['Curve_2'] = preds['Curve_2'].copy() # Copy the correct curve-fitted equation's validation predictions to the validation data file - additional columns
        data['Curve_3'] = preds['Curve_3'].copy() # Copy the correct curve-fitted equation's validation predictions to the validation data file - additional columns
        data['Curve_4'] = preds['Curve_4'].copy() # Copy the correct curve-fitted equation's validation predictions to the validation data file - additional columns
        data['Curve_5'] = preds['Curve_5'].copy() # Copy the correct curve-fitted equation's validation predictions to the validation data file - additional columns
        data.to_csv(file_path + 'Curve-fit_Validation-data_file_all.csv') # Save the final validation file to the user's selected location
    
    return jsonify("Done")

## Splitinfo save

@app.route('/save_splitinfo', methods=['POST','GET'])
def save_splitinfo():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    
    vali_mode_obj= request.form['vali_mode']
    vali_mode=json.loads(vali_mode_obj)

    if( vali_mode=="auto_select_vali" ):
        shutil.copy('model_files/SplitInfo_file.pkl',file_path)
    
    return jsonify("Done")

@app.route('/save_splitinfo_cf', methods=['POST','GET'])
def save_splitinfo_cf():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    
    vali_mode_obj= request.form['vali_mode']
    vali_mode=json.loads(vali_mode_obj)

    if( vali_mode=="auto_select_vali" ):
        shutil.copy('model_files/Curve-fit_SplitInfo_file.pkl',file_path)
    
    return jsonify("Done")

############################### CURVE FIT END


#### SECTION 9 : FUNCTIONS FOR TRAINING MACHINE LEARNING ALGORITHMS AND APPLYING FOUR DIFFERENT VALIDATION STRATEGIES 
 

@app.route('/indep_data',methods=['GET','POST'])
def independent_feats(): # Get the dataset, separate the independent and dependent feature variables and send back a sample of the data
    data=load_data('temp_pipeline_2.csv') # contains only selected columns data + distance
    feats=request.form['sel_feat'] # Request to get the selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list

    print(sel_cols1)
    r=sel_cols1[3] #3 because of east north corrd input. Get R column name  
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    x=data[sel_cols1[3:]].copy()   # Get the subset of main dataset - with feature variables starting from third index ( R feature ) till the last variable - which is the depth feature variable

    # check if 0 value is present in r,g,b
    if(x.describe()[r]['min']==0):    # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator
        x[r]=np.where(x[r]==0,0.1,x[r])
    if(x.describe()[g]['min']==0):    # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                
        x[g]=np.where(x[g]==0,0.1,x[g])
    if(x.describe()[b]['min']==0):    # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator               
        x[b]=np.where(x[b]==0,0.1,x[b])
    
    if(x.describe()[r]['min']<0):     # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                    
        x[r]=np.where(x[r]<0,0.1,x[r])
    if(x.describe()[g]['min']<0):     # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                    
        x[g]=np.where(x[g]<0,0.1,x[g])
    if(x.describe()[b]['min']<0):     # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                   
        x[b]=np.where(x[b]<0,0.1,x[b])
                                                  
    print('You selected:', sel_cols1[3:-1],' as the independent features')
    print('You selected:', sel_cols1[-1],' as the dependent feature')
    
    # check if last row has Nan values
    if(True in list(x.iloc[-1].isnull()[:])): # Sometimes the last row of a dataframe has null values - happened only once, no idea how it came there, but somehow a null row gets added. So in case any such thing exists, get the last row index and remove it.
        drop_ind=x.index[-1] # Get the last row's index value
        x.drop(drop_ind,axis=0,inplace=True) # Drop the last row by it's index value

    x.to_csv('model_files/'+'x_y.csv',index=False) # Save the original files with nan values and min=0 or min<0 values changed to 0.1 - save this file as x_y.csv to access later
    x.drop(sel_cols1[-1],axis=1,inplace=True) # Now drop the last column ( depth variable )
    x=x.round(3)  # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values in the stored csv file
    jsonfiles_x = json.loads(x.head().to_json(orient='records'))  # Save the first 5 rows of the input data with just selected independent features, as json object to show to the user using javascript  
    return (jsonify(jsonfiles_x))

@app.route('/dep_data',methods=['GET','POST'])
def dependent_feats(): # Get the dataset, separate the independent and dependent feature variables and send back a sample of the data
    x=load_data('x_y.csv') # Read this file we stored in the independent_feats function, contains only selected columns' data
    feats=request.form['sel_feat'] # Request to get the selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list
    y=pd.DataFrame(columns={sel_cols1[-1]}) # Create a new dataframe with column name same as input data's depth column name - by extracting the last index's value in the sel_cols1 list, we can get the depth column name(dependent variable)
    y[sel_cols1[-1]]=x[sel_cols1[-1]].values  # Now extract the data values of the last column (depth variable) and copy the values to the newly created dataframe
    print(y.head())
    y=y.round(3)  # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values in the stored csv file
    jsonfiles_y = json.loads(y.head().to_json(orient='records')) # Save the first 5 rows of the input data with just selected dependent feature, as json object to show to the user using javascript  
    return (jsonify(jsonfiles_y))

def best_processing_pool(all_combinations, processed_data, r, g, b, y, colourspaces): # This function determines the best combination or best set of feature engineering methods suitable to the input data - this is done by processing the input combinations of feature enigneering methods and applying a simple xgboost algorithm to compute the rmse scores for each method, and after returning to the parent function where we finally determine the best combination having the lowest rmse score 
    processed_data_new=processed_data.copy() # Make a copy of the processed data
    for process_options_trial in all_combinations: # Now, for each option from the all_combinations list - this might be just a single set of combinations ( ex: ['option1','option 2'] ) or multiple sets of combinations ( ex: [ ['option1','option 2'],  ['option 3','option 4'] ] ), we do the below loop
        processed_data_new=processing_options(processed_data_new,process_options_trial,r,g,b,y, colourspaces) # For each loop of feature combination, pass the input data, particular loop's feature combination set, r,g,b, column names, the depth data values (traget variable data values) and the selected colourspace option
    x_train,x_test,y_train,y_test=tts(processed_data_new,y,0.25) # Use train test split to split the input data into training and validation data - constant value of 0.25 split value (25% validation) - The reason we split here is to avoid any data leaks ( read more on this concept ) - we should not consider properties of validation set, else it would mean that we have chosen feature engineering methods based on rmse scores of models which have already seen the validation data
    xgb_trial=XGBRegressor( n_estimators=500, early_stopping_rounds=10 ,random_state=10 ) # Initialize the xgboost regressor object with constant parameters
    xgb_trial.fit(x_train,y_train) # Fit the algorithm to the training data
    
    train_preds=pd.DataFrame(y_train)     # Save the training target (depth) actual values to a newly created dataframe
    y_pred_model_train=xgb_trial.predict(x_train) # Compute the predictions of model on the training data
    col1=train_preds.columns.tolist()[0] # Get all column names and store as a list
    train_preds['Model_Pred']=y_pred_model_train # Add new column to the previously created dataframe and copy the training data's prediction values to that column
    rmse_train=np.sqrt(mse(train_preds[col1],train_preds['Model_Pred'])) # Compute the rmse score of the training predictions
    
    test_preds=pd.DataFrame(y_test)     # Save the validation target (depth) actual values to a newly created dataframe
    y_pred_model_test=xgb_trial.predict(x_test) # Save the validation prediction values
    col1=test_preds.columns.tolist()[0] # Get all column names and store as a list
    test_preds['Model_Pred']=y_pred_model_test # Add new column to the previously created dataframe and copy the validation data's prediction values to that column
    rmse_test=np.sqrt(mse(test_preds[col1],test_preds['Model_Pred'])) # Compute the rmse score of the validation predictions

    # Calculating a weighted average with 60% to validation rmse and 40% to train rmse  - more importance given to validation scores and so to pick out feature methods that perform better on unseen validation data
    weighted_rmse=0.6*rmse_test + 0.4*rmse_train
#    trial_scores=100
    print(all_combinations,weighted_rmse)
    #i'll create a dictionary with key as the combination subset and value as the trial score or the rmse score
    
    #q.put(trial_scores)
    # return trial_scores
    return all_combinations,round(weighted_rmse,5), processed_data_new

def child_initialize(_all_combinations,_processed_data, _r, _g, _b, _y): ## NON-FUNCTIONAL
     global all_combinations,processed_data, r, g, b, y
     
     all_combinations=_all_combinations
     processed_data = _processed_data
     r = _r
     g=_g
     b=_b
     y=_y


full_result={}
full_combos={}

def accumulateResults(result): ## NON-FUNCTIONAL
    full_result[str(result[0])]=result[1]
    full_combos[str(result[0])]=result[0]


@app.route('/process_train_data',methods=['GET','POST'])
def process_training_data(): ## This function processes training data , chooses the best set of feature engineering methods, applies the best methods on the input data to store the final processed csv for later use. # NOTE: the order of the if blocks are V.IMP
    process_options_obj=request.form['process_options'] # Request to get the processing options ( updated - this is a constant value and is set to best_options in the javascript side ) as json object
    process_options=json.loads(process_options_obj) # Convert the json object to list
    feats=request.form['sel_feat'] # Request to get the selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list
    original_feats_train=[] # Initialize empty list to save the original features used for training - access later in full-data part
    print("cacl")
    for i in range(len(sel_cols1)): # For all values in the sel_cols1
        original_feats_train.append([str(sel_cols1[i])]) # Append every column name to the list
    original_feats_train_df=pd.DataFrame(original_feats_train,columns=['feats'])    # Create a dataframe with column name 'feats' and store the original_feats_train list values      
    original_feats_train_df.to_csv('model_files/'+'original_feats_train.csv',index=False) # Save the dataframe as csv selected to use later
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    data=load_data('x_y.csv') # Load this csv which was stored after input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    x=data[sel_cols1[3:-1]].copy() # Get the processed data without target values - (last column name in the list is the target column name)
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe
    processed_data=x.copy() # Make a copy of the data - has just the selected independent features
    print(processed_data.dtypes)
    all_default=0
    all_best=0
    colourspaces_json=request.form['colourspaces'] # Get what colourspaces have been chosen - rgb or hsv or both - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value

    if("best_options" in process_options): # Like said in the NOTES section ( at the very beginning of the codes - right after all imports), it was mentioned that the updated version of the software runs only best_options part - automatically determines what feature engineering processes would suit best on the dataset with lowest rmse metric scores
        discard_first=0
        # trial_scores=[]
        all_combinations=[] # Create empty list to store all combination of feature engineering methods
        original_options=["logperm_gen", "greyscale_gen", "cluster_gen", "poly_corr_gen", "ss_scale", "minmax_scale", "robust_scale", "pearson_correl", "pca_trans"] # These are the feature engineering methods available in this software
        for L in range(1, len(original_options)+1): # For all the options in the above list
            for subset in itertools.combinations(original_options,L): # Use itertools to compute all possible combinations for the given set of options
                process_options_trial=list(subset) # Convert the itertools object to a list
                all_combinations.append(process_options_trial) # Append this value to the all_combinations list 
        #print(all_combinations, len(all_combinations))  
        import time
        starttime=time.time() # This is just to keep track of the time the whole function takes to determine the best combination values
        processes=[]
        i=0
        results={}
        import gc
        gc.collect() 
        #child_initialize(all_combinations,processed_data,r,g,b,y)   
        #1-8 sec
        #2-13sec
        #3-13 sec
        #4- 14 sec
        num_of_cores=os.cpu_count()
        
        ## For the next lines of code till the end of this function, what the software is trying to achieve is to find the best combination of 
        ## feature engineering methods and also not taking too much computational time. We can reduce the computational time by memoization -
        ## it's a concept of not re-doing stuffs that are already done. So in our case, say the combination ['logperm_gen','ss_scale'] was already computed,
        ## so for another combination say ['logperm_gen','ss_scale','minmax_scale'], we do not have to do computation for the first two functions.
        ## i.e. ['logperm_gen','ss_scale'] - since it was already done, and we have stored the processed data values that comes out of proceesing 
        ## by these two functions. So we can just extract those data values and compute only for 'minmax_scale' which was not previously computed

        ## TERMS FOR UNDERSTANDING THIS CODE : 'feature combos/combinations' looks like this -  ['logperm_gen','ss_scale','minmax_scale']. 'Each processing option present in the combo' looks like this : for the combo  ['logperm_gen','ss_scale','minmax_scale'], there are 3 processing options - 1. 'logperm_gen', 2. 'ss_scale', 3.'minmax_scale'

        memoized_combos={} # Initialize an empty dict to keep track of all the computed feature combos (combinations) and the data values resulting from processing each combo value  - this is to save time and not re-do all combinations or processing options which were already done earlier
        for combo in all_combinations: # loop over all the feature combos
            print(combo)
            combo_temp2=combo.copy() # Make a copy of the current combo
            combo_temp=combo.copy() # Make another copy of the current combo
            ind=0 # Initialize to 0
            len_combo=len(combo_temp) # Get the number of processing methods present in this particular combo
            if(len(combo_temp)>1): # If the combo has more than one processing methods - a combination of multiple processing methods
                #print(" more than one len if block ")
                for i in range(0,len(combo_temp)): # For each processing option present in the combo, do this whole loop. We use the 'combo_temp' value
                    #print(" inside for loop ")
                    str_combo=str(combo) # Get the processing option list and convert to a string value - we use the 'combo' value- this gets updated at every loop based on the processing options' values
                    if( str_combo in list(memoized_combos.keys()) ): # Check if the combo string is already computed - this is done by checking if there are any keys in the dict that match with the combo string value
                        print(" keys match memo : ", str_combo)
                        upd_processed_data= memoized_combos[str_combo] # If there is a match, then get the data values that was previously processed with this combo value
                        rem_combo=[] # Initialize an empty list to keep track of remanining processing methods in the combo - you will understand later on why this line is here
                        for j in range(0,ind): # For all values from 0 to ind
                            rem_combo.append(combo_temp.pop()) # Pop the last element for every loop of 0 to ind - these will give the unprocessed processing options ( remaining methods ) and append to the remaining combo list
                        result0, result1, result2= best_processing_pool(rem_combo ,upd_processed_data, r, g, b, y, colourspaces) # Process the updated data values on the remaining processing methods in that combo value - those which were not done earlier
                        memoized_combos[str(combo_temp2)]=result2   # Store the updated dataset in the memoized combo dict - for this particular combo value
                        full_result[str(combo_temp2)]=result1 # Store the weighted r2 score in the memoized combo dict - for this particular combo value
                        full_combos[str(combo_temp2)]=combo_temp2
                        break
                    elif( ( str_combo not in list(memoized_combos.keys()) ) & ( ind<len_combo ) ): # If the combo string value is not computed already and the ind value is less than the total number of processing options in the combo value - the ind value keeps track of the number of processing options in every combo list that are new ( not previously computed), the reason we check if ind < length of the combo list is to see if there is still atlease one already computed combination present in this combo list ( at least one processing method or combination of processing methods in this combo value, that are already done )
                        #print(" pop here ")
                        combo.pop() # Pop the last element or the last processing option, from the combo value
                        ind+=1 # Increment by 1
                    if( ( str_combo not in list(memoized_combos.keys()) ) & ( ind==len_combo ) ): # If the combo is not computed already and the ind value is equal to the total number of processing options in the combo value - the ind value keeps track of the number of processing options in every combo list that are new ( not previously computed) - if this ind value is equal to len_combo (length of the combo list) - it means after popping each element in the combo list, the ind was incremented, and at the end after popping each element in the combo list - there is no other processing options present in the combo - meaning none of the processing options in this combo list were already computed - so we have to compute all the processing options present in this combo list
                        #print(" no pop all direct ")
                        result0,result1,result2 =best_processing_pool(combo_temp2 ,processed_data, r, g, b, y, colourspaces) # Process the data on the all the processing methods ( the whole combo list ( since none of the subsets of the combo was previously computed ) ) and save the updated dataset to the dict
                        memoized_combos[str(combo_temp2)]=result2 # Store the updated dataset in the memoized combo dict - for this particular combo value
                        full_result[str(combo_temp2)]=result1 # Store the weighted r2 score in the memoized combo dict - for this particular combo value
                        full_combos[str(combo_temp2)]=combo_temp2  
                        break 
            else: # If there is only one processing option in this particular combo value, then directly process the data on this one processing option and store the info to the memoized combo dict
                result0,result1,result2=best_processing_pool(combo_temp2 ,processed_data, r, g, b, y, colourspaces) # Process the data on the new processing methods and save the updated dataset to the dict
                memoized_combos[str(combo_temp2)]=result2 # Store the updated dataset in the memoized combo dict - for this particular combo value
                full_result[str(combo_temp2)]=result1 # Store the weighted r2 score in the memoized combo dict - for this particular combo value
                full_combos[str(combo_temp2)]=combo_temp2

        print(len(full_result), len(full_combos))
        print(len(all_combinations))
        print('That took {} seconds'.format(time.time() - starttime))
        print(full_result)
        print(" Used cores : ",num_of_cores)
        lowest_rmse_id=min(full_result.items(), key=lambda x: x[1])[0] # Get the index of the combo value with the lowest rmse score
        print(lowest_rmse_id)
        del memoized_combos
        print(" The best combination of options is {} , with a rmse score of {}".format(full_combos[lowest_rmse_id], full_result[lowest_rmse_id] ))
        best_process_option=full_combos[lowest_rmse_id] # Get the best processing options by extracting that combo value from the list using the index value of the lowest rmse score combo 
        print(best_process_option)
        option_csv=[] # Initialize an empty list
        for i in range(len(best_process_option)): # For every value in the best processing options list
            option_csv.append([str(best_process_option[i])]) # append the value to the new list
        option_csv_df=pd.DataFrame(option_csv,columns=['best_option'])    # Create a new dataframe with a column name 'best_option' and add the option_csv list as the only value to this dataframe 
        option_csv_df.to_csv('model_files/'+'option_csv.csv',index=False) # Save this dataframe as csv for later use
        processed_data=x.copy() # Make a copy of the original/unchanged input data with just the independent feature variables
        final_processed_data=processing_options(processed_data,best_process_option,r,g,b,y, colourspaces) # Apply only the best processing option to get the final updated (processed) dataset
        final_processed_data.to_csv('model_files/'+'data_for_all_models.csv',index=False) # Save this final dataset as csv to use later
        final_processed_data=pd.read_csv('model_files/'+'data_for_all_models.csv')
        final_processed_data=final_processed_data.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values in the stored csv file
        jsonfiles_final_processed_data = json.loads(final_processed_data.head().to_json(orient='records')) # Get json object for the first 5 rows of the fully processed dataset - in new update (current version), we do not show this processed data in the webpage
        return (jsonify(jsonfiles_final_processed_data))

    if("default_options_all" in process_options): #### As mentioned in the NOTES part at the very beginning of the codes - we use just the best_process options, so codes after this line is just NON-FUNCTIONAL upto the end of this function
        #lr data
        option_csv=pd.DataFrame()
        option_csv['best_option']="default_options_all"
        option_csv.to_csv('model_files/'+'option_csv.csv',index=False)
        lr_processed_data=logperm_gen(processed_data,r,g,b, colourspaces)
        lr_processed_data=greyscale_gen(lr_processed_data,r,g,b)
        lr_processed_data=cluster_data(lr_processed_data,r,g,b)
        lr_processed_data=standard_scale_cluster(lr_processed_data,y,"lr")
        lr_processed_data=pca_reduction_cluster(lr_processed_data,y,"lr")
        lr_x=lr_processed_data.copy()
        lr_x.to_csv('model_files/'+'lr_svr_x.csv',index=False)
        print("LR HEAD",lr_x.head())
        #svr data
        svr_x=lr_processed_data.copy()
        print("SVR HEAD",svr_x.head())

        #rf data
        processed_data=x.copy()
        rf_processed_data=logperm_gen(processed_data,r,g,b, colourspaces)
        rf_processed_data=greyscale_gen(rf_processed_data,r,g,b)
        rf_processed_data=cluster_data(rf_processed_data,r,g,b)
        rf_processed_data=poly_creation_cluster(rf_processed_data,y)
        rf_processed_data=standard_scale_cluster(rf_processed_data,y,"rf")
        rf_processed_data=correl_cluster(rf_processed_data,y,"rf")
        rf_x=rf_processed_data.copy()
        rf_x.to_csv('model_files/'+'rf_xgb_x.csv',index=False)
        print("RF HEAD",rf_x.head())
        #xgb data
        xgb_x=rf_processed_data.copy()
        print("XGB HEAD",xgb_x.head())
        
        #nn data
        processed_data=x.copy()
        nn_processed_data=logperm_gen(processed_data,r,g,b, colourspaces)
        nn_processed_data=greyscale_gen(nn_processed_data,r,g,b)
        nn_processed_data=cluster_data(nn_processed_data,r,g,b)
        nn_processed_data=poly_creation_cluster(nn_processed_data,y)
        nn_processed_data=standard_scale_cluster(nn_processed_data,y,"nn")
        nn_processed_data=correl_cluster(nn_processed_data,y,"nn")

        nn_x=nn_processed_data.copy()
        nn_x.to_csv('model_files/'+'nn_x.csv',index=False)
        print("NN HEAD",nn_x.head())

        all_default=1
        process_options=[]

    if(all_default==1):
        print("All default",xgb_x.head())
        jsonfiles_processed_data = json.loads(xgb_x.head().to_json(orient='records'))
        return (jsonify(jsonfiles_processed_data))

    if("logperm_gen" in process_options):
        processed_data=logperm_gen(processed_data,r,g,b, colourspaces)

    if("greyscale_gen" in process_options):
        processed_data=greyscale_gen(processed_data,r,g,b)

    if("cluster_gen" in process_options):
        processed_data=cluster_data(processed_data,r,g,b)

    if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)):
        processed_data=poly_creation_cluster(processed_data,y)

    if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=poly_creation_no_cluster(processed_data,y)

    #SCALING PART

    if(("ss_scale" in process_options) & ("cluster_gen" in process_options)):
        processed_data=standard_scale_cluster(processed_data,y,"common")

    if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=standard_scale_no_cluster(processed_data,y,"common")

    if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)):
        processed_data=min_max_scale_cluster(processed_data,y,"common")

    if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=min_max_scale_no_cluster(processed_data,y,"common")

    if(("robust_scale" in process_options) & ("cluster_gen" in process_options)):
        processed_data=robust_scale_cluster(processed_data,y,"common")

    if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=robust_scale_no_cluster(processed_data,y,"common")

    if(("power_scale" in process_options) & ("cluster_gen" in process_options)):
        processed_data=power_scale_cluster(processed_data,y,"common")

    if(("power_scale" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=power_scale_no_cluster(processed_data,y,"common")

    #FEAT REDUCTION PART
    if(("pearson_correl" in process_options) & ("cluster_gen" in process_options)):
        processed_data=correl_cluster(processed_data,y,"common")

    if(("pearson_correl" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=correl_no_cluster(processed_data,y,"common")

    if("poly_lasso_trans" in process_options):
        processed_data=lasso_reg(processed_data,y)

    if(("pca_trans" in process_options) & ("cluster_gen" in process_options) & (processed_data.shape[1]>=3) ):
        processed_data=pca_reduction_cluster(processed_data,y,"common")    

    if(("pca_trans" in process_options) & ("cluster_gen" not in process_options) & (processed_data.shape[1]>=3) ):
        processed_data=pca_reduction_no_cluster(processed_data,y,"common")

    if(("pls_trans" in process_options) & ("cluster_gen" in process_options)):
        processed_data=pls_reduction_cluster(processed_data,y)

    if(("pls_trans" in process_options) & ("cluster_gen" not in process_options)):
        processed_data=pls_reduction_no_cluster(processed_data,y)

    else:           
        print(processed_data.head())
        option_csv=[]
        for i in range(len(best_process_option)):
            option_csv.append( [str(best_process_option[i])] )
        option_csv_df=pd.DataFrame(option_csv,columns=['best_option'])    
        option_csv_df.to_csv('model_files/'+'option_csv.csv',index=False)
        processed_data.to_csv('model_files/'+'data_for_all_models.csv',index=False)
        processed_data=pd.read_csv('model_files/'+'data_for_all_models.csv')
        processed_data=processed_data.round(3)
        jsonfiles_processed_data = json.loads(processed_data.head().to_json(orient='records'))
        return (jsonify(jsonfiles_processed_data))

# VALIDATION PARTS
@app.route('/ml_algorithms_random_vali',methods=['GET','POST'])
def ml_algorithms_random_vali(): # Validation strategy - RANDOM INDICES VALIDATION
    process_options_obj=request.form['process_options'] # Request to get the processing options ( updated - this is a constant value and is set to best_options in the javascript side ) as json object
    process_options=json.loads(process_options_obj) # Convert the json object to list
    print(process_options)
    ml_algo_obj=request.form['ml_algo'] # Request to get the selected ml technique as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string
    print(algo_choice)

    tts_value_obj=request.form['tts_value'] # Request to get the user's input split value as json object
    tr_split=json.loads(tts_value_obj) # Convert the json object to string
    print(tr_split)

    feats=request.form['sel_feat'] # Request to get the selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list
    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    test_split=(100-int(tr_split))/100 # Calculate the validation split, 100-the input train split value
    data=load_data('x_y.csv') # Load the previously stored x_y.csv file - contains both dependent and independent data
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe

    # for east and north
    spl_data=load_data('temp_pipeline_2.csv') # This is the original input data - with all columns and variables
    x_train4,x_test4,y_train4,y_test4=tts(spl_data,y,test_split) # We split the non-updated input data (spl_data is the dataset which was read from csv ) - we split and store them as training and validation datasets. We save the spl_data instead of latest updated x_data because the latter consists of data processed values which is not needed while storing csv file for the user

    #Save train and validation csv files - not processed files, but the same indices from original dataset (just with selected columns)
    x_train4.to_csv('model_files/Training-data_file.csv', index=False) # Save as training data file
    x_test4.to_csv('model_files/Validation-data_file.csv', index=False) # Save as validation data file
     

    if("default_options_all" in process_options): # Checks if process_options has default as one of its values - like mentioned before, the process_options list has only 'best_options' as its value
        sub_x=load_data('lr_svr_x.csv')
        x_train2,x_test2,y_train2,y_test2=tts(sub_x,y,test_split)
        # for rf,xgb
        rf_x=load_data('rf_xgb_x.csv')
        totrain=load_data('rf_xgb_x.csv')
        x_train,x_test,y_train,y_test=tts(rf_x,y,test_split)
        # for nn
        nn_x=load_data('nn_x.csv')
        x_train3,x_test3,y_train3,y_test3=tts(nn_x,y,test_split)
        predictions_all=pd.DataFrame(y)
        train_pred=pd.DataFrame(y_train)
        vali_data_y=y_test.copy()
        vali_predictions_all=pd.DataFrame(vali_data_y.copy())
        vali_predictions_all[east]=x_test4[east].copy()
        vali_predictions_all[north]=x_test4[north].copy()
        
    else:
        data=load_data('data_for_all_models.csv') # Load this csv - stored right after the input data was processed and engineered with the best processing options
        sub_x=data.copy() # Create copies of this data for different ml algorithms
        rf_x=data.copy() # Create copies of this data for different ml algorithms
        nn_x=data.copy() # Create copies of this data for different ml algorithms
        x_train2,x_test2,y_train2,y_test2=tts(data,y,test_split)  # Since this is random validation, we split the data into train and validation sets on random indices. X_train2, X_test2 is used for linear regression and svr algorithms
        # for rf,xgb
        x_train,x_test,y_train,y_test=tts(data,y,test_split) # Since this is random validation, we split the data into train and validation sets on random indices. X_train, X_test is used for random forest and xgboost algorithms
        # for nn
        x_train3,x_test3,y_train3,y_test3=tts(data,y,test_split) # Since this is random validation, we split the data into train and validation sets on random indices.  X_train3, X_test3 is used for neural network 
        predictions_all=pd.DataFrame(y) # Save the actual target values of full-input data to the dataframe
        train_pred=pd.DataFrame(y_train)  # Save the actual training target values to the dataframe
        vali_data_y=y_test.copy() # Make a copy of the actual validation target values
        vali_predictions_all=pd.DataFrame(vali_data_y.copy()) # Save the validation target values to a dataframe
        vali_predictions_all[east]=x_test4[east].copy() # Copy the east coordinate values into the validation target values dataframe 
        vali_predictions_all[north]=x_test4[north].copy() # Copy the north coordinate values into the validation target values dataframe
    
    option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
    options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
    
    # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
    no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
    options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
    if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
        no_scale_check_lr=1 # atleast one scaling
    if(no_scale_check_lr==0): # If none present, then do this
        sc=StandardScaler() # Initialize a normal scale object
        x_train2=sc.fit_transform(x_train2) # Fit and transform the object on training data
        x_test2=sc.transform(x_test2) # Just transform the validation data using the scale object that was fit on the training data
        sub_x=sc.transform(sub_x) # Transform the full processed-input data
        joblib.dump(sc,'model_files/'+'sc_forT1&5only.pkl') # Dump this file as pkl to use during the full-data part

    if(algo_choice=="Technique 1"): # If the selected algorithm is technique 1
        lr,predictions_all,fts,valid_results=LR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 1_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 1_traindata_pred.csv',index=False)
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump the model object as a pkl file to access later
        test_preds=lr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique1_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 1_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 2"):  # If the selected algorithm is technique 2
        rf,predictions_all,fts,valid_results=RF_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 2_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 2_traindata_pred.csv',index=False)
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump the model object as a pkl file to access later
        test_preds=rf.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique2_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 2_vali_pred.csv',index=False)  # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 3"): # If the selected algorithm is technique 3
        xgb,predictions_all,fts,valid_results=XGB_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 3_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
       # final_train_preds.to_csv('model_files/'+'Technique 3_traindata_pred.csv',index=False)
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump the model object as a pkl file to access later
        test_preds=xgb.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique3_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 3_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 4"): # If the selected algorithm is technique 4
        #make sure nn recieves scaled data. If scaling not done already , scale it now.
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            x_test3=sc.transform(x_test3) # Just transform the validation data using the scale object that was fit on the training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        nn,predictions_all,fts,valid_results=NN_reg(nn_x,x_train3,x_test3,y_train3,y_test3,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 4_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
       # final_train_preds.to_csv('model_files/'+'Technique 4_traindata_pred.csv',index=False)
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump the model object as a pkl file to access later
        test_preds=nn.predict(x_test3) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique4_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 4_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 5"): # If the selected algorithm is technique 5
        svr,predictions_all,fts,valid_results=SVR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 5_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
       # final_train_preds.to_csv('model_files/'+'Technique 5_traindata_pred.csv',index=False)
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump the model object as a pkl file to access later
        test_preds=svr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique5_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 5_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=='Try all Techniques'): # If all techniques option is selected by the user, then do this
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        
        # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            x_test3=sc.transform(x_test3) # Just transform the validation data using the scale object that was fit on the training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        lr,nn,rf,xgb,svr,predictions_all5,final_scores_test,valid_results,=All_reg(sub_x,nn_x,rf_x,x_train,x_test,y_train,y_test,x_train2,x_test2,y_train2,y_test2,x_train3,x_test3,y_train3,y_test3,predictions_all) # Train all the techniques on training data and return the trained objects, predictions on full-input data, predictions on validation data and the metric scores for each of the five techniques
        print(x_train.shape)
        print(x_test.shape)
        print("Training dataset Evaluation Metrics")
        print(final_scores_test)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all5.head())
        fts=final_scores_test.copy() # Make a copy of the final metric scores of all the models 
        predictions_all_new=predictions_all5.copy() # Make a copy of the dataframe that has predictions of all techinques on the full-input data
        predictions_all_new.to_csv('model_files/'+'all_models_preds.csv',index=False) # Save the model predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'all_models_traindata_pred.csv',index=False)
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump each model object as a pkl file to access later
        test_preds1=lr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique1_vali_pred']=test_preds1 # Save the validation predictions as a dataframe
        test_preds2=rf.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique2_vali_pred']=test_preds2 # Save the validation predictions as a dataframe
        test_preds3=xgb.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique3_vali_pred']=test_preds3 # Save the validation predictions as a dataframe
        test_preds4=nn.predict(x_test3) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique4_vali_pred']=test_preds4 # Save the validation predictions as a dataframe
        test_preds5=svr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique5_vali_pred']=test_preds5 # Save the validation predictions as a dataframe       

        fts=fts.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        valid_results=valid_results.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        predictions_all_new=predictions_all_new.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
        valid_results.to_csv('model_files/'+'validation_statistics.txt',index=False) # Save validation statistics as text file
        vali_predictions_all.to_csv('model_files/'+'all_models_vali_pred.csv',index=False) # Save the model predictions on validation data, as a csv file
        jsonfiles_fts = json.loads(fts.to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
        jsonfiles_valid_results = json.loads(valid_results.to_json(orient='records')) # Get json object for the first 5 rows of the validation metric scores (statistics)
        jsonfiles_predictions_all = json.loads(predictions_all_new.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
        return (jsonify(jsonfiles_fts,jsonfiles_valid_results,jsonfiles_predictions_all)) 

    # Below code runs when the the selected algorithm is not 'all techniques' option

    fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
    valid_results.to_csv('model_files/'+'validation_statistics.txt',index=False) # Save validation statistics as text file
    final_results_train=pd.concat([fts,valid_results],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    final_results_test=pd.concat([valid_results,fts],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    # print("fin res ++++",final_results[valid_results.columns])
    final_results_train=final_results_train.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    final_results_test=final_results_test.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    predictions_all=predictions_all.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values

    jsonfiles_fts = json.loads(final_results_train[fts.columns].to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
    jsonfiles_valid_results = json.loads(final_results_test[valid_results.columns].to_json(orient='records')) # Get json object for the first 5 rows of the validation metric scores (statistics)
    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
    return (jsonify(jsonfiles_fts,jsonfiles_valid_results,jsonfiles_predictions_all))

def Repeat(x): 
    _size = len(x) 
    repeated = [] 
    for i in range(_size): 
        k = i + 1
        for j in range(k, _size): 
            if x[i] == x[j] and x[i] not in repeated: 
                repeated.append(x[i]) 
    return repeated

def get_Groups_Validation(eval_perc, sel_cols1): # This is the main function to split the input data into random groups of validation points
    data=load_data('temp_pipeline_2.csv') # Load the original (unprocessed) input data
    print(" Number of eval points ", int((eval_perc/100)*len(data)) )
    eval_total=int((eval_perc/100)*len(data)) # Calculate the total validation points needed from the input data - total eval points
    max_each_group=50 # We specify here that each group should have a maximum of 50 points
    
    pid=sel_cols1[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    new_sel_cols=sel_cols1.copy() # Make a copy of the user selected features
    new_sel_cols.append('Distance_new') # Add a new value 'Distance_new' to the feature list
    data=data[new_sel_cols].copy() # Make a subset of the data - a new dataframe with just the selected feature columns
    upper_index=len(data) # Upper index is the maximum index of points 
    validation_set=[] # Initialize an empty list
    start=0 # Intialize start variable and make it 0 
    end=upper_index # New variable called end and make it's value equal to the total length (rows) of the input data
    group_id=0 # Intialize a group_id variable and make it 0 - there is no limit to how many groups there can be
    dict={}
    cluster_index=0 # Create a new variable and make it 0 - to divide the whole dataset into 3-4 different groups ( start, mid , end etc )
    to_break=0
    from random import randint
    while(len(validation_set)< eval_total): # While loop to keep track of the total validation points and to not exceed the total eval points there should be
        #print( " new group ", group_id)
        group_id+=1 # Incremenet the group count
        cluster_index+=1 # Increment cluster index - we divide data into multiple clusters to pick out points evenly from all over the river and not just one specifi part
        if(cluster_index==1): # If it's the first cluster
            start=0 # Start point is 0
            end=int(len(data)/3) # And the end point is the end of 1/3rd of the data
        if(cluster_index==2): # If it's the second cluster
            start=int(len(data)/3) # Start point is the last point from 1/3rd of the data
            end=int(len(data)/3) +int(len(data)/3) # And the end point is the end of 2/3rd of the data
        if(cluster_index==3): # If it's the third cluster
            start=int(len(data)/3) +int(len(data)/3) # Start point is the last point from 2/3rd of the data
            end=len(data) # And the end point is the last point of the dataset
            cluster_index=0      # Once last cluster is reached, reset the index value to 0, and for the next group, start again from first cluster to the last cluster and keep repeating
        point_id=randint(start,end-1) # Initiate a random point somewhere between the start and end point ( for every group and every cluster)
        recursion_count=0 # To keep a count of recusion - sometimes the logic can not find any point in a specific cluster, and it remove from a stuck loop, we keep track of the maximum searched points
        
        while( (point_id in validation_set) & (recursion_count < 200) ): # While the point id is in validation set means if a selected random point is already in validation data (must have been taken in a previous loop) - and if its already in the validation set, keep searching for another random point. Here we also make sure the loop does not keep searching forever by having a recursion count to be less than 200
            recursion_count+=1 # For every search, increment the recursion
        # print("in vali")
            point_id=randint(start,end-1) # Choose another random point in that particular cluster - this will be called as current forward point
            if(recursion_count>=200): # If the recursion count exeeded 200 - meaning no new random point is found 
                front_blockage=1 # You will understand later what front_blockage means 
                back_blockage=1 # You will understand later what front_blockage means
                to_break=1 # make the to_break value to 1
                
        if(to_break==1): # Now we check if the to_break is 1, if it is 1, then do the next code
            group_id-=1 # Decrement the group number
            break       # Break the whole loop and start a new group - goes to next while loop iteration -We break the logic because there was no new points left to choose in this cluster or group
        back_id=point_id # If there was no break initiate, then continue and make the current backward point equal to the current random point id ( point_id )
        #print(point_id)
        point_inc=1 # Increment the points counter
        max_count=0 # Maximum number of points per group - resets for every group loop
        to_remove=[] # Keeps track of the points that are removed from the input data for every group - resets to 0 for every group loop
        back_blockage=0 # Make back_blockage = 0 - when it is zero - it means the logic can search backwards from a point (linearly backward search)
        old_point_id=0
        front_blockage=0 # Make front_blockage = 0 - when it is zero - it means the logic can search forward from a point (linearly forward search)
        while((front_blockage == 0) | (back_blockage==0)  ): # When both the sides are open to be searched for taking points for a group
            # Here point_id refers to the current forward point, and the back_id refers to the current backward point. Example, in a list = [ 1,2,3,4,5], say the point_id is 3, at first both current forward point (point_id) and current backward point (back_id) is equal to the point_id which is 3. But as the loop go on, back_id (current backward point) keeps searching backwards from 3 i.e. 2,1. And point_id (current forward point) keeps searching forwards from 3 i.e. 4,5
            back_correct=0 # Initialize back_correct with value 0
            front_correct=0 # Initialize front_correct with value 0
        
            if( (point_id<(len(data)-1)) & (max_count<=max_each_group) & (len(validation_set)<=eval_total) & (point_id not in validation_set) ): # If the current forward point id is not greater than the data's last point's id ( not greater than the total data size), if the maximum count of points for each group is less than 50 (total points for a group), the total validation points as of now is less than the total eval points needed, and when the point id is not already in the validation set 
                if( (distance_cal(data.iloc[point_id][east], data.iloc[point_id][north], data.iloc[point_id+point_inc][east], data.iloc[point_id+point_inc][north])<5) ): # If all conditions match, then calculate the distance between the current forward point id and its new next point id - and if the distance is less than 5m, continue the front search (forward search)
                    
                    front_correct=1 # Keeping it to 1 means continue the forward search of points
            

            if( (back_id>0) & (max_count<=max_each_group) & (len(validation_set)<=eval_total) & (back_id not in validation_set)): # If the current backward point id is greater than the 0 (minimum possible id), if the maximum count of points for each group is less than 50 (total points for a group), the total validation points as of now is less than the total eval points needed, and when the point id is not already in the validation set 
                if( (distance_cal(data.iloc[back_id][east], data.iloc[back_id][north], data.iloc[back_id-point_inc][east], data.iloc[back_id-point_inc][north])<5) ):   # If all conditions match, then calculate the distance between the current backward point id and its new previous(backward) point id - and if the distance is less than 5m, continue the back search (backward search)
                    #to_remove.append(back_id)
                    
                    back_correct=1 # Keeping it to 1 means continue the backward search of points
            
            
            if((back_id<=0) | (max_count>max_each_group) | (len(validation_set)>eval_total) | (back_id in validation_set)): # If the current backward point id is not greater than the 0 (minimum possible id) or if the maximum count of points for each group is more than 50 (total points for a group) or the total validation points as of now is more than the total eval points needed, or and when the point id is already in the validation set 
                back_blockage=1   # Then block the backward search by making back_blockage =1 
                

            if( (back_id>0) ): # If backpoint id is greater than 0
                if( (distance_cal(data.iloc[back_id][east], data.iloc[back_id][north], data.iloc[back_id-point_inc][east], data.iloc[back_id-point_inc][north])>=5) ): # But if the distance between the current backward point and its new previous (backward) point is greater than 5m
                    back_blockage=1   # Then block the backward search by making back_blockage =1 
                

            if((point_id>=(len(data)-1)) | (max_count>max_each_group) | (len(validation_set)>eval_total) | (point_id in validation_set)): # If the current forward point id is greater than the data's last point's id ( greater than the total data size) or if the maximum count of points for each group is greater than 50 (total points for a group) or the total validation points as of now is mroe than the total eval points needed or and when the point id is already in the validation set 
                front_blockage=1 # Then block the forward search by making front_blockage =1 
            
            
            if( point_id<(len(data)-1) ): # If the current forward point id is less than the data's last point's id ( not greater than the total data size)
                if( (distance_cal(data.iloc[point_id][east], data.iloc[point_id][north], data.iloc[point_id+point_inc][east], data.iloc[point_id+point_inc][north])>=5) ): # But if the distance between the current forward point and its new next (forward) point is greater than 5m
                    front_blockage=1 # Then block the forward search by making front_blockage =1 
                
                
            if(front_correct==1): # If all blocks are clear and forward search is allowed for this group and cluster
                validation_set.append(point_id) # Add the current forward point to the validation set
                to_remove.append(point_id) # Add the current forward point to the to_remove set
                old_point_id=point_id # Make old point id as this current forward point id
                point_id=point_id+point_inc # Increment the current forward point id to its next forward point
                max_count+=1 # Increment the max count - number of points a group has - has to be less than 50

            if( back_correct==1 ): # If all blocks are clear and backward search is allowed for this group and cluster
                if(back_id!=old_point_id): # If the current back point is not the same as the current forward point 
                    validation_set.append(back_id)  # Add the current back point to the validation set
                    to_remove.append(back_id) # Add the current back point to the to_remove set
                back_id=back_id-point_inc # Decrement the current back point id - to point to the next previous (backward) point
                max_count+=1 # Increment the max count - number of points a group has - has to be less than 50
        #data.drop(to_remove, axis=0, inplace=True)
    # print(validation_set)
        
        group_name='Group_'+str(group_id) # Name the group by its number
        print(group_name , ' = ' , len(to_remove), ' points ')
        dict[group_name]=to_remove # Add to a dict, for the key value ( the group name ) - dict will have group numbers as keys and point ids from the to_remove list, as values. This list is just the points that are removed from the input data for every group

    lis=[]
    print(Repeat(validation_set))
    new_data=data.iloc[validation_set].copy() # Create a new dataset - it should be a subset of the main input dataset, with point ids from the validation set 
    index_list=list(new_data.index) # Copy the index values of the new dataset to a list
    for i in index_list: # For every value in the index list
        for g in range(1,group_id+1): # For every group that is created
            if(i in dict['Group_'+str(g)]): # Check which group does this point id match with - the dict has group numbers as keys and point ids as values
                lis.append('Group_'+str(g)) # Append the group number to a list

    new_data['Groups']=lis # Copy the group number list to a new column ('Groups') in the dataframe - the groups will correctly match with the index values 
    #print(list(new_data.index))
    train_data=data.drop(new_data.index, axis=0) # Drop the validation indices from the input data - this will become our training data
    #print(list(train_data.index))
   # train_data=train_data.reset_index().drop(['index'],axis=1)
    new_data=new_data.sort_values(by=[pid]) # Sort by id column
    new_data=new_data.reset_index().drop(['index'],axis=1) # Reset the index of the dataframe and remove the index column
    #print(new_data,train_data)
    return new_data,train_data


@app.route('/ml_algorithms_auto_select_vali',methods=['GET','POST'])
def ml_algorithms_auto_select_vali(): # Validation strategy - AUTO-SELECT RANDOM GROUPED VALIDATION

    process_options_obj=request.form['process_options'] # Request to get the processing options ( updated - this is a constant value and is set to best_options in the javascript side ) as json object
    process_options=json.loads(process_options_obj) # Convert the json object to list
    print(process_options)

    colourspaces_json=request.form['colourspaces'] # Request to get the colourspace option as json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to list

    ml_algo_obj=request.form['ml_algo'] # Request to get the selected ml technique as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string
    print(algo_choice)

    tts_value_obj=request.form['tts_value'] # Request to get the user's input split value as json object
    tr_split=json.loads(tts_value_obj) # Convert the json object to string
    print(tr_split)

    feats=request.form['sel_feat'] # Request to get the selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list
    print(sel_cols1)
    # processing the new dataset

    # For identical data
    split_strat_obj=request.form['split_strat'] # Get json objec of what split strategy is used - if it is to use the same split from previously saved split file or to split into new groups 
    split_strat=json.loads(split_strat_obj) # Convert the json object to string value

    identical_inx_obj=request.form['identical_inx'] # This is the json object,  to get if the user chooses to use an identical split to an earlier run done just before the current run. User can select any one of the two options - either this or using a splitfile info
    identical_inx=json.loads(identical_inx_obj) # Convert the json object to string value
    print(split_strat)
    print(identical_inx)
    if(split_strat == 0): # When the split strategy is not to use any old files and to make new groups
        if( str(identical_inx) == "0"): # When a fresh split needs to be done and not select the previous run's split strategy
            print("not identical")
            test_split=(100-int(tr_split))  # Calculate the validation split
            test_dataset, train_data =get_Groups_Validation(test_split, sel_cols1) # Go to its function to see how the groups are split
            

        elif( str(identical_inx) == "1"): # to use the previous run's split strategy and create same groups and train, validation values
            #read indices from file and create data with those indices
            print("identical")
            prev_df_train = pd.read_csv('model_files/Training-data_file.csv') # Read this train-data csv file, if you are running this second time, surely there should be this file present because we save this file after every run
            prev_df_train = prev_df_train.set_index('Unnamed: 0') # Set_index creates a new column with the name 'Unnamed:0' and copies all the index values - need these index values to separate validation and training data
            prev_df_vali = pd.read_csv('model_files/Validation-data_file.csv') # Read this validation-data csv file, if you are running this second time, surely there should be this file present because we save this file after every run
            current_df = load_data('temp_pipeline_2.csv') # Read the original (with selected features only) input data
            train_data = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( prev_df_train[sel_cols1[0]] ) ) ] # Get exact train data by matching the ids of the previous train split data and the get all those rows from input processed data having same id values
            # train_data=train_data.reset_index().drop(['index'],axis=1) 
            test_dataset = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( prev_df_vali[sel_cols1[0]] ) ) ] # Get exact validation data by matching the ids of the previous validation split data and the get all those rows from input processed data having same id values
            test_dataset=test_dataset.reset_index().drop(['index'],axis=1) 
            test_dataset['Groups'] = prev_df_vali['Groups'].copy() # Copy the group values from previous run's dataset to current dataset
            

    elif(split_strat == 1): # When the split strategy is to use an old file and recreate the exact groups and values
        splitfile_name_obj=request.form['splitfile_name'] # Load the json object containing the name of the split-info file
        splitfile_name=json.loads(splitfile_name_obj) # Convert the json object to string value

        df = pd.read_pickle(splitfile_name[0]) # Read pkl files using read_pickle function
        current_df = load_data('temp_pipeline_2.csv') # Read the processed input data
        train_data = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( df.iloc[0]['Train_id'] ) ) ] # Get exact train data by matching the ids of the previous train split data and the get all those rows from input processed data having same id values
        # train_data=train_data.reset_index().drop(['index'],axis=1) 
        test_dataset = current_df.iloc[ np.where( current_df[sel_cols1[0]].isin( df.iloc[0]['Validation_id'] ) ) ] # Get exact validation data by matching the ids of the previous validation split data and the get all those rows from input processed data having same id values
        test_dataset = test_dataset.reset_index().drop(['index'],axis=1) 
        test_dataset['Groups'] = list(df.iloc[0]['Groups'].copy()) # Copy the group values from previous run's dataset to current dataset
        del df  # Delete temp dataframe


    # Save splitinfo for user if he decides to save locally
    splitinfo_df = pd.DataFrame(columns=['Unnamed: 0','Train_id','Validation_id','Groups','Splitvalue','ID_name']) # Create a dataframe to store the new splitinfo details as pkl file - this new splitinfo can be an import splitfile itself. Save this to use for next iteration
    splitinfo_df['Unnamed: 0'] = [np.array(train_data.index)] # Create a temporary column to save the index of the training data - index is important to get correct data from correct rows
    splitinfo_df['Train_id'] = [np.array(train_data[sel_cols1[0]])] # This column has one single row with an array of all train id values
    splitinfo_df['Validation_id'] = [np.array(test_dataset[sel_cols1[0]])] # This column has one single row with an array of all validation id values
    splitinfo_df['Groups'] = [np.array(test_dataset['Groups'])] # This column has one single row with an array of all groups - only validation has group values
    splitinfo_df['Splitvalue'] = tr_split # Store the training split value
    splitinfo_df['ID_name'] = str(sel_cols1[0])  # Store the ID name
    splitinfo_df.to_pickle('model_files/SplitInfo_file.pkl') # Save the dataframe as pickle file
    del splitinfo_df # Delete temp dataframe
    
    #Save train and validation csv files - not processed files, but the same indices from original dataset (just with selected columns)
    train_data.to_csv('model_files/Training-data_file.csv', index=True) # Save as training data file
    test_dataset.to_csv('model_files/Validation-data_file.csv', index=True) # Save as validation data file

    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    sel_cols3=sel_cols1[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
    print(sel_cols1)
    data=load_data('x_y.csv') # Load this csv which was stored after input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe

    valid_data=test_dataset[sel_cols3].copy() # Copy the actual validation data values to new dataframe
    vali_data_y=test_dataset[sel_cols1[-1]].copy() # Copy the actual validation target values to new dataframe
    vali_predictions_all=pd.DataFrame(vali_data_y.copy()) # Create a dataframe with actual validation target values 
    vali_predictions_all['Groups_generated']=test_dataset['Groups'].copy() # Copy the groups generated for validation
    vali_predictions_all[east]=test_dataset[east].copy() # Copy the east coordinate values
    vali_predictions_all[north]=test_dataset[north].copy() # Copy the north coordinate values
    vali_predictions_all['Distance_new']=test_dataset['Distance_new'].copy() # Copy the distance column values
    print(valid_data.head())
    
    # agg=0
    # for i in range(1,len(vali_predictions_all)):
    #     diff=distance_cal(vali_predictions_all[vali_predictions_all.columns[2]].iloc[i],vali_predictions_all[vali_predictions_all.columns[3]].iloc[i],vali_predictions_all[vali_predictions_all.columns[2]].iloc[i-1],vali_predictions_all[vali_predictions_all.columns[3]].iloc[i-1])
    #     agg=agg+diff
    #     vali_predictions_all['Distance_new'].iloc[i]=agg

    if("default_options_all" in process_options): # Checks if process_options has default as one of its values - like mentioned before, the process_options list has only 'best_options' as its value
        sub_x=load_data('lr_svr_x.csv')
        x_train2,y_train2=sub_x.iloc[list(train_data.index)],y.iloc[list(train_data.index)]
        # for rf,xgb
        rf_x=load_data('rf_xgb_x.csv')
        totrain=load_data('rf_xgb_x.csv')
        x_train,y_train=rf_x.iloc[list(train_data.index)],y.iloc[list(train_data.index)]
        # for nn
        nn_x=load_data('nn_x.csv')
        x_train3,y_train3=nn_x.iloc[list(train_data.index)],y.iloc[list(train_data.index)]

        predictions_all=pd.DataFrame(y)
        train_pred=pd.DataFrame(y_train)
        orig_data=load_data('temp_pipeline2.csv')
        some_temp=orig_data.iloc[list(train_data.index)]['Distance_new']
        some_temp=some_temp.reset_index().drop(['index'],axis=1)
        train_pred['Distance_new']= some_temp['Distance_new']
        del some_temp
        

        # process validation data
        lr_processed_data=logperm_gen_full(valid_data,r,g,b, colourspaces)#lr
        lr_processed_data=greyscale_gen_full(lr_processed_data,r,g,b)
        lr_processed_data=cluster_data_full(lr_processed_data,r,g,b)
        lr_processed_data=standard_scale_cluster_full(lr_processed_data,"lr")
        lr_processed_data=pca_reduction_cluster_full(lr_processed_data,"lr")
        
        rf_processed_data=logperm_gen_full(valid_data,r,g,b, colourspaces)
        rf_processed_data=greyscale_gen_full(rf_processed_data,r,g,b)
        rf_processed_data=cluster_data_full(rf_processed_data,r,g,b)
        rf_processed_data=poly_creation_cluster_full(rf_processed_data)
        rf_processed_data=standard_scale_cluster_full(rf_processed_data,"rf")
        rf_processed_data=correl_full(rf_processed_data,"rf")

        nn_processed_data=logperm_gen_full(valid_data,r,g,b, colourspaces)  
        nn_processed_data=greyscale_gen_full(nn_processed_data,r,g,b)
        nn_processed_data=cluster_data_full(nn_processed_data,r,g,b)
        nn_processed_data=poly_creation_cluster_full(nn_processed_data)
        nn_processed_data=standard_scale_cluster_full(nn_processed_data,"nn")
        nn_processed_data=correl_full(nn_processed_data,"nn")

        x_test2,y_test2=lr_processed_data, vali_data_y
        x_test,y_test=rf_processed_data, vali_data_y
        x_test3,y_test3=nn_processed_data, vali_data_y

    else:
        data=load_data('data_for_all_models.csv') # Load this csv - stored right after the input data was processed and engineered with the best processing options
        sub_x=data.copy() # Create copies of this data for different ml algorithms
        rf_x=data.copy() # Create copies of this data for different ml algorithms
        nn_x=data.copy() # Create copies of this data for different ml algorithms
        x_train2,y_train2=data.iloc[list(train_data.index)],y.iloc[list(train_data.index)] # Since we have split the validation data into different groups and also have the index values of trianing and validation data separately, we can just use those index value to copy the training and validation data for ml algorithms to train and predict on
        # for rf,xgb
        x_train,y_train=data.iloc[list(train_data.index)], y.iloc[list(train_data.index)] # Since we have split the validation data into different groups and also have the index values of trianing and validation data separately, we can just use those index value to copy the training and validation data for ml algorithms to train and predict on
        # for nn
        x_train3,y_train3=data.iloc[list(train_data.index)], y.iloc[list(train_data.index)] # Since we have split the validation data into different groups and also have the index values of trianing and validation data separately, we can just use those index value to copy the training and validation data for ml algorithms to train and predict on
        predictions_all=pd.DataFrame(y)   # Save the actual target values of full-input data to the dataframe
        train_pred=pd.DataFrame(y_train) # Save the actual training target values to the dataframe
        orig_data=load_data('temp_pipeline_2.csv') # Load the original (with user selected features only) dataset
        some_temp=orig_data.iloc[list(train_data.index)]['Distance_new'] # Copy distance column values for the training data - using the index values
        some_temp=some_temp.reset_index().drop(['index'],axis=1) # Reset and drop the index values for the updated dataframe 
        train_pred['Distance_new']= some_temp['Distance_new'] # Copy the distance values into the train_pred dataframe - copy at the correct index values
        del some_temp # Delete the temp dataframe

        ## We run the below lines of code to process the grouped validation data in the same way the training data ( input data ) was processed. NOTE I did not take the validation directly from the already processed data by matching the index values of the processed data and the grouped validation data - this can also be done 
        if("best_options" in process_options): # This always runs because the process options has 'best_options' as its only value
            option_csv=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
            process_options=[] # Initialze an empty list
            for i in range(len(option_csv)): # For all the values in the option_csv dataset
                process_options.append(option_csv.iloc[i]['best_option']) # Append the values into the new list

            processed_data=valid_data.copy() # Make a copy of the validation data
            if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
                processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                print("start1",processed_data.shape)

            if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
                processed_data=greyscale_gen_full(processed_data,r,g,b)
                print("start2",processed_data.shape)

            if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
                processed_data=cluster_data_full(processed_data,r,g,b)  
                print("start3",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=poly_creation_cluster_full(processed_data)    
                print("start4",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=poly_creation_no_cluster_full(processed_data)     
                print("start4",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=standard_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start5",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=standard_scale_no_cluster_full(processed_data,"common")    # Common specifies same data for all ml techniques 
                print("start5",processed_data.shape)
            
            if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=min_max_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start6",processed_data.shape)

            if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=min_max_scale_no_cluster_full(processed_data,"common")    # Common specifies same data for all ml techniques  

            if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=robust_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=robust_scale_no_cluster_full(processed_data,"common")  # Common specifies same data for all ml techniques   

            if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=power_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=power_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques

            if("pearson_correl" in process_options): # This line checks if pearson_correl is in the process_options list 
                processed_data=correl_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start7",processed_data.shape)

            if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
                lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv') # Load this csv which was stored when the poly_lasso_trans function ran while finding the best preprocessing methods 
                processed_data=lasso_reg_full(processed_data,lasso_result) # We pass the csv to copy the column names present in the lasso_result dataset - lasso_result dataset was saved after computing lasso reg during feature engineering phase
                print("start8",processed_data.shape)

            if(("pca_trans" in process_options) & ("cluster_gen" in process_options)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=pca_reduction_cluster_full(processed_data,"common")    # Common specifies same data for all ml techniques 

            if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=pca_reduction_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques      

            x_test2,y_test2= processed_data.copy(), vali_data_y.copy() # Make copies of validation data, and  validation target values. X_train2, Y_test2 is used for linear regression and svr algorithms
            x_test,y_test= processed_data.copy(), vali_data_y.copy() # Make copies of validation data, and  validation target values. X_train, Y_test is used for random forest and xgboost algorithms
            x_test3,y_test3= processed_data.copy(), vali_data_y.copy() # Make copies of validation data, and  validation target values. X_train3, Y_test3 is used for neural network
            

        else: #### This else part will not run because the process_options will always be 'best_options' as its value - might change in the future
            processed_data=valid_data.copy()
            if("logperm_gen" in process_options):
                processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                print("start1",processed_data.shape)

            if("greyscale_gen" in process_options):
                processed_data=greyscale_gen_full(processed_data,r,g,b)
                print("start2",processed_data.shape)

            if("cluster_gen" in process_options):
                processed_data=cluster_data_full(processed_data,r,g,b)  
                print("start3",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)):
                processed_data=poly_creation_cluster_full(processed_data)    
                print("start4",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=poly_creation_no_cluster_full(processed_data)     
                print("start4",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=standard_scale_cluster_full(processed_data,"common")
                print("start5",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=standard_scale_no_cluster_full(processed_data,"common")    
                print("start5",processed_data.shape)
            
            if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=min_max_scale_cluster_full(processed_data,"common")
                print("start6",processed_data.shape)

            if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=min_max_scale_no_cluster_full(processed_data,"common")    

            if(("robust_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=robust_scale_cluster_full(processed_data,"common")

            if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=robust_scale_no_cluster_full(processed_data,"common")    

            if(("power_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=power_scale_cluster_full(processed_data,"common")

            if(("power_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=power_scale_no_cluster_full(processed_data,"common")  

            if("pearson_correl" in process_options):
                processed_data=correl_full(processed_data,"common")
                print("start7",processed_data.shape)

            if("poly_lasso_trans" in process_options):
                lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv')
                processed_data=lasso_reg_full(processed_data,lasso_result)
                print("start8",processed_data.shape)

            if(("pca_trans" in process_options) & ("cluster_gen" in process_options)):
                processed_data=pca_reduction_cluster_full(processed_data,"common")    

            if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=pca_reduction_no_cluster_full(processed_data,"common")        

            x_test2,y_test2= processed_data, vali_data_y
            x_test,y_test= processed_data, vali_data_y
            x_test3,y_test3= processed_data, vali_data_y

    # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
    option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
    options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
    no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
    options_values=list(option_csv_df['best_option'])[0]  # Get the first row of the best processing options dataframe and convert to a list
    if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
        no_scale_check_lr=1 # atleast one scaling
    if(no_scale_check_lr==0): # If none present, then do this
        sc1=StandardScaler() # Initialize a normal scale object
        x_train2=sc1.fit_transform(x_train2) # Fit and transform the object on training data
        x_test2=sc1.transform(x_test2) # Just transform the validation data using the scale object that was fit on the training data
        sub_x=sc1.transform(sub_x) # Transform the full processed-input data
        joblib.dump(sc1,'model_files/'+'sc_forT1&5only.pkl') # Dump this file as pkl to use during the full-data part

    if(algo_choice=="Technique 1"): # If the selected algorithm is technique 1
        lr,predictions_all,fts,valid_results=LR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 1_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 1_traindata_pred.csv',index=False)
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump the model object as a pkl file to access later
        test_preds=lr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique1_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 1_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file


    if(algo_choice=="Technique 2"): # If the selected algorithm is technique 2
        rf,predictions_all,fts,valid_results=RF_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 2_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file 
       # final_train_preds.to_csv('model_files/'+'Technique 2_traindata_pred.csv',index=False)
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump the model object as a pkl file to access later
        test_preds=rf.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique2_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 2_vali_pred.csv',index=False)  # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 3"): # If the selected algorithm is technique 3
        xgb,predictions_all,fts,valid_results=XGB_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 3_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 3_traindata_pred.csv',index=False)
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump the model object as a pkl file to access later
        test_preds=xgb.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique3_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 3_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 4"): # If the selected algorithm is technique 4
        #make sure nn recieves scaled data. If scaling not done already , scale it now.
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            x_test3=sc.transform(x_test3) # Just transform the validation data using the scale object that was fit on the training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        nn,predictions_all,fts,valid_results=NN_reg(nn_x,x_train3,x_test3,y_train3,y_test3,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 4_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
       # final_train_preds.to_csv('model_files/'+'Technique 4_traindata_pred.csv',index=False)
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump the model object as a pkl file to access later
        test_preds=nn.predict(x_test3) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique4_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 4_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 5"): # If the selected algorithm is technique 5
        svr,predictions_all,fts,valid_results=SVR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 5_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
       # final_train_preds.to_csv('model_files/'+'Technique 5_traindata_pred.csv',index=False)
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump the model object as a pkl file to access later
        test_preds=svr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique5_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 5_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=='Try all Techniques'): # If all techniques option is selected by the user, then do this
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        
        # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            x_test3=sc.transform(x_test3) # Just transform the validation data using the scale object that was fit on the training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        lr,nn,rf,xgb,svr,predictions_all5,final_scores_test,valid_results=All_reg(sub_x,nn_x,rf_x,x_train,x_test,y_train,y_test,x_train2,x_test2,y_train2,y_test2,x_train3,x_test3,y_train3,y_test3,predictions_all) # Train all the techniques on training data and return the trained objects, predictions on full-input data, predictions on validation data and the metric scores for each of the five techniques
        print(x_train.shape)
        print(x_test.shape)
        print("Training dataset Evaluation Metrics")
        print(final_scores_test)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all5.head())
        fts=final_scores_test.copy() # Make a copy of the final metric scores of all the models 
        predictions_all_new=predictions_all5.copy() # Make a copy of the dataframe that has predictions of all techinques on the full-input data
        predictions_all_new.to_csv('model_files/'+'all_models_preds.csv',index=False) # Save the model predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'all_models_traindata_pred.csv',index=False)
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump each model object as a pkl file to access later
        test_preds1=lr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique1_vali_pred']=test_preds1 # Save the validation predictions as a dataframe
        test_preds2=rf.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique2_vali_pred']=test_preds2 # Save the validation predictions as a dataframe
        test_preds3=xgb.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique3_vali_pred']=test_preds3 # Save the validation predictions as a dataframe
        test_preds4=nn.predict(x_test3) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique4_vali_pred']=test_preds4 # Save the validation predictions as a dataframe
        test_preds5=svr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique5_vali_pred']=test_preds5    # Save the validation predictions as a dataframe     

        fts=fts.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        valid_results=valid_results.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        predictions_all_new=predictions_all_new.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        print(predictions_all_new.head())
        fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
        valid_results.to_csv('model_files/'+'validation_statistics.txt',index=False) # Save validation statistics as text file
        vali_predictions_all.to_csv('model_files/'+'all_models_vali_pred.csv',index=False) # Save the model predictions on validation data, as a csv file
        jsonfiles_fts = json.loads(fts.to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
        jsonfiles_valid_results = json.loads(valid_results.to_json(orient='records')) # Get json object for the first 5 rows of the validation metric scores (statistics)
        jsonfiles_predictions_all = json.loads(predictions_all_new.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
        return (jsonify(jsonfiles_fts,jsonfiles_valid_results,jsonfiles_predictions_all)) 

    # Below code runs when the the selected algorithm is not 'all techniques' option

    fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
    valid_results.to_csv('model_files/'+'validation_statistics.txt',index=False) # Save validation statistics as text file
    final_results_train=pd.concat([fts,valid_results],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    final_results_test=pd.concat([valid_results,fts],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    # print("fin res ++++",final_results[valid_results.columns])
    final_results_train=final_results_train.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    final_results_test=final_results_test.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    predictions_all=predictions_all.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values

    jsonfiles_fts = json.loads(final_results_train[fts.columns].to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
    jsonfiles_valid_results = json.loads(final_results_test[valid_results.columns].to_json(orient='records')) # Get json object for the first 5 rows of the validation metric scores (statistics)
    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3))  # Get json object for the first 5 rows of the predictions on the full-input data
    return (jsonify(jsonfiles_fts,jsonfiles_valid_results,jsonfiles_predictions_all))



@app.route('/check_if_file_present_vali',methods=['POST','GET'])
def check_if_file_present_vali(): #### NON-FUNCTIONAL
    cluster_count_obj=request.form['cluster_count']
    cluster_count=json.loads(cluster_count_obj)
    downloads_path=get_download_path()
    if(str(cluster_count)!= "-9"):
        downloads_path=str(downloads_path)+ '/vali_data_from_bctk_software' + str(cluster_count) + '.csv'
    elif(str(cluster_count)=="-9"):
        downloads_path=str(downloads_path)+ '/vali_data_from_bctk_software.csv'
    print(downloads_path)
    if(os.path.exists(downloads_path)):
        os.remove(downloads_path)
        return jsonify(" Exisiting file deleted - Great")
    else:
        return jsonify(" No path present - Great")    

# vali data csv input and process
@app.route('/vali_data_input',methods=['POST'])
def vali_data_input(): #### NON-FUNCTIONAL
    print("Merging")
    cluster_count_obj=request.form['cluster_count']
    cluster_count=json.loads(cluster_count_obj)
    full_river=pd.DataFrame()
    copy_done=0
    downloads_path=get_download_path()
    if(str(cluster_count) != "-9"):
        for i in range(1,int(cluster_count)+1):           
            file_path=str(downloads_path)+ '/vali_data_from_bctk_software' + str(i) + '.csv'
            df=pd.read_csv(file_path)
            full_river=pd.concat([full_river,df],axis=0)
            full_river=full_river.reset_index().drop(['index'],axis=1)
        copy_done=1

    elif(str(cluster_count)=="-9"): 
        print("-9")
        file_s=''
        downloads_path=''
        downloads_path=get_download_path()
        file_s=str(downloads_path)+ '/vali_data_from_bctk_software.csv'
        full_river=pd.read_csv(file_s)
        copy_done=1
        print("copy done")
            
    print("Train data sample" , full_river.head())
    print(full_river.tail())
    print(full_river.isnull().sum())
    if(True in list(full_river.iloc[-1].isnull()[:])):
        drop_ind=full_river.index[-1]
        print(drop_ind)
        full_river.drop(drop_ind,axis=0,inplace=True)
    full_columns=full_river.columns
    print(list(full_columns))    
    full_river.to_csv('model_files/'+'vali_data_software_generated.csv',index=False)
    if( (str(cluster_count) != "-9") & (copy_done==1) ):
        for i in range(1,int(cluster_count)+1):
            file_path=str(downloads_path)+ '/vali_data_from_bctk_software' + str(i) + ".csv"
            if(os.path.exists(file_path)):
                os.remove(file_path)
                print(" Deleted ",cluster_count)

    elif( (str(cluster_count) == "-9") & (copy_done==1) ):
        downloads_path=get_download_path()
        file_path=str(downloads_path)+ '/vali_data_from_bctk_software.csv'
        if(os.path.exists(file_path)):
            os.remove(file_path)
            print(" Deleted single file ")
        
    df=pd.read_csv('model_files/'+'vali_data_software_generated.csv', nrows=10) 
    full_columns=df.columns
    df_jsonfiles = json.loads(df.head().to_json(orient='records'))
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))

@app.route('/upload_vali_csv_file_ml', methods=['POST'])
def upload_vali_csv_file_ml(): # Save the uploaded validation csv to local path and rename to 'vali_data_software_generated.csv'
    if( request.method=="POST" ):
        if( request.files ):
            file=request.files["file"] # request variable is the flask variable which contains all the information sent from javascript side to python for processing, we access values by request.files[ name_of_variable_given_in_javascript_side ]
            print(file)
            file.save(os.path.join(app.config['FILE_SAVE'], file.filename)) # Same as input csv file, get the validation file from user and save it to model_files folder
            print(" file saved ")
            #f"{file.filename}
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'],"vali_data_software_generated.csv") )): # Remove any existing files with name vali_data_software_generated.csv. Note that the original location of the file is unchanged - meaning the original file untouched, but a copy of the file is stored in model_files folder and that file is renamed to temp_pipeline.csv which will be used almost everywhere in the code
                os.remove( os.path.join(app.config['FILE_SAVE'],"vali_data_software_generated.csv") ) # If another file named vali_data_software_generated.csv already exists, then delete that and rename the new file to vali_data_software_generated.csv
            os.rename( os.path.join(app.config['FILE_SAVE'], file.filename), os.path.join(app.config['FILE_SAVE'],"vali_data_software_generated.csv") ) # Rename the copied file to vali_data_software_generated.csv
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'], file.filename)) ): # Remove the initial copy of the user's copied file - we only rename it, so this line won't probably run, but still if multiple copies of same file are created somehow, then delete them
                os.remove( os.path.join(app.config['FILE_SAVE'], file.filename) ) # Remove the duplicate files
            
            res = make_response(jsonify({"message":  f" Validation data uploaded successfully "}),200) # Send back response to user saying files are uploaded

            return res
        return render_template('simplified_workflow.html/upload_vali_csv_file_ml')


@app.route('/ml_algorithms_upload_vali_csv',methods=['GET','POST'])
def ml_algorithms_upload_vali_csv(): # Validation strategy - UPLOAD EXTERNAL CSV AS VALIDATION DATA
    process_options_obj=request.form['process_options'] # Request to get the processing options ( updated - this is a constant value and is set to best_options in the javascript side ) as json object
    process_options=json.loads(process_options_obj) # Convert the json object to list
    print(process_options)
    ml_algo_obj=request.form['ml_algo'] # Request to get the selected ml technique as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string
    print(algo_choice)

    colourspaces_json=request.form['colourspaces'] # Request to get the colourspace option as json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to list

    tr_split=100 # Use 100% of input data for training - since we have a separate input csv for validation
    print(tr_split)

    feats=request.form['sel_feat'] # Request to get the selected features ( FOR THE NEW UPLOADED CSV ) as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list

    selected_feats_b4_json=request.form['selected_feats_b4'] # Get json object of what features were used for the input data-processing.
    feats_temp  = json.loads(selected_feats_b4_json) # Convert the json object to list

    ## Now for the uploaded csv - do all the pre-processing we did over the input dataset

    test_dataset=pd.read_csv('model_files/'+'vali_data_software_generated.csv') # Input the validation csv file
    # edit test_dataset and add required colourspaces
    r=feats_temp[3] #3 because of east north corrd input. Get R column name 
    g=feats_temp[4] # Get G column name
    b=feats_temp[5] # Get B column name
    
    # check if 0 value is present in r,g,b
    if(test_dataset.describe()[r]['min']==0):   # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                 
        test_dataset[r]=np.where(test_dataset[r]==0,0.1,test_dataset[r])
    if(test_dataset.describe()[g]['min']==0):   # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
        test_dataset[g]=np.where(test_dataset[g]==0,0.1,test_dataset[g])
    if(test_dataset.describe()[b]['min']==0):   # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
        test_dataset[b]=np.where(test_dataset[b]==0,0.1,test_dataset[b])
    
    if(test_dataset.describe()[r]['min']<0):    # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                
        test_dataset[r]=np.where(test_dataset[r]<0,0.1,test_dataset[r])
    if(test_dataset.describe()[g]['min']<0):    # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                
        test_dataset[g]=np.where(test_dataset[g]<0,0.1,test_dataset[g])
    if(test_dataset.describe()[b]['min']<0):    # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                               
        test_dataset[b]=np.where(test_dataset[b]<0,0.1,test_dataset[b])

    print(colourspaces)
    if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
        print('same dataset - no hsv only rgb')

    elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
        print('same dataset - no hsv only rgb')

    elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
        print('change in dataset - no rgb only hsv')
        # hsv_vals = matplotlib.colors.rgb_to_hsv(test_dataset[[r,g,b]]/255)
        hsv_vals = test_dataset[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        test_dataset2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
        test_dataset[r] = test_dataset2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset[g] = test_dataset2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset[b] = test_dataset2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        test_dataset = test_dataset.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
        for ix, valx in enumerate(feats_temp): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
            if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                feats_temp[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
            if(valx == g):
                feats_temp[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
            if(valx == b):
                feats_temp[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
        
        # check if 0 value is present in r,g,b
        if(test_dataset.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                
            test_dataset['H_generated']=np.where(test_dataset['H_generated']==0,0.1,test_dataset['H_generated'])
        if(test_dataset.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                               
            test_dataset['S_generated']=np.where(test_dataset['S_generated']==0,0.1,test_dataset['S_generated'])
        if(test_dataset.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                               
            test_dataset['V_generated']=np.where(test_dataset['V_generated']==0,0.1,test_dataset['V_generated'])       

        del test_dataset2 # Delete the temporary dataframe

    elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
        print('change in dataset - rgb and hsv')
        hsv_vals = test_dataset[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
        test_dataset2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'] ) # Create new dataframe with just hsv values
        test_dataset.insert(6, 'H_generated', test_dataset2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset.insert(7, 'S_generated', test_dataset2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
        test_dataset.insert(8, 'V_generated', test_dataset2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

        #insert in feats_temp - since we created new features, they should be added for further use in the next parts of the code
        feats_temp.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        feats_temp.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
        feats_temp.insert(8, 'V_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

        # check if 0 value is present in r,g,b
        if(test_dataset.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                     
            test_dataset['H_generated']=np.where(test_dataset['H_generated']==0,0.1,test_dataset['H_generated'])
        if(test_dataset.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                    
            test_dataset['S_generated']=np.where(test_dataset['S_generated']==0,0.1,test_dataset['S_generated'])
        if(test_dataset.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                   
            test_dataset['V_generated']=np.where(test_dataset['V_generated']==0,0.1,test_dataset['V_generated'])       

        del test_dataset2 # Delete the temporary dataframe


    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    r=sel_cols1[3] # Get R column name
    g=sel_cols1[4] # Get G column name
    b=sel_cols1[5] # Get B column name
    sel_cols3=sel_cols1[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
    data=load_data('x_y.csv') # Load this csv which was stored after input data pre-processing
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe
    valid_data=test_dataset[sel_cols3].copy() # Copy the actual validation data values to new dataframe
    vali_data_y=test_dataset[sel_cols1[-1]].copy() # Copy the actual validation target values to new dataframe
    vali_predictions_all=pd.DataFrame(vali_data_y.copy()) # Create a dataframe with actual validation target values 
    vali_predictions_all[east]=test_dataset[east].copy() # Copy the east coordinate values
    vali_predictions_all[north]=test_dataset[north].copy() # Copy the north coordinate values

    vali_predictions_all['Distance_new']=0 # To create Distance values for the new validation dataset - Create new column named Distance_new and set all rows to 0
    agg=0 # Set aggregate value to 0 
    for i in range(1,len(vali_predictions_all)): # for every row in the dataframe
        diff=distance_cal(vali_predictions_all[vali_predictions_all.columns[1]].iloc[i],vali_predictions_all[vali_predictions_all.columns[2]].iloc[i],vali_predictions_all[vali_predictions_all.columns[1]].iloc[i-1],vali_predictions_all[vali_predictions_all.columns[2]].iloc[i-1]) # Calculate distance between current point and previous point 
        agg=agg+diff # Add the distance to get the aggregate value
        vali_predictions_all['Distance_new'].iloc[i]=agg # Set the aggregate value to the Distance_new column in teh dataframe, for every row
       
    if("default_options_all" in process_options): # Checks if process_options has default as one of its values - like mentioned before, the process_options list has only 'best_options' as its value
        sub_x=load_data('lr_svr_x.csv')
        x_train2,y_train2=sub_x,y
        # for rf,xgb
        rf_x=load_data('rf_xgb_x.csv')
        totrain=load_data('rf_xgb_x.csv')
        x_train,y_train=rf_x,y
        # for nn
        nn_x=load_data('nn_x.csv')
        x_train3,y_train3=nn_x,y

        predictions_all=pd.DataFrame(y)
        train_pred=pd.DataFrame(y_train)

        # process validation data
        lr_processed_data=logperm_gen_full(valid_data,r,g,b, colourspaces)#lr
        lr_processed_data=greyscale_gen_full(lr_processed_data,r,g,b)
        lr_processed_data=cluster_data_full(lr_processed_data,r,g,b)
        lr_processed_data=standard_scale_cluster_full(lr_processed_data,"lr")
        lr_processed_data=pca_reduction_cluster_full(lr_processed_data,"lr")
        
        rf_processed_data=logperm_gen_full(valid_data,r,g,b, colourspaces)
        rf_processed_data=greyscale_gen_full(rf_processed_data,r,g,b, colourspaces)
        rf_processed_data=cluster_data_full(rf_processed_data,r,g,b)
        rf_processed_data=poly_creation_cluster_full(rf_processed_data)
        rf_processed_data=standard_scale_cluster_full(rf_processed_data,"rf")
        rf_processed_data=correl_full(rf_processed_data,"rf")

        nn_processed_data=logperm_gen_full(valid_data,r,g,b, colourspaces)  
        nn_processed_data=greyscale_gen_full(nn_processed_data,r,g,b)
        nn_processed_data=cluster_data_full(nn_processed_data,r,g,b)
        nn_processed_data=poly_creation_cluster_full(nn_processed_data)
        nn_processed_data=standard_scale_cluster_full(nn_processed_data,"nn")
        nn_processed_data=correl_full(nn_processed_data,"nn")

        x_test2,y_test2=lr_processed_data, vali_data_y
        x_test,y_test=rf_processed_data, vali_data_y
        x_test3,y_test3=nn_processed_data, vali_data_y

    else:
        data=load_data('data_for_all_models.csv') # Load this csv - stored right after the input data was processed and engineered with the best processing options
        sub_x=data.copy() # Create copies of this data for different ml algorithms
        rf_x=data.copy() # Create copies of this data for different ml algorithms
        nn_x=data.copy()  # Create copies of this data for different ml algorithms
        x_train2,y_train2=data,y # Since the validation csv is already uploaded, we have made copies of that dataset, so now we assign the processed input data as training data and the newly processed uploaded dataset as the validation data. X_train2, X_test2 is used for linear regression and svr algorithms
        # for rf,xgb
        x_train,y_train=data,y # Since the validation csv is already uploaded, we have made copies of that dataset, so now we assign the processed input data as training data and the newly processed uploaded dataset as the validation data. X_train, X_test is used for random forest and xgboost algorithms
        # for nn
        x_train3,y_train3=data,y # Since the validation csv is already uploaded, we have made copies of that dataset, so now we assign the processed input data as training data and the newly processed uploaded dataset as the validation data.  X_train3, X_test3 is used for neural network 
        predictions_all=pd.DataFrame(y) # Save the actual target values of full-input data to the dataframe
        train_pred=pd.DataFrame(y_train)   # Save the actual training target values to the dataframe 

        if("best_options" in process_options): # This will always run - 'best_options' is the only value present in the process_options list
            option_csv=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
            process_options=[] # Initialze an empty list
            for i in range(len(option_csv)): # For all the values in the option_csv dataset
                process_options.append(option_csv.iloc[i]['best_option']) # Append the values into the new list

            ## We run the below lines of code to process the new validation data in the same way the training data ( input data ) was processed.

            processed_data=valid_data.copy() # Make a copy of the new uploaded validation csv
            if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
                processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                print("start1",processed_data.shape)

            if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
                processed_data=greyscale_gen_full(processed_data,r,g,b)
                print("start2",processed_data.shape)

            if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
                processed_data=cluster_data_full(processed_data,r,g,b)  
                print("start3",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=poly_creation_cluster_full(processed_data)    
                print("start4",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=poly_creation_no_cluster_full(processed_data)     
                print("start4",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=standard_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start5",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=standard_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques  
                print("start5",processed_data.shape)
            
            if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=min_max_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start6",processed_data.shape)

            if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=min_max_scale_no_cluster_full(processed_data,"common")  # Common specifies same data for all ml techniques   

            if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=robust_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=robust_scale_no_cluster_full(processed_data,"common")  # Common specifies same data for all ml techniques   

            if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=power_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=power_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques

            if("pearson_correl" in process_options): # This line checks if pearson_correl is in the process_options list 
                processed_data=correl_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start7",processed_data.shape)

            if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
                lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv') # Load this csv which was stored when the poly_lasso_trans function ran while finding the best preprocessing methods 
                processed_data=lasso_reg_full(processed_data,lasso_result) # We pass the csv to copy the column names present in the lasso_result dataset - lasso_result dataset was saved after computing lasso reg during feature engineering phase
                print("start8",processed_data.shape)

            if(("pca_trans" in process_options) & ("cluster_gen" in process_options)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=pca_reduction_cluster_full(processed_data,"common")     # Common specifies same data for all ml techniques

            if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=pca_reduction_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques      

            x_test2,y_test2= processed_data, vali_data_y
            x_test,y_test= processed_data, vali_data_y
            x_test3,y_test3= processed_data, vali_data_y

        else: #### This else part will not run because the process_options will always be 'best_options' as its value - might change in the future
            processed_data=valid_data.copy()
            if("logperm_gen" in process_options):
                processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                print("start1",processed_data.shape)

            if("greyscale_gen" in process_options):
                processed_data=greyscale_gen_full(processed_data,r,g,b)
                print("start2",processed_data.shape)

            if("cluster_gen" in process_options):
                processed_data=cluster_data_full(processed_data,r,g,b)  
                print("start3",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)):
                processed_data=poly_creation_cluster_full(processed_data)    
                print("start4",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=poly_creation_no_cluster_full(processed_data)     
                print("start4",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=standard_scale_cluster_full(processed_data,"common")
                print("start5",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=standard_scale_no_cluster_full(processed_data,"common")    
                print("start5",processed_data.shape)
            
            if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=min_max_scale_cluster_full(processed_data,"common")
                print("start6",processed_data.shape)

            if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=min_max_scale_no_cluster_full(processed_data,"common")    

            if(("robust_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=robust_scale_cluster_full(processed_data,"common")

            if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=robust_scale_no_cluster_full(processed_data,"common")    

            if(("power_scale" in process_options) & ("cluster_gen" in process_options)):
                processed_data=power_scale_cluster_full(processed_data,"common")

            if(("power_scale" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=power_scale_no_cluster_full(processed_data,"common")  

            if("pearson_correl" in process_options):
                processed_data=correl_full(processed_data,"common")
                print("start7",processed_data.shape)

            if("poly_lasso_trans" in process_options):
                lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv')
                processed_data=lasso_reg_full(processed_data,lasso_result)
                print("start8",processed_data.shape)

            if(("pca_trans" in process_options) & ("cluster_gen" in process_options)):
                processed_data=pca_reduction_cluster_full(processed_data,"common")    

            if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)):
                processed_data=pca_reduction_no_cluster_full(processed_data,"common")        

            x_test2,y_test2= processed_data, vali_data_y
            x_test,y_test= processed_data, vali_data_y
            x_test3,y_test3= processed_data, vali_data_y

    # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
    option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
    options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
    no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
    options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
    if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
        no_scale_check_lr=1 # atleast one scaling
    if(no_scale_check_lr==0):  # If none present, then do this
        sc=StandardScaler() # Initialize a normal scale object
        x_train2=sc.fit_transform(x_train2) # Fit and transform the object on training data
        x_test2=sc.transform(x_test2) # Just transform the validation data using the scale object that was fit on the training data
        sub_x=sc.transform(sub_x) # Transform the full processed-input data
        joblib.dump(sc,'model_files/'+'sc_forT1&5only.pkl') # Dump this file as pkl to use during the full-data part

    if(algo_choice=="Technique 1"): # If the selected algorithm is technique 1
        lr,predictions_all,fts,valid_results=LR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 1_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 1_traindata_pred.csv',index=False)
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump the model object as a pkl file to access later
        test_preds=lr.predict(x_test2)  # Use the trained model to predict on the validation data
        vali_predictions_all['Technique1_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 1_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file


    if(algo_choice=="Technique 2"): # If the selected algorithm is technique 2
        rf,predictions_all,fts,valid_results=RF_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 2_pred_temp.csv',index=False)  # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 2_traindata_pred.csv',index=False)
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump the model object as a pkl file to access later
        test_preds=rf.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique2_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 2_vali_pred.csv',index=False)  # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 3"): # If the selected algorithm is technique 3
        xgb,predictions_all,fts,valid_results=XGB_reg(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 3_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
       #final_train_preds.to_csv('model_files/'+'Technique 3_traindata_pred.csv',index=False)
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump the model object as a pkl file to access later
        test_preds=xgb.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique3_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 3_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 4"): # If the selected algorithm is technique 4
        # make sure nn recieves scaled data. If scaling not done already , scale it now.
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done)  
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            x_test3=sc.transform(x_test3) # Just transform the validation data using the scale object that was fit on the training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        nn,predictions_all,fts,valid_results=NN_reg(nn_x,x_train3,x_test3,y_train3,y_test3,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 4_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 4_traindata_pred.csv',index=False)
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump the model object as a pkl file to access later
        test_preds=nn.predict(x_test3) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique4_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 4_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=="Technique 5"): # If the selected algorithm is technique 5
        svr,predictions_all,fts,valid_results=SVR_reg(sub_x,x_train2,x_test2,y_train2,y_test2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, predictions on validation data and the metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 5_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'Technique 5_traindata_pred.csv',index=False)
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump the model object as a pkl file to access later
        test_preds=svr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique5_vali_pred']=test_preds # Save the validation predictions as a dataframe
        vali_predictions_all.to_csv('model_files/'+'Technique 5_vali_pred.csv',index=False) # Save this dataframe as csv and let user choose a location to copy this file

    if(algo_choice=='Try all Techniques'): # If all techniques option is selected by the user, then do this
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        
        # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            x_test3=sc.transform(x_test3) # Just transform the validation data using the scale object that was fit on the training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        lr,nn,rf,xgb,svr,predictions_all5,final_scores_test,valid_results=All_reg(sub_x,nn_x,rf_x,x_train,x_test,y_train,y_test,x_train2,x_test2,y_train2,y_test2,x_train3,x_test3,y_train3,y_test3,predictions_all) # Train all the techniques on training data and return the trained objects, predictions on full-input data, predictions on validation data and the metric scores for each of the five techniques
        print(x_train.shape)
        print(x_test.shape)
        print("Training dataset Evaluation Metrics")
        print(final_scores_test)
        print("Validation dataset Evaluation Metrics :")
        print(valid_results)
        print("Predictions")
        print(predictions_all5.head())
        fts=final_scores_test.copy() # Make a copy of the final metric scores of all the models 
        predictions_all_new=predictions_all5.copy() # Make a copy of the dataframe that has predictions of all techinques on the full-input data
        predictions_all_new.to_csv('model_files/'+'all_models_preds.csv',index=False) # Save the model predictions on full-input data, as a csv file
        #final_train_preds.to_csv('model_files/'+'all_models_traindata_pred.csv',index=False)
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump each model object as a pkl file to access later
        test_preds1=lr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique1_vali_pred']=test_preds1 # Save the validation predictions as a dataframe
        test_preds2=rf.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique2_vali_pred']=test_preds2 # Save the validation predictions as a dataframe
        test_preds3=xgb.predict(x_test) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique3_vali_pred']=test_preds3 # Save the validation predictions as a dataframe
        test_preds4=nn.predict(x_test3) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique4_vali_pred']=test_preds4 # Save the validation predictions as a dataframe
        test_preds5=svr.predict(x_test2) # Use the trained model to predict on the validation data
        vali_predictions_all['Technique5_vali_pred']=test_preds5    # Save the validation predictions as a dataframe     


        fts=fts.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        valid_results=valid_results.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        predictions_all_new=predictions_all_new.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
        valid_results.to_csv('model_files/'+'validation_statistics.txt',index=False) # Save validation statistics as text file
        vali_predictions_all.to_csv('model_files/'+'all_models_vali_pred.csv',index=False) # Save the model predictions on validation data, as a csv file
        jsonfiles_fts = json.loads(fts.to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
        jsonfiles_valid_results = json.loads(valid_results.to_json(orient='records')) # Get json object for the first 5 rows of the validation metric scores (statistics)
        jsonfiles_predictions_all = json.loads(predictions_all_new.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
        return (jsonify(jsonfiles_fts,jsonfiles_valid_results,jsonfiles_predictions_all)) 

    # Below code runs when the the selected algorithm is not 'all techniques' option

    fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
    valid_results.to_csv('model_files/'+'validation_statistics.txt',index=False) # Save validation statistics as text file
    final_results_train=pd.concat([fts,valid_results],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    final_results_test=pd.concat([valid_results,fts],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    # print("fin res ++++",final_results[valid_results.columns])
    final_results_train=final_results_train.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    final_results_test=final_results_test.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    predictions_all=predictions_all.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values

    jsonfiles_fts = json.loads(final_results_train[fts.columns].to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
    jsonfiles_valid_results = json.loads(final_results_test[valid_results.columns].to_json(orient='records')) # Get json object for the first 5 rows of the validation metric scores (statistics)
    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
    return (jsonify(jsonfiles_fts,jsonfiles_valid_results,jsonfiles_predictions_all))


@app.route('/ml_algorithms_no_vali',methods=['GET','POST'])
def ml_algorithms_no_vali(): # Validation strategy - NO VALIDATION
    process_options_obj=request.form['process_options'] # Request to get the processing options ( updated - this is a constant value and is set to best_options in the javascript side ) as json object
    process_options=json.loads(process_options_obj) # Convert the json object to list
    print(process_options)
    ml_algo_obj=request.form['ml_algo'] # Request to get the selected ml technique as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string
    print(algo_choice)

    tr_split=100 # Make train split as 100 - to assign all input data as training data since there is no validation needed
    print(tr_split)

    feats=request.form['sel_feat'] # Request to get the selected features as json object
    sel_cols1  = json.loads(feats) # Convert the json object to list
    east=sel_cols1[1] # Get east coordinate column name
    north=sel_cols1[2] # Get north coordinate column name
    test_split=(100-int(tr_split))/100 # Not needed - no validation
    data=load_data('x_y.csv') # Load the previously stored x_y.csv file - contains both dependent and independent data
    data.sort_index(inplace=True) # Sort index to make sure everything is in order
    y=data[sel_cols1[-1]].copy() # Get the data's target values and store as separate dataframe

    # for east and north
    spl_data=load_data('temp_pipeline_2.csv') # This is the original input data - with all columns and variables
    x_train4,y_train4=spl_data,y # No need to do this line - no split needed
    
    if("default_options_all" in process_options): # Checks if process_options has default as one of its values - like mentioned before, the process_options list has only 'best_options' as its value
        sub_x=load_data('lr_svr_x.csv')
        x_train2,y_train2=sub_x,y
        # for rf,xgb
        rf_x=load_data('rf_xgb_x.csv')
        totrain=load_data('rf_xgb_x.csv')
        x_train,y_train=rf_x,y
        x_train_temp,x_test,y_train_temp,y_test=tts(rf_x,y,0.25)
        # for nn
        nn_x=load_data('nn_x.csv')
        x_train3,y_train3=nn_x,y
        predictions_all=pd.DataFrame(y)
        
    else:
        data=load_data('data_for_all_models.csv') # Load this csv - stored right after the input data was processed and engineered with the best processing options
        sub_x=data.copy() # Create copies of this data for different ml algorithms
        rf_x=data.copy() # Create copies of this data for different ml algorithms
        nn_x=data.copy() # Create copies of this data for different ml algorithms
        x_train2,y_train2=data,y # Assign the processed input data as training data, and copy the actual input-data target values as training target values. X_train2, X_test2 is used for linear regression and svr algorithms 
        # for rf,xgb
        x_train,y_train=data,y # Assign the processed input data as training data, and copy the actual input-data target values as training target values. X_train, X_test is used for random forest and xgboost algorithms
        x_train_temp,x_test,y_train_temp,y_test=tts(rf_x,y,0.25) # This is very important line - the reason we split only for rf_x data ( which is used by xgboost and random forest) is because the grid search in xgboost needs validation and training data set even if there is no validation - grid search cv is used to find best parameters and passing full input data to would overfit too much and so we just take out a 0.25 fraction and pass rest for grid search. We then train the xgboost with best parameters over the full-input data without any split
        # for nn
        x_train3,y_train3=data,y # Assign the processed input data as training data, and copy the actual input-data target values as training target values.  X_train3, X_test3 is used for neural network 
        predictions_all=pd.DataFrame(y)   # Save the actual target values of full-input data to the dataframe

    option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
    options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
    
    # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
    no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
    options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
    if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
        no_scale_check_lr=1 # atleast one scaling
    if(no_scale_check_lr==0): # If none present, then do this
        sc=StandardScaler() # Initialize a normal scale object
        x_train2=sc.fit_transform(x_train2) # Fit and transform the object on training data
        sub_x=sc.transform(sub_x) # Transform the full processed-input data
        joblib.dump(sc,'model_files/'+'sc_forT1&5only.pkl') # Dump this file as pkl to use during the full-data part

    if(algo_choice=="Technique 1"): # If the selected algorithm is technique 1
        lr,predictions_all,fts=LR_reg_no_vali(sub_x,x_train2,y_train2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, and the training metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 1_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump the model object as a pkl file to access later
        

    if(algo_choice=="Technique 2"): # If the selected algorithm is technique 2
        rf,predictions_all,fts=RF_reg_no_vali(rf_x,x_train,y_train,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, and the training metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 2_pred_temp.csv',index=False)  # Save the model's predictions on full-input data, as a csv file
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump the model object as a pkl file to access later

    if(algo_choice=="Technique 3"): # If the selected algorithm is technique 3
        xgb,predictions_all,fts=XGB_reg_no_vali(rf_x,x_train,x_test,y_train,y_test,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, and the training metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 3_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl') # Dump the model object as a pkl file to access later

    if(algo_choice=="Technique 4"): # If the selected algorithm is technique 4
        # make sure nn recieves scaled data. If scaling not done already , scale it now.
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        nn,predictions_all,fts=NN_reg_no_vali(nn_x,x_train3,y_train3,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, and the training metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 4_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump the model object as a pkl file to access later

    if(algo_choice=="Technique 5"): # If the selected algorithm is technique 5
        svr,predictions_all,fts=SVR_reg_no_vali(sub_x,x_train2,y_train2,predictions_all) # Train the technique on training data and return the trained object, predictions on full-input data, and the training metric scores for the technique
        print("Training dataset Evaluation Metrics")
        print(fts)
        print("Predictions")
        print(predictions_all.head())
        predictions_all.to_csv('model_files/'+'Technique 5_pred_temp.csv',index=False) # Save the model's predictions on full-input data, as a csv file
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump the model object as a pkl file to access later

    if(algo_choice=='Try all Techniques'): # If all techniques option is selected by the user, then do this
        option_csv_df=load_data('option_csv.csv') # Load the options_csv file which contains the best processing methods list
        options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
        
        # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
        no_scale_check=0 # Initialize a scaling check for nn (lr and svr data already done) 
        options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
        if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
            no_scale_check=1 # atleast one scaling
        if(no_scale_check==0): # If none present, then do this
            sc=StandardScaler() # Initialize a normal scale object
            x_train3=sc.fit_transform(x_train3) # Fit and transform the object on training data
            nn_x=sc.transform(nn_x) # Transform the full processed-input data
            joblib.dump(sc,'model_files/'+'sc_forT4only.pkl') # Dump this file as pkl to use during the full-data part
        lr,nn,rf,xgb,svr,predictions_all5,final_scores_test=All_reg_no_vali(sub_x,nn_x,rf_x,x_train,x_test,y_train,y_test,x_train2,y_train2,x_train3,y_train3,predictions_all) # Train all the techniques on training data and return the trained objects, predictions on full-input data, and the metric scores for each of the five techniques
        print(x_train.shape)
        print(x_test.shape)
        print("Training dataset Evaluation Metrics")
        print(final_scores_test)
        print("Predictions")
        print(predictions_all5.head())
        fts=final_scores_test.copy() # Make a copy of the final metric scores of all the models 
        predictions_all_new=predictions_all5.copy() # Make a copy of the dataframe that has predictions of all techinques on the full-input data
        predictions_all_new.to_csv('model_files/'+'all_models_preds.csv',index=False) # Save the model predictions on full-input data, as a csv file
        joblib.dump(lr,'model_files/'+'Technique 1.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(svr,'model_files/'+'Technique 5.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(nn,'model_files/'+'Technique 4.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(rf,'model_files/'+'Technique 2.pkl') # Dump each model object as a pkl file to access later
        joblib.dump(xgb,'model_files/'+'Technique 3.pkl')     # Dump each model object as a pkl file to access later

        fts=fts.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        predictions_all_new=predictions_all_new.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
        fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
        jsonfiles_fts = json.loads(fts.to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
        jsonfiles_predictions_all = json.loads(predictions_all_new.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
        return (jsonify(jsonfiles_fts,jsonfiles_predictions_all)) 

    # Below code runs when the the selected algorithm is not 'all techniques' option

    fts.to_csv('model_files/'+'training_statistics.txt',index=False) # Save training statistics as text file
    valid_results=pd.DataFrame(data=fts.values,columns=['1. temp_Model Name','2. temp_Root Mean Squared Error(RMSE)','3. temp_Mean Absolute Error(MAE)','4. temp_Pearson Correlation Coefficient','5. temp_Maximum Residual Error','6. temp_Explained variance Score']) # Convert the metric values list to a dataframe with proper column names 
    final_results_train=pd.concat([fts,valid_results],axis=0) # Merge both training statistics and validation statistics into one dataframe - json object sometimes has problems managing and sending a single row of data - so we merge both rows from training and validation and send by choosing row number for each json object - depending on if the json object is for validation or training metrics
    # print("fin res ++++",final_results[valid_results.columns])
    final_results_train=final_results_train.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values
    predictions_all=predictions_all.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values of original values

    jsonfiles_fts = json.loads(final_results_train[fts.columns].to_json(orient='records')) # Get json object for the first 5 rows of the training metric scores (statistics)
    jsonfiles_predictions_all = json.loads(predictions_all.head().to_json(orient='records',double_precision=3)) # Get json object for the first 5 rows of the predictions on the full-input data
    return (jsonify(jsonfiles_fts,jsonfiles_predictions_all))



#### SECTION 10 : GRAPHING FUNCTIONS FOR MACHINE-LEARNING MODULE AND SAVE PLOTS AS PDF


# Store all GROUPED validation plots to local folder and export to user's selected location
@app.route('/export_validation_plots_cf', methods=['POST','GET'])
def export_validation_plots_cf():
    
    vali_mode_obj=request.form['vali_mode']  # Get the validation mode as json object
    vali_mode=json.loads(vali_mode_obj) # Convert the json object to string

    file_path_obj= request.form['file_path'] # Get a json object of the file path - of the user's selected location
    file_path=json.loads(file_path_obj) # Convert the json object to string

    send_feats_obj=request.form['send_feats'] # Get a json object of all the feature names selected by the user during data input phase
    send_feats=json.loads(send_feats_obj) # Convert the json object to string
    
    print(vali_mode)
    if(vali_mode=="auto_select_vali"): # If the validation mode is auto_select_vali (grouped validation), then do this. For other validation modes, this function is not accessible

        vali_preds = pd.read_csv('model_files/all_models_vali_pred.csv') # Load the validation predictions dataframe which also has actual target values, we saved this file earlier - after curve-fitting 
        print(vali_preds.head())

        if(os.path.exists('model_files/All_validation_plots')): # If this file 'All_validation_plots' already exists in local path
            shutil.rmtree('model_files/All_validation_plots', ignore_errors=True) # If it's there, then remove the folder
        os.mkdir('model_files/All_validation_plots') # Create a new directory/folder called 'All_validation_plots'
        
        # Get max group count and save plots for all groups
        max_grp = len(vali_preds['Groups_generated'].unique())
        print(max_grp)
        
        import matplotlib
        matplotlib.use('agg') # This is very important line - avoids some issues with pytinkter (which matplotlib uses to show its graph windows) and builder files
        import matplotlib.pyplot as plt
        from PyPDF2 import PdfFileMerger
        # If you have issues with numpy and pypdf2 - check this out to replace this code:  https://github.com/mstamy2/PyPDF2/issues/67
        plt.style.use('seaborn-whitegrid')
        # plt.figure(figsize=(100,100))
        pdfs = [] # Create an empty list - this is to store all the names of output pdfs (for each validation groups)
        
        # For all curves
    
        techs=['Curve 1','Curve 2','Curve 3','Curve 4','Curve 5'] # Save all curve headers to a list
        for tech in techs: # For each curve header
            pdfs = [] # Initialize this new list again
            plt.figure(figsize=(15,5)) # Set a constant figure size to correctly fit into a pdf file
            if(tech=="Curve 1"):
                colname="Curve_1" # Define a column name to search from the dataset, for each curve header
            if(tech=="Curve 2"):
                colname="Curve_2" # Define a column name to search from the dataset, for each curve header
            if(tech=="Curve 3"):
                colname="Curve_3" # Define a column name to search from the dataset, for each curve header
            if(tech=="Curve 4"):
                colname="Curve_4" # Define a column name to search from the dataset, for each curve header
            if(tech=="Curve 5"):
                colname="Curve_5" # Define a column name to search from the dataset, for each curve header

            for numb in range(1, max_grp+1): # For all groups from 1 to the maximum group number
                plt.figure(figsize=(15,5)) # Set a constant figure size to correctly fit into a pdf file
                group_numb = "Group_"+str(numb) # Get the group number as string - in format 'Group_1' or 'Group_10' etc
                print(group_numb)
                plt.plot( vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new'] , vali_preds[vali_preds['Groups_generated']==str(group_numb)][send_feats[-1]], '-ok',color='blue',label = "Actual Depth",markersize=3) # Plot actual target (depth) values in y-axis for every particular group number against the distance values in x-axis, with a markersize (point sizes in plots) of 3. -ok means show connection between points as continous lines
                plt.plot( vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new'] , vali_preds[vali_preds['Groups_generated']==str(group_numb)][colname], '-ok',color='orange',label = "Predicted Depths",markersize=3) # Plot predicted target (depth) values in y-axis for every particular group number against the distance values in x-axis, with a markersize (point sizes in plots) of 3. -ok means show connection between points as continous lines
                plt.xlabel('Distance')  # Add label to x axis
                plt.ylabel('Depth')  # Add label to y axis
                plt.legend() # Add legend to plot
                plt.title( tech + " - " + str( group_numb)) # Add title to plot - title is just the curve number and the current group number value 
                plt.savefig("model_files/All_validation_plots/plots_" + str(group_numb)+".pdf",dpi=100) # Save the plot fig for every group and every curve as a pdf. Look more on what dpi here means
                pdfs.append("model_files/All_validation_plots/plots_" + str(group_numb) + ".pdf") # Append the pdf file name that was saved in the above line, append this name to 'pdfs' list
            
            print('merging')
            # Now we have to merge all the separate pdfs into a single pdf file
            merger = PdfFileMerger() # This library will allow us to merge multiple pdfs 
            for pdf in pdfs: # for all the pdf file names we stored, append to a pdffilemerger object called 'merger'
                merger.append(pdf) # append to pdffilemerger object

            merger.write( "model_files/All_validation_plots/" + tech + " grouped validation plots.pdf") # For all file names in the merger object, write all the contents in every pdf to a single new pdf file, this would happen for every curve - so in total there are 5 pdfs created for each curve equation and in those 5 pdfs there are validation plots for all groups 
            merger.close() # Close this object
            merger= None

            # After merging, we can remove the separate (single) pdfs
            for numb in range(1, max_grp+1):
                group_numb = "Group_"+str(numb) # For each group number
                os.remove('model_files/All_validation_plots/plots_' + str(group_numb) + ".pdf") # Remove that group's plots


        print('done')
        plt = None
        import glob
        for i in glob.glob('model_files/All_validation_plots/*'): # Now to copy all the 5 pdf files to the user's location, we use glob to find all files in a directory (folder)
            print(i)
            shutil.copy(i,file_path) # Copy the separate pdf files to the user's selected location
            os.remove(i) # Remove the files from local path after copying to user's location
        shutil.rmtree('model_files/All_validation_plots/') # Remove the whole folder
        # for grp in max_grp:

        # write group number on top of pdf as heading and file name - save in new folder - or keep group number as title
        # New pdf for each group
        # use pypdf2 to merge all pdfs

        return jsonify('Done') 






# Store all validation plots to local folder and export
@app.route('/export_validation_plots', methods=['POST','GET'])
def export_validation_plots():
    ml_algo_obj=request.form['ml_algo_name'] # Get the selected ml algo name as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string
    
    vali_mode_obj=request.form['vali_mode'] # Get the validation mode as json object
    vali_mode=json.loads(vali_mode_obj) # Convert the json object to string

    file_path_obj= request.form['file_path'] # Get a json object of the file path - of the user's selected location
    file_path=json.loads(file_path_obj) # Convert the json object to string

    send_feats_obj=request.form['send_feats'] # Get a json object of all the feature names selected by the user during data input phase
    send_feats=json.loads(send_feats_obj) # Convert the json object to string
    
    print(vali_mode)
    if(vali_mode=="auto_select_vali"): # If the validation mode is auto_select_vali (grouped validation), then do this. For other validation modes, this function is not accessible
        if(algo_choice=="Technique 1"): # If the selected algorithm is technique 1 
            vali_preds=load_data('Technique 1_vali_pred.csv') # Then import technique 1's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
            colname="Technique1_vali_pred" # Define a column name to search from the dataset, for each curve header
        if(algo_choice=="Technique 2"): # If the selected algorithm is technique 2
            vali_preds=load_data('Technique 2_vali_pred.csv') # Then import technique 2's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models
            colname="Technique2_vali_pred" # Define a column name to search from the dataset, for each curve header
        if(algo_choice=="Technique 3"): # If the selected algorithm is technique 3
            vali_preds=load_data('Technique 3_vali_pred.csv') # Then import technique 3's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models
            colname="Technique3_vali_pred" # Define a column name to search from the dataset, for each curve header
        if(algo_choice=="Technique 4"): # If the selected algorithm is technique 4
            vali_preds=load_data('Technique 4_vali_pred.csv') # Then import technique 4's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models
            colname="Technique4_vali_pred" # Define a column name to search from the dataset, for each curve header
        if(algo_choice=="Technique 5"): # If the selected algorithm is technique 5
            vali_preds=load_data('Technique 5_vali_pred.csv') # Then import technique 5's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models
            colname="Technique5_vali_pred" # Define a column name to search from the dataset, for each curve header
        if(algo_choice=="Try all Techniques"): # If the selected algorithm is 'all_techniques'
            vali_preds=load_data('all_models_vali_pred.csv') # Load the csv containing validation predictions of all 5 models, which also has the actual target values, we saved this file earlier - after curve-fitting 

        if(os.path.exists('model_files/All_validation_plots')): # If this file 'All_validation_plots' already exists in local path
            shutil.rmtree('model_files/All_validation_plots', ignore_errors=True) # If it's there, then remove the folder
        os.mkdir('model_files/All_validation_plots') # Create a new directory/folder called 'All_validation_plots'
        
        # Get max group count and save plots for all groups
        max_grp = len(vali_preds['Groups_generated'].unique())
        print(max_grp)

        #### FOR SINGLE MODELS
        
        import matplotlib
        matplotlib.use('agg') # This is very important line - avoids some issues with pytinkter (which matplotlib uses to show its graph windows) and builder files
        import matplotlib.pyplot as plt
        from PyPDF2 import PdfFileMerger
        # If you have issues with numpy and pypdf2 - check this out for a replace sentence https://github.com/mstamy2/PyPDF2/issues/67
        plt.style.use('seaborn-whitegrid')
        # plt.figure(figsize=(100,100))
        pdfs = []

        print(vali_preds.head())
        if(algo_choice!="Try all Techniques"): # If the selected ml algorithm is not 'all_techniques' (Try all Techniques)
            for numb in range(1, max_grp+1): # For all groups from 1 to the maximum group number
                plt.figure(figsize=(15,5)) # Set a constant figure size to correctly fit into a pdf file
                group_numb = "Group_"+str(numb) # Get the group number as string - in format 'Group_1' or 'Group_10' etc
                print(group_numb)
                plt.plot( vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new'] , vali_preds[vali_preds['Groups_generated']==str(group_numb)][send_feats[-1]], '-ok',color='blue',label = "Actual Depth",markersize=3) # Plot actual target (depth) values in y-axis for every particular group number against the distance values in x-axis, with a markersize (point sizes in plots) of 3. -ok means show connection between points as continous lines
                plt.plot( vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new'] , vali_preds[vali_preds['Groups_generated']==str(group_numb)][colname], '-ok',color='orange',label = "Predicted Depths",markersize=3) # Plot predicted target (depth) values in y-axis for every particular group number against the distance values in x-axis, with a markersize (point sizes in plots) of 3. -ok means show connection between points as continous lines
                plt.xlabel('Distance') # Add label to x axis
                plt.ylabel('Depth')  # Add label to y axis
                plt.legend() # Add legend to plot
                plt.title( algo_choice + " - " + str( group_numb)) # Add title to plot - title is just the algorithm name and the current group number value 
                plt.savefig("model_files/All_validation_plots/plots_"+str(group_numb)+".pdf",dpi=100) # Save the plot fig for every group and every ml algorithm as a pdf. Look more on what dpi here means
                pdfs.append("model_files/All_validation_plots/plots_" + str(group_numb) + ".pdf") # Append the pdf file name that was saved in the above line, append this name to 'pdfs' list
            
            print('merging')
            # Now we have to merge all the separate pdfs into a single pdf file
            merger = PdfFileMerger() # This library will allow us to merge multiple pdfs 
            for pdf in pdfs: # for all the pdf file names we stored, append to a pdffilemerger object called 'merger'
                merger.append(pdf) # append to pdffilemerger object

            merger.write( "model_files/All_validation_plots/" + algo_choice + " grouped validation plots.pdf") # For all file names in the merger object, write all the contents in every pdf to a single new pdf file, this would happen for every ml algorithm - so in total there are 5 pdfs created for each ml algo and in those 5 pdfs there are validation plots for all groups 
            merger.close() # Close this object
            merger= None

            # After merging, we can remove the separate (single) pdfs
            for numb in range(1, max_grp+1): # For all groups from 1 to the maximum group number
                group_numb = "Group_"+str(numb) # For each group number
                os.remove('model_files/All_validation_plots/plots_' + str(group_numb) + ".pdf") # Remove that group's plots
        
        # For all models
        if(algo_choice == "Try all Techniques"): # If the selected ml algorithm is 'all_techniques' (Try all Techniques)
            techs=['Technique 1','Technique 2','Technique 3','Technique 4','Technique 5'] # Save all technique name headers to a list
            for tech in techs: # For each technique
                pdfs = [] # Initialize this new list
                plt.figure(figsize=(15,5)) # Set a constant figure size to correctly fit into a pdf file
                if(tech=="Technique 1"): # If the current loop's algorithm is technique 1 
                    colname="Technique1_vali_pred" # Define a column name to search from the dataset, for each curve header
                if(tech=="Technique 2"): # If the current loop's algorithm is technique 2 
                    colname="Technique2_vali_pred" # Define a column name to search from the dataset, for each curve header
                if(tech=="Technique 3"): # If the current loop's algorithm is technique 3 
                    colname="Technique3_vali_pred" # Define a column name to search from the dataset, for each curve header
                if(tech=="Technique 4"): # If the current loop's algorithm is technique 4  
                    colname="Technique4_vali_pred" # Define a column name to search from the dataset, for each curve header
                if(tech=="Technique 5"): # If the current loop's algorithm is technique 5 
                    colname="Technique5_vali_pred" # Define a column name to search from the dataset, for each curve header

                for numb in range(1, max_grp+1): # For all groups from 1 to the maximum group number
                    plt.figure(figsize=(15,5)) # Set a constant figure size to correctly fit into a pdf file
                    group_numb = "Group_"+str(numb) # Get the group number as string - in format 'Group_1' or 'Group_10' etc
                    print(group_numb)
                    plt.plot( vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new'] , vali_preds[vali_preds['Groups_generated']==str(group_numb)][send_feats[-1]], '-ok',color='blue',label = "Actual Depth",markersize=3) # Plot actual target (depth) values in y-axis for every particular group number against the distance values in x-axis, with a markersize (point sizes in plots) of 3. -ok means show connection between points as continous lines
                    plt.plot( vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new'] , vali_preds[vali_preds['Groups_generated']==str(group_numb)][colname], '-ok',color='orange',label = "Predicted Depths",markersize=3) # Plot predicted target (depth) values in y-axis for every particular group number against the distance values in x-axis, with a markersize (point sizes in plots) of 3. -ok means show connection between points as continous lines
                    plt.xlabel('Distance')  # Add label to x axis
                    plt.ylabel('Depth')  # Add label to y axis
                    plt.legend() # Add legend to plot
                    plt.title( tech + " - " + str( group_numb)) # Add title to plot - title is just the curve number and the current group number value 
                    plt.savefig("model_files/All_validation_plots/plots_" + str(group_numb)+".pdf",dpi=100) # Save the plot fig for every group and every curve as a pdf. Look more on what dpi here means
                    pdfs.append("model_files/All_validation_plots/plots_" + str(group_numb) + ".pdf") # Append the pdf file name that was saved in the above line, append this name to 'pdfs' list
                
                print('merging')
                # Now we have to merge all the separate pdfs into a single pdf file
                merger = PdfFileMerger() # This library will allow us to merge multiple pdfs 
                for pdf in pdfs: # for all the pdf file names we stored, append to a pdffilemerger object called 'merger'
                    merger.append(pdf) # append to pdffilemerger object

                merger.write( "model_files/All_validation_plots/" + tech + " grouped validation plots.pdf") # For all file names in the merger object, write all the contents in every pdf to a single new pdf file, this would happen for every ml algorithm - so in total there are 5 pdfs created for each ml algo and in those 5 pdfs there are validation plots for all groups 
                merger.close() # Close this object
                merger= None

                # After merging, we can remove the separate (single) pdfs
                for numb in range(1, max_grp+1):
                    group_numb = "Group_"+str(numb) # For each group number
                    os.remove('model_files/All_validation_plots/plots_' + str(group_numb) + ".pdf") # Remove that group's plots


        print('done')
        plt = None
        import glob
        for i in glob.glob('model_files/All_validation_plots/*'): # Now to copy all the 5 pdf files to the user's location, we use glob to find all files in a directory (folder)
            print(i)
            shutil.copy(i,file_path) # Copy the separate pdf files to the user's selected location
            os.remove(i) # Remove the files from local path after copying to user's location
        shutil.rmtree('model_files/All_validation_plots/') # Remove the whole folder
        # for grp in max_grp:

        # write group number on top of pdf as heading and file name - save in new folder - or keep group number as title
        # New pdf for each group
        # use pypdf2 to merge all pdfs

        return jsonify('Done') 






# Graphs for single models ( not all techniques )
@app.route('/graphs_predictions_single_model', methods=['POST','GET'])
def graphs_predictions_sm():
    show_graphs_check_obj=request.form['show_graphs_check'] # Get json object of show_graphs_check - the values can be yes or no
    show_graphs_check=json.loads(show_graphs_check_obj) # Convert the json object to string value
    ml_algo_obj=request.form['ml_algo_name'] # Get the selected ml algo name as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string value

    vali_mode_obj=request.form['vali_mode'] # Get the validation mode json object
    vali_mode=json.loads(vali_mode_obj) # Convert the json object to string value

    preds=pd.DataFrame() # Initialize empty dataframe
    data=load_data('temp_pipeline_2.csv') # Load the processed input data - we get the distance values for every graph from this dataset
    if(show_graphs_check=="Yes"): # If show_graphs_check is yes then compute graph values and send back
        if(algo_choice=="Technique 1"): # If the selected ml algorithm is technique 1 
            preds=load_data('Technique 1_pred_temp.csv') # Then import technique 1's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
        if(algo_choice=="Technique 2"): # If the selected ml algorithm is technique 2
            preds=load_data('Technique 2_pred_temp.csv') # Then import technique 2's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
        if(algo_choice=="Technique 3"): # If the selected ml algorithm is technique 3
            preds=load_data('Technique 3_pred_temp.csv') # Then import technique 3's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
        if(algo_choice=="Technique 4"): # If the selected ml algorithm is technique 4
            preds=load_data('Technique 4_pred_temp.csv') # Then import technique 4's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
        if(algo_choice=="Technique 5"): # If the selected ml algorithm is technique 5
            preds=load_data('Technique 5_pred_temp.csv')   # Then import technique 5's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models               

        print(preds.head())
        pre=preds.copy() # Make a copy of this dataframe - to not lose original if any changes are made
        col0=pre.columns.tolist()[-2] # Get the ID (first) column name
        pre=pre.sort_values(by=col0) # Sort the dataframe by the ID column
        pre=pre.reset_index() # Reset the index just to be sure all the indices are in order
        pre.drop(['index'],axis=1,inplace=True) # When you reset index, an extra column called 'index' is created, so remove that
        #print(pre)
        if((vali_mode=="random_vali") | (vali_mode=="no_vali")): # When the vali mode is random vali or no validation at all, return the below values
            return jsonify({'preds': list(preds[preds.columns[-1]]), 'actual':list(preds[preds.columns[-2]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_y1':list(pre[pre.columns[-2]]), 'sorted_y2':list(pre[pre.columns[-1]]) })
        
        if(vali_mode=="auto_select_vali"): # When the vali mode is auto grouping vali, return the below values
            if(algo_choice=="Technique 1"): # If the selected ml algorithm is technique 1 
                vali_preds=load_data('Technique 1_vali_pred.csv') # Then import technique 1's validation predictions dataframe, we saved this file earlier - after training the models 
                
            if(algo_choice=="Technique 2"): # If the selected ml algorithm is technique 2
                vali_preds=load_data('Technique 2_vali_pred.csv') # Then import technique 2's validation predictions dataframe, we saved this file earlier - after training the models 
                
            if(algo_choice=="Technique 3"): # If the selected ml algorithm is technique 3
                vali_preds=load_data('Technique 3_vali_pred.csv') # Then import technique 3's validation predictions dataframe, we saved this file earlier - after training the models 
                
            if(algo_choice=="Technique 4"): # If the selected ml algorithm is technique 4
                vali_preds=load_data('Technique 4_vali_pred.csv') # Then import technique 4's validation predictions dataframe, we saved this file earlier - after training the models 
                
            if(algo_choice=="Technique 5"): # If the selected ml algorithm is technique 5
                vali_preds=load_data('Technique 5_vali_pred.csv')     # Then import technique 5's validation predictions dataframe, we saved this file earlier - after training the models 
                

            # get the total unqiue group count and by default show group 1
            group_count=int(len(vali_preds['Groups_generated'].unique()))
            #'train_preds': list(train_preds[train_preds.columns[-1]]), 'train_actual':list(train_preds[train_preds.columns[-2]]), 'train_distance':list(train_preds['Distance_new']),
            return jsonify({ 'preds': list(preds[preds.columns[-1]]), 'actual':list(preds[preds.columns[-2]]), 'distance':list(data['Distance_new']), 'sorted_y1':list(pre[pre.columns[-2]]), 'sorted_y2':list(pre[pre.columns[-1]]), 'group_count': group_count, 'vali_x':list(vali_preds[vali_preds['Groups_generated']=='Group_1']['Distance_new']), 'vali_y1':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[0]]), 'vali_y2':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-1]]), 'vali_x_all':list(vali_preds['Distance_new']), 'vali_y1_all':list(vali_preds[vali_preds.columns[0]]), 'vali_y2_all':list(vali_preds[vali_preds.columns[-1]]), 'x_all':list(data[data.columns[1]]), 'y_all':list(data[data.columns[2]]), 'x_vali':list(vali_preds[vali_preds.columns[2]]), 'y_vali': list(vali_preds[vali_preds.columns[3]]) }) # We return values which are needed to plot graphs - most of the values are pretty straightforward. For some of these values, the reason we took only group 1's values is when the page or plot is refreshed - the first thing to load should be the group 1, this would allow the user to then change to any group number they want to view
             
        if(vali_mode=="upload_vali_csv"): # When the vali mode is upload external csv vali, return the below values    
            if(algo_choice=="Technique 1"): # If the selected ml algorithm is technique 1 
                vali_preds=load_data('Technique 1_vali_pred.csv') # Then import technique 1's validation predictions dataframe, we saved this file earlier - after training the models 
            if(algo_choice=="Technique 2"): # If the selected ml algorithm is technique 2
                vali_preds=load_data('Technique 2_vali_pred.csv') # Then import technique 2's validation predictions dataframe, we saved this file earlier - after training the models 
            if(algo_choice=="Technique 3"): # If the selected ml algorithm is technique 3
                vali_preds=load_data('Technique 3_vali_pred.csv') # Then import technique 3's validation predictions dataframe, we saved this file earlier - after training the models 
            if(algo_choice=="Technique 4"): # If the selected ml algorithm is technique 4
                vali_preds=load_data('Technique 4_vali_pred.csv') # Then import technique 3's validation predictions dataframe, we saved this file earlier - after training the models 
            if(algo_choice=="Technique 5"): # If the selected ml algorithm is technique 5
                vali_preds=load_data('Technique 5_vali_pred.csv')     # Then import technique 5's validation predictions dataframe, we saved this file earlier - after training the models 

            return jsonify({'preds': list(preds[preds.columns[-1]]), 'actual':list(preds[preds.columns[-2]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_y1':list(pre[pre.columns[-2]]), 'sorted_y2':list(pre[pre.columns[-1]]), 'vali_x':list(vali_preds['Distance_new']), 'vali_y1':list(vali_preds[vali_preds.columns[0]]), 'vali_y2':list(vali_preds[vali_preds.columns[-1]]) })
            

@app.route('/provide_group_values', methods=['POST','GET'])
def provide_group_values(): # This function returns appropriate data values back to javascript, every time the user changes the group number while viewing the graphs   - THIS IS ONLY FOR SINGLE MODELS
    ml_algo_obj=request.form['ml_algo_name'] # Get the selected ml algo name as json object
    algo_choice=json.loads(ml_algo_obj) # Convert the json object to string value

    group_numb_obj=request.form['group_numb'] # Get the user's selected group number as a json object
    group_numb=json.loads(group_numb_obj) # Get the selected group number by converting json object to string
    
    if(algo_choice=="Technique 1"): # If the selected ml algorithm is technique 1 
        vali_preds=load_data('Technique 1_vali_pred.csv') # Then import technique 1's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
    if(algo_choice=="Technique 2"): # If the selected ml algorithm is technique 2 
        vali_preds=load_data('Technique 2_vali_pred.csv') # Then import technique 2's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
    if(algo_choice=="Technique 3"): # If the selected ml algorithm is technique 3 
        vali_preds=load_data('Technique 3_vali_pred.csv') # Then import technique 3's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
    if(algo_choice=="Technique 4"): # If the selected ml algorithm is technique 4 
        vali_preds=load_data('Technique 4_vali_pred.csv') # Then import technique 4's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 
    if(algo_choice=="Technique 5"): # If the selected ml algorithm is technique 5 
        vali_preds=load_data('Technique 5_vali_pred.csv') # Then import technique 5's validation predictions dataframe which also has actual target values, we saved this file earlier - after training the models 

    return jsonify({ 'vali_x':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new']), 'vali_y1':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[0]]), 'vali_y2':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-1]]) }) 


@app.route('/provide_group_values_all', methods=['POST','GET'])
def provide_group_values_all(): # This function is to send back values for every group change in graph - THIS IS ONLY WHEN THE USER HAD SELECTED 'TRY ALL TECHNIQUES' IN THE ALGORITHM SELECTION
    group_numb_obj=request.form['group_numb'] # Get the json object of the group number
    group_numb=json.loads(group_numb_obj) # Get the selected group number by converting json object to string
    
    vali_preds=load_data('all_models_vali_pred.csv') # Load the csv containing validation predictions of all 5 models, which also has the actual target values, we saved this file earlier - after training the models  
    time.sleep(0.1)    
    return jsonify({ 'vali_x':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)]['Distance_new']), 'vali_y1':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[0]]), 'vali_lr':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-5]]), 'vali_rf':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-4]]), 'vali_xgb':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-3]]), 'vali_nn':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-2]]), 'vali_svr':list(vali_preds[vali_preds['Groups_generated']==str(group_numb)][vali_preds.columns[-1]]) })  # We return values which are needed to plot graphs - most of the values are pretty straightforward. For some of these values, the reason we took only group 1's values is when the page or plot is refreshed - the first thing to load should be the group 1, this would allow the user to then change to any group number they want to view


# Graphs for ALL models ( ALL techniques )
@app.route('/graphs_predictions_all_models', methods=['POST','GET'])
def graphs_predictions_am():
    show_graphs_check_obj=request.form['show_graphs_check'] # Get json object of show_graphs_check - the values can be yes or no
    show_graphs_check=json.loads(show_graphs_check_obj) # Convert the json object to string value
    vali_mode_obj=request.form['vali_mode'] # Get the validation mode json object
    vali_mode=json.loads(vali_mode_obj) # Convert the json object to string value


    if(show_graphs_check=="Yes"): # If show_graphs_check is yes then compute graph values and send back
        preds=load_data('all_models_preds.csv') # Load the csv containing predictions on full-input data of all 5 models, which also has the actual target values, we saved this file earlier - after training the models  
        data=load_data('temp_pipeline_2.csv') # Load the processed input data - we get the distance values for every graph from this dataset
        print(preds.head())
        pre=preds.copy() # Make a copy of this dataframe - to not lose original if any changes are made
        col0=pre.columns.tolist()[0] # Get the ID (first) column name
        pre=pre.sort_values(by=col0) # Sort the dataframe by the ID column
        pre=pre.reset_index() # Reset the index just to be sure all the indices are in order
        pre.drop(['index'],axis=1,inplace=True) # When you reset index, an extra column called 'index' is created, so remove that
       # print(pre)
       # print(list(preds['NN_full_Pred'].head()))
        #print(list(preds['RF_full_Pred'].head()))
        
        if((vali_mode=="random_vali") | (vali_mode=="no_vali")): # When the vali mode is random vali or no validation at all, return the below values
            return jsonify({'lr_preds': list(preds['Technique1_full_Pred']), 'rf_preds':list(preds['Technique2_full_Pred']), 'xgb_preds':list(preds['Technique3_full_Pred']), 'nn_preds':list(preds['Technique4_full_Pred']), 'svr_preds':list(preds['Technique5_full_Pred']),'actual':list(preds[preds.columns[0]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_lr_preds': list(pre['Technique1_full_Pred']), 'sorted_rf_preds':list(pre['Technique2_full_Pred']), 'sorted_xgb_preds':list(pre['Technique3_full_Pred']), 'sorted_nn_preds':list(pre['Technique4_full_Pred']), 'sorted_svr_preds':list(pre['Technique5_full_Pred']),'sorted_y1':list(pre[pre.columns[0]]) })
        
        if(vali_mode=="auto_select_vali"): # When the vali mode is auto grouping vali, return the below values
            vali_preds=load_data('all_models_vali_pred.csv') # Load the csv containing validation predictions of all 5 models, which also has the actual target values, we saved this file earlier - after training the models  
            group_count=int(len(vali_preds['Groups_generated'].unique())) # get the total unqiue group count and by default show group 1
            return jsonify({'lr_preds': list(preds['Technique1_full_Pred']), 'rf_preds':list(preds['Technique2_full_Pred']), 'xgb_preds':list(preds['Technique3_full_Pred']), 'nn_preds':list(preds['Technique4_full_Pred']), 'svr_preds':list(preds['Technique5_full_Pred']),'actual':list(preds[preds.columns[0]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_lr_preds': list(pre['Technique1_full_Pred']), 'sorted_rf_preds':list(pre['Technique2_full_Pred']), 'sorted_xgb_preds':list(pre['Technique3_full_Pred']), 'sorted_nn_preds':list(pre['Technique4_full_Pred']), 'sorted_svr_preds':list(pre['Technique5_full_Pred']),'sorted_y1':list(pre[pre.columns[0]]), 'group_count': group_count, 'vali_x':list(vali_preds[vali_preds['Groups_generated']=='Group_1']['Distance_new']), 'vali_y1':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[0]]), 'vali_lr':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-5]]), 'vali_rf':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-4]]), 'vali_xgb':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-3]]), 'vali_nn':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-2]]), 'vali_svr':list(vali_preds[vali_preds['Groups_generated']=='Group_1'][vali_preds.columns[-1]]), 'x_all':list(data[data.columns[1]]), 'y_all':list(data[data.columns[2]]), 'x_vali':list(vali_preds[vali_preds.columns[2]]), 'y_vali': list(vali_preds[vali_preds.columns[3]])  }) # We return values which are needed to plot graphs - most of the values are pretty straightforward. For some of these values, the reason we took only group 1's values is when the page or plot is refreshed - the first thing to load should be the group 1, this would allow the user to then change to any group number they want to view
        
        if(vali_mode=="upload_vali_csv"): # When the vali mode is upload external csv vali, return the below values    
            vali_preds=load_data('all_models_vali_pred.csv') # Load the csv containing validation predictions of all 5 models, which also has the actual target values, we saved this file earlier - after training the models  
            return jsonify({'lr_preds': list(preds['Technique1_full_Pred']), 'rf_preds':list(preds['Technique2_full_Pred']), 'xgb_preds':list(preds['Technique3_full_Pred']), 'nn_preds':list(preds['Technique4_full_Pred']), 'svr_preds':list(preds['Technique5_full_Pred']),'actual':list(preds[preds.columns[0]]), 'distance':list(data['Distance_new']), 'sorted_x':list(pre.index), 'sorted_lr_preds': list(pre['Technique1_full_Pred']), 'sorted_rf_preds':list(pre['Technique2_full_Pred']), 'sorted_xgb_preds':list(pre['Technique3_full_Pred']), 'sorted_nn_preds':list(pre['Technique4_full_Pred']), 'sorted_svr_preds':list(pre['Technique5_full_Pred']),'sorted_y1':list(pre[pre.columns[0]]), 'vali_x':list(vali_preds['Distance_new']), 'vali_y1':list(vali_preds[vali_preds.columns[0]]), 'vali_lr':list(vali_preds[vali_preds.columns[-5]]), 'vali_rf':list(vali_preds[vali_preds.columns[-4]]), 'vali_xgb':list(vali_preds[vali_preds.columns[-3]]), 'vali_nn':list(vali_preds[vali_preds.columns[-2]]), 'vali_svr':list(vali_preds[vali_preds.columns[-1]]) })
        

#surface plot
@app.route('/surface_plot_data', methods=['POST','GET'])
def surface_plot_data(): ## NON-FUNCTIONAL
    show_surface_plot_check_obj=request.form['show_surface_plot_check']
    show_surface_plot_check=json.loads(show_surface_plot_check_obj)
    sel_feats=request.form['feats']
    feats  = json.loads(sel_feats)
    ml_algo_obj=request.form['ml_algo_name']
    algo_choice=json.loads(ml_algo_obj)
    print(feats)
    east=feats[1]
    north=feats[2]
    if(show_surface_plot_check=="Yes"):
        data=load_data('temp_pipeline_2.csv')
        if(algo_choice=="Try all Techniques"):
            preds=load_data('all_models_preds.csv')
        elif(algo_choice=="Technique 1"):
            preds=load_data('Technique 1_pred_temp.csv')
        elif(algo_choice=="Technique 2"):
            preds=load_data('Technique 1_pred_temp.csv')
        elif(algo_choice=="Technique 3"):
            preds=load_data('Technique 1_pred_temp.csv')
        elif(algo_choice=="Technique 4"):
            preds=load_data('Technique 1_pred_temp.csv')
        elif(algo_choice=="Technique 5"):
            preds=load_data('Technique 1_pred_temp.csv')
              
        x=list(data[east].iloc[:500].values)
        y=list(data[north].iloc[:500].values)
        z=[list(data[preds.columns[0]].iloc[:500].values)]*len(data[east].iloc[:500])
        print(len(x))
        #df3 = {'x':[1, 2, 3, 4, 5],'y':[10, 20, 30, 40, 50],'z': [[5, 4, 3, 2, 1]]*5}
        #z=np.array(data['Depth'].head()).reshape((len(data['Y_NZTM'].head()), len(data['X_NZTM'].head()) ))
        #return jsonify({'x': list(data['X_NZTM'].head()), 'y':list(data['Y_NZTM'].head()), 'z':list(z) })
        return jsonify({'x': x, 'y': y, 'z':z})




#### SECTION 11: PREDICTION ON FULL-DATA, FUNCTIONS FOR MACHINE-LEARNING MODULE

## BELOW FUNCTION IS NON-FUNTIONAL - WE ENTIRELY MOVE AWAY FROM THE METHOD OF COPYING FULL-DATA FILES WHICH ARE USUALLY LARGE IN SIZE, INSTEAD OPT TO A REMOTE ACCESS BASED METHOD        
@app.route('/upload_fulldata_file',methods=['POST','GET']) ## NON-FUNCTIONAL
def upload_fulldata_file(): # Upload the input full-data csv from user, and copy to local file path - NEED TO MAKE PROCESSING CHANGES TO DATASET, SO DO NOT JUST EDIT THE FILE FROM THE ORIGINAL LOCATION OF THE FILE
    print("in file present")
    if( request.method=="POST" ): # If the request is POST
        if( request.files ): # If there are input files from the user
            file=request.files["file"] # request variable is the flask variable which contains all the information sent from javascript side to python for processing, we access values by request.files[ name_of_variable_given_in_javascript_side ]
            print(file)
            file.save(os.path.join(app.config['FILE_SAVE'], file.filename)) # We have already set app config to model_files. This will allow the flask app to store input data files directly into the model_files path. In this line we do file.save to save files with names file.filename into the recently set app.config value - which is model_files
            print(" file saved ")
            #f"{file.filename}
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'],"full_river_data_software_generated.csv"))): # Rename the input data to full_river_data_software_generated.csv. Note that the original location of the file is unchanged - meaning the original file untouched, but a copy of the file is stored in model_files folder and that file is renamed to full_river_data_software_generated.csv which will be used almost everywhere in the code
                os.remove( os.path.join(app.config['FILE_SAVE'],"full_river_data_software_generated.csv")) # If another file named full_river_data_software_generated.csv already exists, then delete that and rename the new file to full_river_data_software_generated.csv
            os.rename( os.path.join(app.config['FILE_SAVE'], file.filename), os.path.join(app.config['FILE_SAVE'],"full_river_data_software_generated.csv") ) # rename the new file to full_river_data_software_generated.csv
            if(os.path.exists( os.path.join(app.config['FILE_SAVE'], file.filename)) ): # Remove the initial copy of the user's copied file - we only rename it, so this line won't probably run, but still if multiple copies of same file are created somehow, then delete them
                os.remove( os.path.join(app.config['FILE_SAVE'], file.filename) ) # Remove the duplicate files
            res = make_response(jsonify({"message":  f"File uploaded successfully "}),200) # Send back response to user saying files are uploaded

            return res
        return render_template('simplified_workflow.html/upload_fulldata_file.html')



# full river input and process
@app.route('/full_river_input',methods=['POST'])
def full_river_input():
    print("Merging")
    # cluster_count_obj=request.form['cluster_count']
    # cluster_count=json.loads(cluster_count_obj)
    # full_river=pd.DataFrame()
    # copy_done=0
    # downloads_path=get_download_path()
    # if(str(cluster_count) != "-9"):
    #     for i in range(1,int(cluster_count)+1):           
    #         file_path=str(downloads_path)+ '/full_river_data_from_bctk_software' + str(i) + '.csv'
    #         df=pd.read_csv(file_path)
    #         full_river=pd.concat([full_river,df],axis=0)
    #         full_river=full_river.reset_index().drop(['index'],axis=1)
    #     copy_done=1

    # elif(str(cluster_count)=="-9"): 
    #     print("-9")
    #     file_s=''
    #     downloads_path=''
    #     downloads_path=get_download_path()
    #     file_s=str(downloads_path)+ '/full_river_data_from_bctk_software.csv'
    #     full_river=pd.read_csv(file_s)
    #     copy_done=1
    #     print("copy done")
            
    # print("Full river sample" , full_river.head())
    # print(full_river.tail())
    # print(full_river.isnull().sum())
    # if(True in list(full_river.iloc[-1].isnull()[:])):
    #     drop_ind=full_river.index[-1]
    #     print(drop_ind)
    #     full_river.drop(drop_ind,axis=0,inplace=True)
    # full_columns=full_river.columns
    # print(list(full_columns))    
    # full_river.to_csv('model_files/'+'full_river_data_software_generated.csv',index=False)
 
    # if( (str(cluster_count) != "-9") & (copy_done==1) ):
    #     for i in range(1,int(cluster_count)+1):
    #         file_path=str(downloads_path)+ '/full_river_data_from_bctk_software' + str(i) + ".csv"
    #         if(os.path.exists(file_path)):
    #             os.remove(file_path)
    #             print(" Deleted ",cluster_count)

    # elif( (str(cluster_count) == "-9") & (copy_done==1) ):
    #     downloads_path=get_download_path()
    #     file_path=str(downloads_path)+ '/full_river_data_from_bctk_software.csv'
    #     if(os.path.exists(file_path)):
    #         os.remove(file_path)
    #         print(" Deleted single file ")
    
    full_files_name_obj=request.form['full_files_name']
    full_files_name=json.loads(full_files_name_obj) # get the file name - location of the full-data csv file. We don't copy the file to the local 'model_files' folder because full-data is usually very large and we do not have to consume extra space, we can just get the file location path, and access/run functions on the dataset without having to copy the csv

    df=pd.read_csv(full_files_name[0], nrows=10)  # read first 10 rows and send to javascript to view in app page
    full_columns=df.columns # Get all the column names of the dataframe
    df_jsonfiles = json.loads(df.head().to_json(orient='records')) # Always load as json and then jsonify the data (next two lines) before sending - only way to communicate between python and javascript
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))


@app.route('/full_river_predictions',methods=['POST','GET'])
def full_river_predictions(): # This function takes in the full-data csv, processes it with the same functions and feature engineering methods that were used on the training dataset. After processing, load the trained model and its parameters, make predictions and store the output csv to the user's selected location
    ml_algo_obj=request.form['full_ml_algo']
    ml_algo_name=json.loads(ml_algo_obj) # Get the user's selected ml algorithm (NOT THE MODEL SELECTED IN TRAINING PHASE - A NEW SELECTION JUST FOR FULL-DATA ) to apply on full-data
    model='' # Initialize an empty string

    full_files_name_obj=request.form['full_files_name']
    full_files_name=json.loads(full_files_name_obj) # Get the full-data csv file path
    
    sel_feats=request.form['feats']
    sel_cols3 = json.loads(sel_feats) # Columns selected for full-data

    actual_status=request.form['actual_status'] # If an actual target variable is present or not
    # actual_status=json.loads(actual_status_obj)
    
    target_json=request.form['actual_variable']
    target=json.loads(target_json) # If actual target variable is present, then which variable is it
    print(target)

    send_feats_train_obj=request.form['send_feats_train']
    send_feats_train=json.loads(send_feats_train_obj) # The feature names which were used while training - it is important to use these names and rename the same columns selected for full-data (which might have different names ex: in training it can be 'R' column while in full-data that column might have the name 'R_05m') so rename them to apply the feature processing methods
    new_cols=send_feats_train[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)

    process_options_obj=request.form['process_options'] # Get the json object of what process_option value is - As mentioned already, as of the current version, the only value process_option can possibly have is 'best_options' - which is to automatically select the best feature engineering methods combination
    process_options=json.loads(process_options_obj) # Convert the json object to string

    pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols3[1] # Get east coordinate column name
    north=sel_cols3[2] # Get north coordinate column name
    
    r=send_feats_train[3] # Get R column name
    g=send_feats_train[4] # Get G column name
    b=send_feats_train[5] # Get B column name
    
    if(actual_status=="Yes"): # Check if there is an actual target variable
        sel_tar=str(target) # Get the name of the target column
        print("sel tar done")
    if(actual_status=="No"): # Check if there is no actual target variable
        sel_tar="none"  # If no target variable, make this string to none

    colourspaces_json=request.form['colourspaces'] # Get what colourspace was used in training - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value

    print("Process options",process_options)
    print("Actual status",actual_status)
    print(" Model name",ml_algo_name)
    print("features",sel_cols3)

    full_data=pd.read_csv(full_files_name[0], chunksize=1000000)   # Read the full-data csv not as whole, but in chunks of size 10000000 each - batch processing by performing multiple iterations to complete all batches and then finally merge prediction values of all batches into one single csv and output to user

    if("best_options" in process_options): # When the value in process_options list is 'best_options'
        print("in best options")
        full_river=pd.DataFrame() # Initialize an empty dataframe
        # data_reference=load_data('data_for_all_models.csv')
        if(ml_algo_name=="Technique 1"): # If the selected algorithm to apply on full-data is technique 1
            model=joblib.load('model_files/'+'Technique 1.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 1" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 5"): # If the selected algorithm to apply on full-data is technique 5
            model=joblib.load('model_files/'+'Technique 5.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 5" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 2"): # If the selected algorithm to apply on full-data is technique 2
            model=joblib.load('model_files/'+'Technique 2.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 2" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 3"): # If the selected algorithm to apply on full-data is technique 3
            model=joblib.load('model_files/'+'Technique 3.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 3" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 4"): # If the selected algorithm to apply on full-data is technique 4
            model=joblib.load('model_files/'+'Technique 4.pkl')  # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 4" # Define a column name to add to the final dataframe
        z=0
        option_csv=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
        process_options=[] # Initialze an empty list
        for i in range(len(option_csv)): # For all the values in the option_csv dataset
            process_options.append(option_csv.iloc[i]['best_option']) # Append the values into the new list
        print(process_options)
        for jun in full_data: # For every batch ( chunk ) of the full-data, do all the below codes
            jun=jun.dropna() # Drop nan values if any
            processed_data=jun.copy() # Make a copy of the batch full-data
            z=z+1 # Keep track of the chunk or cluster or current batch number
            print("cluster ",z)
            print(processed_data.index[0],processed_data.index[-1])
            if(actual_status=="Yes"): # Check if there is an actual target variable
                predi=pd.DataFrame() # Initiate empty dataframe
                predi[sel_tar]=processed_data[sel_tar].copy() # If actual variable is present and selected by the user, then copy the target values and keep this as a separate dataframe and merge along with the predictions dataframe at the end
            elif(actual_status=="No"): # Check if there is no actual target variable
                predi=pd.DataFrame() # If no actual target variable, then just initiate empty dataframe

            to_merge=pd.DataFrame() # Empty dataframe to merge later
            merged=pd.DataFrame() # Empty dataframe to merge predictions of every batch processing iteration and the id, east and north of every batch-processing iteration - there is no batch processing of full-data in curve-fitting, but there is batch processing in simplified workflow since its computation load is huge
            to_merge=pd.DataFrame(processed_data[[pid,east,north]]) # Copy the id, east and north columns from the main dataframe to be processed, into the 'to_merge' dataframe - use this dataframe to merge with the  predictions dataframe at the end

            # other selected colourspaces add
            # check if 0 value is present in r,g,b
            r=sel_cols3[3] #3 because of east north corrd input. Get R column name 
            g=sel_cols3[4] # Get G column name
            b=sel_cols3[5] # Get B column name  

            new_sel_cols3 = sel_cols3.copy() # Keep a copy of the original sel_cols3 - which has the selected full-data features. The reason to keep a copy is in the next few steps - new features will be added to the list and hence we need to have the original features to extract initial columns from the next batch of the batch-processed full-data

            if(processed_data.describe()[r]['min']==0):     # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                
                processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
            if(processed_data.describe()[g]['min']==0):     # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                               
                processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
            if(processed_data.describe()[b]['min']==0):     # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                              
                processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

            if(processed_data.describe()[r]['min']<0):      # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                             
                processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
            if(processed_data.describe()[g]['min']<0):      # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                            
                processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
            if(processed_data.describe()[b]['min']<0):      # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                            
                processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

            if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
                print('same dataset - no hsv only rgb')

            elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
                print('same dataset - no hsv only rgb')

            elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
                print('change in dataset - no rgb only hsv')
                hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                processed_data[r] = processed_data2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data[g] = processed_data2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data[b] = processed_data2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
                for ix, valx in enumerate(new_sel_cols3): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
                    if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                        new_sel_cols3[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
                    if(valx == g):
                        new_sel_cols3[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
                    if(valx == b):
                        new_sel_cols3[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
                
                # check if 0 value is present in r,g,b
                if(processed_data.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                
                    processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                if(processed_data.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                              
                    processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                if(processed_data.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                             
                    processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                del processed_data2 # Delete the temporary dataframe

            elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
                print('change in dataset - rgb and hsv')
                hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                #insert in new_sel_cols3  - since we created new features, they should be added for further use in the next parts of the code
                new_sel_cols3.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                new_sel_cols3.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                new_sel_cols3.insert(8, 'V_generated')  # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

                # check if 0 value is present in r,g,b
                if(processed_data.describe()['H_generated']['min']==0):     # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                    
                    processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                if(processed_data.describe()['S_generated']['min']==0):     # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                  
                    processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                if(processed_data.describe()['V_generated']['min']==0):     # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                  
                    processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                del processed_data2 # Delete the temporary dataframe

            r=send_feats_train[3] # Get R column name - the R feature name in the INPUT TRAINING DATA
            g=send_feats_train[4] # Get G column name - the G feature name in the INPUT TRAINING DATA
            b=send_feats_train[5] # Get B column name - the B feature name in the INPUT TRAINING DATA

            sel_cols2=new_sel_cols3[3:]     # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
            processed_data=processed_data[sel_cols2].copy() # Create the main subset from the initial dataframe - this new dataframe would have only features from R,G,B, to other variables - NOT east, north, id etc, - note that there is no target variable here
            old_cols=new_sel_cols3[3:] # To rename the full-data column names to be same as the input training-data column names, get all the current full-data column names from r,g,b, till the last selected feature
            print(old_cols, new_cols)
            for j in range(len(old_cols)): # For every current full-data column name
                processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True) # Like mentioned before, need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
            

            print("start",processed_data.shape)
            if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
                processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                print("start1",processed_data.shape)

            if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
                processed_data=greyscale_gen_full(processed_data,r,g,b)
                print("start2",processed_data.shape)

            if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
                processed_data=cluster_data_full(processed_data,r,g,b)  
                print("start3",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=poly_creation_cluster_full(processed_data)    
                print("start4",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=poly_creation_no_cluster_full(processed_data)     
                print("start4",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=standard_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start5",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=standard_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques  
                print("start5",processed_data.shape)
            
            if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=min_max_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start6",processed_data.shape)

            if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=min_max_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques  

            if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=robust_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=robust_scale_no_cluster_full(processed_data,"common")     # Common specifies same data for all ml techniques

            if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=power_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=power_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques

            if("pearson_correl" in process_options): # This line checks if pearson_correl is in the process_options list 
                processed_data=correl_full(processed_data,"common") # Common specifies same data for all ml techniques
                print("start7",processed_data.shape)

            if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
                lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv') # Load this csv which was stored when the poly_lasso_trans function ran while finding the best preprocessing methods 
                processed_data=lasso_reg_full(processed_data,lasso_result) # We pass the csv to copy the column names present in the lasso_result dataset - lasso_result dataset was saved after computing lasso reg during feature engineering phase
                print("start8",processed_data.shape)

            if(("pca_trans" in process_options) & ("cluster_gen" in process_options)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=pca_reduction_cluster_full(processed_data,"common")     # Common specifies same data for all ml techniques

            if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=pca_reduction_no_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

            print(processed_data.tail())
            print("model",model,ml_algo_name)
            
            # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
            if(ml_algo_name=="Technique 1" or ml_algo_name=="Technique 5"): # If the selected algorithm is technique 1 or if the selected algorithm is technique 5
                option_csv_df=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
                options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                    no_scale_check_lr=1 # atleast one scaling
                if(no_scale_check_lr==0): # If none present, then do this
                    sc_t1_5=joblib.load('model_files/'+'sc_forT1&5only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                    processed_data=sc_t1_5.transform(processed_data) # Scale and transform the processed full-data
                    print('no scale')

            if(ml_algo_name=="Technique 4"): # If the selected algorithm is technique 4
                option_csv_df=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                no_scale_check=0 # Initialize a scaling check for nn (lr and svr already done)
                options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                    no_scale_check=1 # atleast one scaling
                if(no_scale_check==0): # If none present, then do this
                    sc_t4=joblib.load('model_files/'+'sc_forT4only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                    processed_data=sc_t4.transform(processed_data) # Scale and transform the processed full-data
                    print('no scale')
            predi[str(col_name)+'_Pred']=model.predict(processed_data) # Predict depth values for the processed batch-data and etore the prediction values to the dataframe under the column name 'ALGO_NAME_pred' - ALGO_NAME is the chosen ml algorithm name
            df_preds2=pd.DataFrame(data=predi[str(col_name)+'_Pred'].values,columns= [str(col_name)+'_Pred'], index=to_merge.index) # Copy the same dataframe to a new one - with index from this batch's input full-data dataset - copy the index from this dataframe to this new dataframe so that the values are in correct order while merging all batches at the end 
            if(actual_status=="Yes"): # Check if there is an actual target variable
                df_preds2[sel_tar]=predi[sel_tar]    # If actual variable is present and selected by the user, then copy the target values into this new dataframe
            
            merged=pd.concat([to_merge,df_preds2],axis=1) # Merge all columns and predicted values into one dataframe
            merged=merged.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
            full_river=pd.concat([full_river,merged],axis=0) # Merge the current batch values with the previous batch (if any) values - concats on row base, adds more rows after the previous batches' rows
            full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
            del merged # Delete the temporary dataframe
            del df_preds2 # Delete the temporary dataframe
            del predi # Delete the temporary dataframe

        full_river=full_river.sort_values(by=pid) # Sort the values of ID column
        full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the sorted dataframe and remove the index column
        print("Sample of the output")
        print(full_river.head())
        preds_file_name='Predictions_'+str(col_name).lower()+'.csv' # This is the output file name - 'predictions' + ml algo name, and save as csv file 
        full_river.to_csv('model_files/'+preds_file_name,index=False) # Save the final all merged output predictions csv file 
        full_river=full_river.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values in the stored csv file
        jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records')) # Save the first 5 rows of the input data with just selected independent features, as json object to show to the user using javascript  
        if(actual_status=='Yes'): # If actual target is present calculate the metrics between predicted and the actual target variables
            print(sel_tar)
            print(full_river.head())
            full_river_preds_metrics=calculate_metrics(ml_algo_name, full_river, sel_tar, str(col_name)) # Get the metric scores by passing in the prediction values and the actual target values
            full_river_preds_metrics_temp=calculate_metrics_temp(full_river, sel_tar, str(col_name)) # Get the metric scores by passing in the prediction values and the actual target values
            full_river_preds_metrics_temp.to_csv('model_files/'+'full_data_statistics.txt',index=False) # Save the final full-data metric scores as text file

            # for saving last used model name for highlighting
            model_textfile_name="FINAL_MODEL_USED.txt"
            if(os.path.exists('model_files/'+str(model_textfile_name))):
                os.remove('model_files/'+str(model_textfile_name))

            model_textfile=pd.DataFrame() # Initialize empty dataframe
            print(ml_algo_name)
            model_textfile['Model Name']=[str(ml_algo_name)] # Under the column 'Model_Name' save the ml algo name which was selected by the user to predict on full-data 
            model_textfile.to_csv('model_files/'+str(model_textfile_name),index=False)     # Save this text file to local path to use it later in the pre-trianed module
            
            # Save columns chosen for full river
            col_textfile_name="FINAL_COLS_USED.txt"
            if(os.path.exists('model_files/'+str(col_textfile_name))):
                os.remove('model_files/'+str(col_textfile_name))
            cols_textfile=pd.DataFrame(columns=['Columns']) # Initialize a new dataframe with column name 'Columns'
            ALL_COLS='' # Initialize empty string 
            for i in sel_cols3: # For every column name used while processing the full-data
               ALL_COLS+= str(i) + ', ' # Append each of those column names to a string - so each column value is separated by a ','
            cols_textfile.loc[0]=ALL_COLS # Add this string as the value of first row of the new dataframe 
            cols_textfile.to_csv('model_files/'+str(col_textfile_name),index=False)     # Save this text file to local path to use it later in the pre-trianed module

            final_preds_full_concat=pd.concat([full_river_preds_metrics,full_river_preds_metrics_temp],axis=0) # Sometimes json object has problems sending data with just one row, so we add another row with same values as the first row
            final_preds_full_concat=final_preds_full_concat.round(3) # Round the metrics to 3 decimals
            
            jsonfiles_full_river_preds_metrics = json.loads(final_preds_full_concat[full_river_preds_metrics.columns].to_json(orient='records')) # Now create and send a json object for the metric scores - since there is an extra row with same values as first row, just send them both - and in javascript we show only one row
            del full_river
            return (jsonify({'preds':jsonfiles_full_river_preds,'metrics':jsonfiles_full_river_preds_metrics}))    
        if(actual_status=="No"): # If actual target is not present then just save the model name and used column values
            model_textfile_name="FINAL_MODEL_USED.txt" # for saving last used model name for highlighting
            if(os.path.exists('model_files/'+str(model_textfile_name))):
                os.remove('model_files/'+str(model_textfile_name))

            model_textfile=pd.DataFrame() # Initialize empty dataframe
            print(ml_algo_name)
            model_textfile['Model Name']=[str(ml_algo_name)] # Under the column 'Model_Name' save the ml algo name which was selected by the user to predict on full-data 
            model_textfile.to_csv('model_files/'+str(model_textfile_name),index=False)     # Save this text file to local path to use it later in the pre-trained module

            # Save columns chosen for full river
            col_textfile_name="FINAL_COLS_USED.txt"
            if(os.path.exists('model_files/'+str(col_textfile_name))):
                os.remove('model_files/'+str(col_textfile_name))
            cols_textfile=pd.DataFrame(columns=['Columns']) # Initialize a new dataframe with column name 'Columns'
            ALL_COLS='' # Initialize empty string 
            print("selcols3",sel_cols3)
            for i in sel_cols3: # For every column name used while processing the full-data
               ALL_COLS+= str(i) + ', ' # Append each of those column names to a string - so each column value is separated by a ','
            cols_textfile.loc[0]=ALL_COLS # Add this string as the value of first row of the new dataframe 
            cols_textfile.to_csv('model_files/'+str(col_textfile_name),index=False)  # Save this text file to local path to use it later in the pre-trained module

            del full_river # Delete the temporary dataframe
            return (jsonify(jsonfiles_full_river_preds))

            
@app.route('/save_model_files', methods=['POST','GET'])
def save_model_files():
    model_loc_obj= request.form['model_save_loc']
    model_loc=json.loads(model_loc_obj)

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    algo_choice_obj= request.form['ml_algo_name']
    algo_choice=json.loads(algo_choice_obj)
    print(algo_choice)

    if os.path.exists(model_loc):
        shutil.rmtree(model_loc)
        
    # shutil.copytree('model_files',model_loc)

    file_path=file_path+model_loc
    shutil.copytree('model_files',file_path)
    
    return jsonify("Done")







########## FOR TIF BASED INPUT FILE

# full river input and process
@app.route('/full_tif_input',methods=['POST'])
def full_tif_input(): # This function is to get the input raster files and send json object with details about raster band names, number of bands etc
    print("tif files mode")
    
    full_files_name_obj=request.form['full_files_name'] # Json object containing the tif file path name - it is a list of names - the first value is the rgb raster, the subsequent values are extra raster files
    full_files_name=json.loads(full_files_name_obj) # Convert the json list object to list of values
    count_raster = 0 # Initialize counter for number of raster files
    tif_files = [] # Initialize empty list to store all tif files
    rasterfile_bands_dict = {} # Initialize dict to store tif files info

    if os.path.exists('tif_processing'): # If this folder already exists
        shutil.rmtree('tif_processing', ignore_errors=True) # Delete that folder
    if not os.path.exists('tif_processing'): # If the folder doesn't exist
        os.mkdir('tif_processing') # Then create it - this is the folder where all the tif files and its split parts will be stored

    # Here full_files_name[0] is the rgb raster and the rest are additional feature rasters
  
    for file in full_files_name: # For every file in this list
        file_ext=str(str(file).split('.')[-1])   # Get its extension
        if(file_ext=="tif"): # Check if the extension is tif, if yes then continue
            count_raster+=1  # Keeps count of number of tif files imported
            tif_files.append( file ) # Append the tif file to the list
            #for each raster, get number of bands present
            if(len(tif_files)>0 ):
                raster=rasterio.open( file ) # Use rasterio to open and read a tif file
                bands= raster.count # get the band count of the tif file

                rasterfile_bands_dict[file] = int(bands) # Store the band count as the value and the filename as the key, to this dict

    print(rasterfile_bands_dict)
    return jsonify( { 'count_raster':count_raster, 'rasterfile_bands_dict':rasterfile_bands_dict, 'tif_files':tif_files } )

# input - df: a Dataframe, chunkSize: the chunk size
# output - a list of DataFrame
# purpose - splits the DataFrame into smaller chunks 
# FOR TIF BASED FILES
def split_dataframe(df, chunk_size = 10000000): 
    chunks = list()
    num_chunks = len(df) // chunk_size + 1
    for i in range(num_chunks):
        chunks.append(df[i*chunk_size:(i+1)*chunk_size])
    return chunks


@app.route('/full_river_predictions_tif',methods=['POST','GET'])
def full_river_predictions_tif(): # This function takes in the full-data tif, splits into several tif files, batch processes them with the same functions and feature engineering methods that were used on the training dataset. After processing, load the trained model / trained curve-fit parameters, make predictions on each tif, and after all batches, merge all separate output tif files into one tif file and store that output tif to the user's selected location

    # Remove all output tifs if present
    import glob,os
    demList1 = glob.glob("tif_processing/output_[0-9]_part.tif") # Get all the list of files with [0-9] (1-10) names
    demList2 = glob.glob("tif_processing/output_[0-9][0-9]_part.tif") # Get all the list of files with [0-9][0-9] (10-100) names
    demList3 = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Get all the list of files with [0-9][0-9][0-9] (100-400) names

    demList=[] # Initialize empty list
    for i in demList1: # For all file names in this list
        demList.append(i) # Append to the main list
    for i in demList2: # For all file names in this list
        demList.append(i) # Append to the main list
    for i in demList3: # For all file names in this list
        demList.append(i) # Append to the main list

    for i in demList: # For all file names in the final list
        os.remove(i) # If there are any files with the names present in this list, remove them files

    ml_algo_obj=request.form['full_ml_algo'] # Get what ml algorithm the user has selected to apply on full-data - this is a json object
    ml_algo_name=json.loads(ml_algo_obj) # Convert the json object to string
    model=''
    
    model_save_loc_obj=request.form['model_save_loc']
    model_save_loc=json.loads(model_save_loc_obj)

    file_path_model_files_obj=request.form['file_path_model_files'] # Get the path to the location where the user chose to save the 'model_files' folder - sometimes user might select the option to save model files folder to apply the pre-trained model later. We get that path now because there are two text files in there that we need to edit - the text file that indicates what ml_algo user last chose to apply on full-data and what columns from full-data were used for processing the data
    file_path_model_files=json.loads(file_path_model_files_obj)

    full_files_name_obj=request.form['tif_files'] # Get the tif files path as json list object
    full_files_name=json.loads(full_files_name_obj) # Convert the json object to list
    
    sel_feats=request.form['feats'] # Columns selected for full-data as json object
    sel_cols3 = json.loads(sel_feats) # Convert the json object to list
    
    actual_status=request.form['actual_status'] # If an actual target variable is present or not - for TIF based full-data, we have kept no actual status value by default
    # actual_status=json.loads(actual_status_obj)

    send_feats_train_obj=request.form['send_feats_train'] # The feature names which were used while training - it is important to use these names and rename the same columns selected for full-data (which might have different names ex: in training it can be 'R' column while in full-data that column might have the name 'R_05m') so rename them to apply the feature processing methods
    send_feats_train=json.loads(send_feats_train_obj) # Convert the json object to list
    new_cols=send_feats_train[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)

    process_options_obj=request.form['process_options'] # Get the json object of the process options value- as we know by default it is best_options - the only value as of the current app version
    process_options=json.loads(process_options_obj) # Convert the json object to list

    pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols3[1] # Get east coordinate column name
    north=sel_cols3[2] # Get north coordinate column name
    
    r=send_feats_train[3] # Get R column name
    g=send_feats_train[4] # Get G column name
    b=send_feats_train[5] # Get B column name
    
    if(actual_status=="Yes"): # Check if there is any actual target variable - FOR TIF based full-data functions, there will be no option for the user to enter target variable
        sel_tar=str(target)
        print("sel tar done")
    if(actual_status=="No"):  # Check if there is no actual target variable
        sel_tar="none"  # If no target variable, make this string to none

    colourspaces_json=request.form['colourspaces'] # Get what colourspace was used in training - this is a json object
    colourspaces  = json.loads(colourspaces_json) # Convert the json object to string value

    print("Process options",process_options)
    print("Actual status",actual_status)
    print(" Model name",ml_algo_name)
    print("features",sel_cols3)

    # Get band names
    band_name_number_json=request.form['band_name_number'] # Get json object for the band details dict that contains the band names and band number in the order of input files - first would be the rgb raster band names and then band name and its number for other rasters
    band_name_number  = json.loads(band_name_number_json) # Convert the json object to dict

    print(band_name_number)

    rasterfile_bands_dict_json=request.form['rasterfile_bands_dict'] # This variable has all the details of each band of every raster file
    rasterfile_bands_dict  = json.loads(rasterfile_bands_dict_json)

    print(rasterfile_bands_dict)


    # full_data=pd.read_csv(full_files_name[0], chunksize=1000000)
    #### Create csv from tif file
    import os,gc
    import gdal
    import geopandas as gpd
    from osgeo import gdal
    import rasterio

    # Read only RGB raster first
    raster_filename = full_files_name[0]
    print(raster_filename)
    
    output_count=0 # Initialze this to 0 - this is a counter to keep count of the output file number for every split


    # Warp all other rasters except the first (rgb) raster
    from osgeo import gdal
    dem = gdal.Open(raster_filename) # get raster file dem
    gt = dem.GetGeoTransform() # get extents
    print(gt)
    # get coordinates of upper left corner
    xmin = gt[0]
    ymax = gt[3]
    res_all = gt[1]

    ulx, xres, xskew, uly, yskew, yres = dem.GetGeoTransform()
    lrx = ulx + (dem.RasterXSize*xres) # Calculate lower right and bottom coordinates
    lry = uly + (dem.RasterYSize*yres)

    rastfile_warp = {}
    # Save the raster object in a dict with filename as key
    for rast_file in full_files_name: # For all files in the list
        if(rast_file!=raster_filename): # Proceed only if the file name is not the first raster file - which is the rgb raster
            print(" Warping raster file ", rast_file)
            # Warp the chainage raster
            # We are warping the chainage raster to match its extents and projection to that of the main rgb raster - only if they match, we can extract band values on those matching coordinates
            ds = gdal.Open(rast_file,1) # Use gdal to open and read the raster file
            src=None # Make this to none first
            src = gdal.Warp("", ds,format="vrt",
                outputBounds=[ ulx, lry, lrx, uly ], 
                xRes=res_all, yRes=-res_all,
                resampleAlg=gdal.GRA_NearestNeighbour, options=['COMPRESS=DEFLATE']) # This function warps the raster to match with the given output bounds and resolution. 
            print(src.GetGeoTransform())

            rastfile_warp[str(rast_file)] = src # Save each dem object of each raster file to a list and use it later
            src = None # Make it none and use this for next raster file in next loop
            ds = None # Make it none and use this for next raster file in next loop

    # determine total length of raster
    xlen = res_all * dem.RasterXSize
    ylen = res_all * dem.RasterYSize

    # number of tiles in x and y direction
    div = 20 # Time taken for the whole process till end:  For large data (500mb input raster file) - 30 div = 3590 sec, 15 div = 3000 sec, 10 div = 3300 sec. For small data (50mb input raster file) - 5 div = 343 sec, 15 div = 299 sec, 50 div = 817 sec, 30 div = 400 sec
    # ydiv = 2

    # size of a single tile
    xsize = xlen/div
    ysize = ylen/div
    
    # create lists of x and y coordinates
    xsteps = [xmin + xsize * iq for iq in range(div+1)]
    ysteps = [ymax - ysize * iq for iq in range(div+1)]
    
    
    # loop over min and max x and y coordinates
    for ii in range(div):
        for jj in range(div):
            xmin_all = xsteps[ii] # Get initial end of the current split-square x coordinate (left-top corner)
            xmax_all = xsteps[ii+1] # Get far end of the current split-square x coordinate (right-top corner)
            ymax_all = ysteps[jj] # Get initial end of the current split-square y coordinate (left-bottom corner)
            ymin_all = ysteps[jj+1] # Get far end of the current split-square y coordinate (right-bottom corner)
            
            print("xmin: "+str(xmin_all))
            print("xmax: "+str(xmax_all))
            print("ymin: "+str(ymin_all))
            print("ymax: "+str(ymax_all))
            print("\n")

            output_count += 1 # For every square (split) or for every loop, increment this counter - represents the output number of each tif split part
            main_raster_obj = None # Main raster - rgb raster
            other_raster_obj = None # Other rasters - chainage, weight etc.  rasters

            main_raster_obj = gdal.Translate("tif_processing/input_"+ str(output_count) +"_part.tif", dem, projWin = (xmin_all, ymax_all, xmax_all, ymin_all), xRes = res_all, yRes = -res_all, creationOptions=["COMPRESS=DEFLATE"]) # Split into a small part
            # First get RGB values from main raster (first input raster is always the main raster)

            nodata_val = main_raster_obj.GetRasterBand(1).GetNoDataValue() # Get the nodata value of the rgb raster
            print(nodata_val)
            if(nodata_val==None): # If nodata value is none then make it 256 for rgb raster
                print('None nodata value')
                nodata_val =  np.float(256) # 256 - nodatavalue for rgb raster
                print(nodata_val)
            band1=main_raster_obj.GetRasterBand(1).ReadAsArray() # Read the first band of the main rgb raster as array
            print(band1.shape)
            xmin = main_raster_obj.GetGeoTransform()[0] # Get lower extents
            ymax = main_raster_obj.GetGeoTransform()[3] # Get top extents
            xsize = main_raster_obj.RasterXSize # Get total length in X axis
            ysize = main_raster_obj.RasterYSize # Get total length in X axis
            xstart = xmin +main_raster_obj.GetGeoTransform()[1]/2 # Get the start point - this should be the mid-point of the left bottom most grid - that is why we add ( for X-axis go right side from min X ) half of the resolution value to the initial start point - to get the mid-point 
            ystart = ymax - main_raster_obj.GetGeoTransform()[1]/2 # Get the start point - this should be the mid-point of the right topmost grid - that is why we subtract ( for Y-axis go down side from max Y ) half of the resolution value to the initial start point - to get the mid-point 
            res = main_raster_obj.GetGeoTransform()[1] # Save the resolution value

            x = np.arange(xstart, xstart+xsize*res, res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the initial grid's mid-point and add half resolution values all the way till topmost grid
            y = np.arange(ystart, ystart-ysize*res, -res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the topmost grid's mid-point and subtract half resolution values all the way till bottom most grid

            # The X values should in ascending order and Y values in descending order

            # Sometimes translating a raster leaves some pixels in the corners behind and so the shape of the raster slightly changes. To match with the original shape, we compare the original shape and translated raster and add/remove pixel values to match them both
            print(x.shape,y.shape)
            
            if(x.shape[0]>band1.shape[1]): # If the created x array has more values than the band's x values
                x_diff = x.shape[0]-band1.shape[1] # Then remove the extra values. This line gets the count of extra values
                x=x[:-x_diff] # Slice the array by removing the extra value's indices

            if(x.shape[0]<band1.shape[1]): # If the created x array has less values than the band's x values
                x_diff = band1.shape[1] - x.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                for idf in range(x_diff): # For each extra value needed
                    x = np.insert(x, x[-1]+res) # Adding one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

            if(y.shape[0]>band1.shape[0]): # If the created x array has more values than the band's y values
                y_diff = y.shape[0]-band1.shape[0] # Then remove the extra values. This line gets the count of extra values
                y=y[:-y_diff] # Slice the array by removing the extra value's indices

            if(y.shape[0]<band1.shape[0]): # If the created x array has less values than the band's y values
                y_diff = band1.shape[0] - y.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                for idf in range(y_diff): # For each extra value needed
                    y = np.insert(y, y[-1]-res) # Subtracting one more coordinate value with value equal to the previous value subtracted by the resolution value - this will be the center point of next below grid

            print(x.shape,y.shape)
            X1,Y1 = np.meshgrid(x,y,copy=False) # Create a meshgrid  - this will output two variables which have same format as the band values from raster - look up meshgrid to know more. Important - meshgrid should be in this format : X values should in ascending order and Y values in descending order

            del x,y # Delete this x and y
            gc.collect()
            save_this_df = pd.DataFrame(columns=['East_generated','North_generated']) # Create a new dataframe with east and north column names as specified
            save_this_df['East_generated'] = X1.flatten() # Flattening will convert to 1-d array from n-d array
            save_this_df['North_generated'] = Y1.flatten() # Flattening will convert to 1-d array from n-d array
            print('With all nan and values ', save_this_df.shape)
            gc.collect()   

            # # here get nodata values and remove
            mr = np.ma.masked_equal(band1,nodata_val) ## Try to get new mask spcific to each band/file - because a lot of times they might not be the same as the rgb raster and using the same maks as the rgb raster might give nodata values 
            new_x = np.ma.masked_array(X1, mr.mask) # Apply the mask to x and y meshgrid values
            new_y = np.ma.masked_array(Y1, mr.mask) # Apply the mask to x and y meshgrid values
            del X1,Y1
            gc.collect()
            new_x = np.ma.compressed(new_x) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
            new_y = np.ma.compressed(new_y) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
            gc.collect()
            print(new_x.shape)
            print(new_y.shape)

            df = pd.DataFrame(columns=['East_generated','North_generated']) # Create another dataframe with these east and north column names
            df['East_generated']=np.array(new_x) # Copy the compressed east coordinate array values into the east column # Store x and y to dataframe- these do not have any nan or no-data values
            df['North_generated']=np.array(new_y) # Copy the compressed north coordinate array values into the north column # Store x and y to dataframe- these do not have any nan or no-data values
            del new_x,new_y
            gc.collect()

            for i in sel_cols3[3:6]: # Now using that mask to apply on three bands of the raster - to get r,g,b. Also, find the band number of r,g,b instead of taking first three bands directly - they might be in different order
                band_number = band_name_number[i] # Get band number of each feature (r,g,b) from this list
                band1 = main_raster_obj.GetRasterBand(band_number).ReadAsArray() # Get the band values as array
                new_band1 = np.ma.masked_array(band1, mr.mask) #Apply mask for band values
                del band1
                new_band1 = np.ma.compressed(new_band1) # Compress to get only values without nodata values
                df[i]=np.array(new_band1) # Copy the compressed band values into a new column named after the Ith feature name # Store to dataframe

            del new_band1
            gc.collect()


            df = df.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values
            df=df.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
            print( " Main raster df ")
            print(df.shape)
            print(df.describe())
            print(df.head())

            print("  THESE MANY ROWS ARE AVAILABLE",len(df))

            if(len(df) == 0 ): # If there are no proper values - when we split tif files into many parts, some parts may not have any band values - just nodata values. So once we remove all no-data values, there will be no other points left - in that case just write this tif file back to output tif and merge it at the end with all other output split parts
                save_this_df['NODATA']=np.float32(nodata_val) # Get the nodata values and store to this dataframe
                mdf = np.array(save_this_df['NODATA'].values, dtype=np.float32) # Convert to array and change data type to float32 - uses less memory to store files
                import rasterio as rio   
                with rio.open("tif_processing/input_"+ str(output_count) +"_part.tif") as src2: # Read the input split file and copy its metadata to store it to output tif file of this specific split part
                    ras_data = src2.read() # Read the raster data
                    ras_meta = src2.profile # Read the meta data of raster
                # make any necessary changes to raster properties, e.g.:
                
                ras_meta['dtype'] = "float32" # Since all values are nodata values, we can just use a float 32 value to store them - higher the precision- more storage the tif file would consume
                ras_meta['nodata'] = np.float32(nodata_val)
                ras_meta['count'] = 1 # Specify only one band in output tif file
                ras_meta['options']=['COMPRESS=DEFLATE']
                # inDs = gdal.Open("input_"+ str(output_count) +"_part.tif")
                ras_meta['crs'] = main_raster_obj.GetProjection() # Get CRS on main rgb raster file and save it as the meta data crs of the output raster file
                
                if(output_count<10): # Store the file in format 001,002 upto 400 - since we divide the main raster into 20x20 filter, there will be 400 parts
                    with rio.open("tif_processing/output_00"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 001 to 010
                elif(output_count>10 and output_count<100):
                    with rio.open("tif_processing/output_0"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 010 to 099
                else:
                    with rio.open("tif_processing/output_"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1)  # Store for output number values between 100 to 400 

                dst = None
                src2= None
                main_raster_obj =None
                inDs =None

                # gdal.Translate('output_' + str(output_count) +'_part.tif', dem, projWin = (xmin, ymax, xmax, ymin), xRes = res, yRes = -res, dtype='float32' ,options=['COMPRESS=DEFLATE'] )
                # main_raster_obj = None
                import os
                os.remove("tif_processing/input_"+ str(output_count)+"_part.tif") # Remove the input split file of this iteration

            # Now if there are actual values present, which are not nodata values - then get other raster values like chainage etc. by matching the coordiante x and y values 
            if(len(df)>0):
                for i in sel_cols3[6:]: # For rest of the columns other than r,g,b
                    # for multiple files
                    band_numb = band_name_number[i] # Get the band number of the Ith feature name
                    total_count = 0
                    determined_band_numb = 0
                    loop_done=0

                    # determine which file it belongs to
                    # For every file do the same as done earlier - get the specific split part( we already computed xmin xmax ymin ymax above), get this small part, check for proper values and merge with dataframe
                    for fl in full_files_name: # Go through all files and check which file has the band values / band name of the particular iteration (Ith feature name)
                        if(loop_done==0): # As long as loop is active
                            fl_count = rasterfile_bands_dict[fl] # For every file , get the total band count from this dict
                            total_count = total_count + fl_count # Add the total count to band count
                            print(fl_count, fl,i, band_numb, total_count )
                            if( (band_numb > total_count) == False ): # Compare with the values from javascript - if the band number of this feature is not greater than the total count - it means this band must be in this file. So we search more in this particular file 
                                file_name_val = fl # We set the file name to the current iteration's filename

                                # determine the band number in that file name
                                total_count = total_count - fl_count # Subtract the total count from the file's total band count - this is to reset the count to the start value of this band. So now, we can have a loop over the total band count of the file, keep adding 1 to the total count and see if the values match. If they match - that Kth value is the band number of the Ith feature in that raster file
                                for k in range( fl_count ): # K is the iteraction count for the total band count of this raster file 'fl'
                                    if( band_numb == ( total_count + (k+1) ) ): # If the band number from the list matches with the total count + Kth value+1 - then K+1 is the  band number
                                        src = None
                                        determined_band_numb = k + 1 # Set band number of the Ith feature
                                        print(determined_band_numb,file_name_val)
                                        
                                        # Get the raster object of the file
                                        src = rastfile_warp[str(file_name_val)] # Get the dem object saved earlier, for the raster filename 'file_name_val' 

                                        # For every tif split (tile), get the same split from other files 
                                        other_raster_obj= gdal.Translate("", src, format='vrt', projWin = (xmin_all, ymax_all, xmax_all, ymin_all), xRes = res_all, yRes = -res_all)

                                        nodata_val_ch = other_raster_obj.GetRasterBand(determined_band_numb).GetNoDataValue()  # Get the nodata value of the raster
                                        print(nodata_val_ch)
                                        if(nodata_val_ch==None): # If nodata value is none then make it -3.402823e+38 for non-rgb raster
                                            print('None nodata')
                                            nodata_val_ch= np.float(-3.402823e+38) # nodatavalue for non-rgb raster
                                            print(nodata_val_ch)
                                        
                                        band1 = other_raster_obj.GetRasterBand(determined_band_numb).ReadAsArray() # Read the first band of the main rgb raster as array
                                        xmin = other_raster_obj.GetGeoTransform()[0] # Get lower extents
                                        ymax = other_raster_obj.GetGeoTransform()[3] # Get top extents
                                        xsize = other_raster_obj.RasterXSize # Get total length in X axis
                                        ysize = other_raster_obj.RasterYSize # Get total length in Y axis
                                        xstart = xmin + other_raster_obj.GetGeoTransform()[1] / 2 # Get the start point - this should be the mid-point of the left bottom most grid - that is why we add ( for X-axis go right side from min X ) half of the resolution value to the initial start point - to get the mid-point 
                                        ystart = ymax - other_raster_obj.GetGeoTransform()[1] / 2 # Get the start point - this should be the mid-point of the right topmost grid - that is why we subtract ( for Y-axis go down side from max Y ) half of the resolution value to the initial start point - to get the mid-point 
                                        res = other_raster_obj.GetGeoTransform()[1] # Save the resolution value

                                        x = np.arange(xstart, xstart + xsize * res, res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the initial grid's mid-point and add half resolution values all the way till topmost grid
                                        y = np.arange(ystart, ystart - ysize * res, -res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the topmost grid's mid-point and subtract half resolution values all the way till bottom most grid

                                        print(band1.shape)
                                        print(x.shape,y.shape)

                                        # The X values should in ascending order and Y values in descending order

                                        # Sometimes translating a raster leaves some pixels in the corners behind and so the shape of the raster slightly changes. To match with the original shape, we compare the original shape and translated raster and add/remove pixel values to match them both
                                        if(x.shape[0]>band1.shape[1]): # If the created x array has more values than the band's x values
                                            x_diff = x.shape[0]-band1.shape[1] # Then remove the extra values. This line gets the count of extra values
                                            x=x[:-x_diff] # Slice the array by removing the extra value's indices

                                        if(x.shape[0]<band1.shape[1]): # If the created x array has less values than the band's x values
                                            x_diff = band1.shape[1] - x.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                                            for i in range(x_diff): # For each extra value needed
                                                x = np.insert(x, x[-1]+res) # Adding one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

                                        if(y.shape[0]>band1.shape[0]): # If the created x array has more values than the band's y values
                                            y_diff = y.shape[0]-band1.shape[0] # Then remove the extra values. This line gets the count of extra values
                                            y=y[:-y_diff] # Slice the array by removing the extra value's indices

                                        if(y.shape[0]<band1.shape[0]): # If the created x array has less values than the band's y values
                                            y_diff = band1.shape[0] - y.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                                            for i in range(y_diff): # For each extra value needed
                                                y = np.insert(y, y[-1]-res) # Subtracting one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

                                        X1, Y1 = np.meshgrid(x, y, copy=False) # Create a meshgrid  - this will output two variables which have same format as the band values from raster - look up meshgrid to know more. Important - meshgrid should be in this format : X values should in ascending order and Y values in descending order
                                        del x
                                        del y
                                        gc.collect()

                                        
                                        mr = np.ma.masked_equal(band1, nodata_val_ch) ## Try to get new mask spcific to each band/file - because a lot of times they might not be the same as the rgb raster and using the same maks as the rgb raster might give nodata values 
                                        new_x = np.ma.masked_array(X1, mr.mask) # Apply the mask to x and y meshgrid values
                                        new_y = np.ma.masked_array(Y1, mr.mask) # Apply the mask to x and y meshgrid values

                                        del X1,Y1
                                        gc.collect()
                                        new_x = np.ma.compressed(new_x) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
                                        new_y = np.ma.compressed(new_y) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 

                                        temp_df = pd.DataFrame(columns=['East_generated', 'North_generated', i]) # Create another dataframe with these east and north column names
                                        temp_df['East_generated'] = np.array(new_x) # Copy the compressed east coordinate array values into the east column
                                        temp_df['North_generated'] = np.array(new_y) # Copy the compressed north coordinate array values into the north column

                                        del new_x
                                        del new_y
                                        gc.collect()

                                        new_band1 = np.ma.masked_array(band1, mr.mask) #Apply mask for band values
                                        del band1
                                        new_band1 = np.ma.compressed(new_band1) # Compress to get only values without nodata values

                                        temp_df[i] = np.array(new_band1) # Copy the compressed band values into a new column named after the Ith feature name # Store to dataframe
                                        temp_df = temp_df.drop_duplicates(subset=['East_generated','North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values
                                        temp_df = temp_df.reset_index().drop(['index'], axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                                        df = pd.merge(df, temp_df, how='inner', on=['East_generated','North_generated']) # Now merge on common values from df and temp_df - this appends all feature columns from different iteration of features, into one single dataframe
                                        print (' Adding other raster df ')
                                        print(df.head())
                                        print(df.shape)


                gc.collect()
    
                df=df.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                df['ID'] = np.array(df.index) # Add an ID column
                print('After all bands')
                print(df.head())
                print(df.shape)

                process_options=json.loads(process_options_obj) # Get the list of the process options value- as we know by default it is best_options - the only value as of the current app version
                
                ## This df is the final full-data for processing
                full_data = split_dataframe(df)  # Read the full-data csv not as whole, but in chunks of size 10000000 each : Check the split_dataframe function - batch processing by performing multiple iterations to complete all batches and then finally merge prediction values of all batches into one single csv and output to user
                del df
                if("best_options" in process_options): # When the value in process_options list is 'best_options'
                    print("in best options")
                    full_river=pd.DataFrame() # Initialize an empty dataframe
                    # data_reference=load_data('data_for_all_models.csv')
                    if(ml_algo_name=="Technique 1"): # If the selected algorithm to apply on full-data is technique 1
                        model=joblib.load('model_files/'+'Technique 1.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 1" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 5"): # If the selected algorithm to apply on full-data is technique 5
                        model=joblib.load('model_files/'+'Technique 5.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 5" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 2"): # If the selected algorithm to apply on full-data is technique 2
                        model=joblib.load('model_files/'+'Technique 2.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 2" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 3"): # If the selected algorithm to apply on full-data is technique 3
                        model=joblib.load('model_files/'+'Technique 3.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 3" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 4"): # If the selected algorithm to apply on full-data is technique 4
                        model=joblib.load('model_files/'+'Technique 4.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 4" # Define a column name to add to the final dataframe
                    z=0
                    option_csv=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                    process_options=[] # Initialze an empty list
                    for i in range(len(option_csv)): # For all the values in the option_csv dataset
                        process_options.append(option_csv.iloc[i]['best_option']) # Append the values into the new list
                    print(process_options)
                    for jun in full_data: # For every batch ( chunk ) of the full-data, do all the below codes
                        jun=jun.dropna() # Drop nan values if any
                        processed_data=jun.copy() # Make a copy of the batch full-data
                        z=z+1 # Keep track of the chunk or cluster or current batch number
                        print("cluster ",z)
                        print(processed_data.index[0],processed_data.index[-1])
                        if(actual_status=="Yes"): # Check if there is an actual target variable
                            predi=pd.DataFrame() # Initiate empty dataframe
                            predi[sel_tar]=processed_data[sel_tar].copy() # If actual variable is present and selected by the user, then copy the target values and keep this as a separate dataframe and merge along with the predictions dataframe at the end
                        elif(actual_status=="No"): # Check if there is no actual target variable
                            predi=pd.DataFrame() # If no actual target variable, then just initiate empty dataframe

                        to_merge=pd.DataFrame() # Empty dataframe to merge later
                        merged=pd.DataFrame() # Empty dataframe to merge predictions of every batch processing iteration and the id, east and north of every batch-processing iteration - there is no batch processing of full-data in curve-fitting, but there is batch processing in simplified workflow since its computation load is huge
                        to_merge=pd.DataFrame(processed_data[[pid,east,north]]) # Copy the id, east and north columns from the main dataframe to be processed, into the 'to_merge' dataframe - use this dataframe to merge with the  predictions dataframe at the end

                        # other selected colourspaces add
                        # check if 0 value is present in r,g,b
                        r=sel_cols3[3] #3 because of east north corrd input. Get R column name  
                        g=sel_cols3[4] # Get G column name
                        b=sel_cols3[5] # Get B column name  

                        new_sel_cols3 = sel_cols3.copy() # Keep a copy of the original sel_cols3 - which has the selected full-data features. The reason to keep a copy is in the next few steps - new features will be added to the list and hence we need to have the original features to extract initial columns from the next batch of the batch-processed full-data

                        if(processed_data.describe()[r]['min']==0):   # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                
                            processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
                        if(processed_data.describe()[g]['min']==0):   # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                
                            processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
                        if(processed_data.describe()[b]['min']==0):   # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                
                            processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

                        if(processed_data.describe()[r]['min']<0):    # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                            
                            processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
                        if(processed_data.describe()[g]['min']<0):    # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                           
                            processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
                        if(processed_data.describe()[b]['min']<0):    # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                           
                            processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

                        if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
                            print('same dataset - no hsv only rgb')

                        elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
                            print('same dataset - no hsv only rgb')

                        elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
                            print('change in dataset - no rgb only hsv')
                            hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                            processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                            processed_data[r] = processed_data2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data[g] = processed_data2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data[b] = processed_data2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                            processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
                            for ix, valx in enumerate(new_sel_cols3): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
                                if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                                    new_sel_cols3[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
                                if(valx == g):
                                    new_sel_cols3[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
                                if(valx == b):
                                    new_sel_cols3[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
                            
                            # check if 0 value is present in r,g,b
                            if(processed_data.describe()['H_generated']['min']==0):  # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                 
                                processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                            if(processed_data.describe()['S_generated']['min']==0):  # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                 
                                processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                            if(processed_data.describe()['V_generated']['min']==0):  # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                 
                                processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                            del processed_data2

                        elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
                            print('change in dataset - rgb and hsv')
                            hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                            processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                            processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                            #insert in new_sel_cols3  - since we created new features, they should be added for further use in the next parts of the code
                            new_sel_cols3.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                            new_sel_cols3.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                            new_sel_cols3.insert(8, 'V_generated')  # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

                            # check if 0 value is present in r,g,b
                            if(processed_data.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                                    
                                processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                            if(processed_data.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                                   
                                processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                            if(processed_data.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                                   
                                processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                            del processed_data2 # Delete the temporary dataframe

                        r=send_feats_train[3] # Get R column name - the R feature name in the INPUT TRAINING DATA
                        g=send_feats_train[4] # Get G column name - the G feature name in the INPUT TRAINING DATA
                        b=send_feats_train[5] # Get B column name - the B feature name in the INPUT TRAINING DATA

                        sel_cols2=new_sel_cols3[3:]     # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
                        processed_data=processed_data[sel_cols2].copy() # Create the main subset from the initial dataframe - this new dataframe would have only features from R,G,B, to other variables - NOT east, north, id etc, - note that there is no target variable here
                        old_cols=new_sel_cols3[3:] # To rename the full-data column names to be same as the input training-data column names, get all the current full-data column names from r,g,b, till the last selected feature
                        print(old_cols, new_cols)
                        for j in range(len(old_cols)): # For every current full-data column name
                            processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True) # Like mentioned before, need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
                        

                        print("start",processed_data.shape)
                        if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
                            processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                            print("start1",processed_data.shape)

                        if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
                            processed_data=greyscale_gen_full(processed_data,r,g,b)
                            print("start2",processed_data.shape)

                        if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
                            processed_data=cluster_data_full(processed_data,r,g,b)  
                            print("start3",processed_data.shape)

                        if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=poly_creation_cluster_full(processed_data)    
                            print("start4",processed_data.shape)

                        if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=poly_creation_no_cluster_full(processed_data)     
                            print("start4",processed_data.shape)

                        if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=standard_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                            print("start5",processed_data.shape)

                        if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=standard_scale_no_cluster_full(processed_data,"common")  # Common specifies same data for all ml techniques   
                            print("start5",processed_data.shape)
                        
                        if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=min_max_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques
                            print("start6",processed_data.shape)

                        if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=min_max_scale_no_cluster_full(processed_data,"common")  # Common specifies same data for all ml techniques   

                        if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=robust_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

                        if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=robust_scale_no_cluster_full(processed_data,"common")    # Common specifies same data for all ml techniques 

                        if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=power_scale_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

                        if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=power_scale_no_cluster_full(processed_data,"common")   # Common specifies same data for all ml techniques

                        if("pearson_correl" in process_options): # This line checks if pearson_correl is in the process_options list 
                            processed_data=correl_full(processed_data,"common") # Common specifies same data for all ml techniques
                            print("start7",processed_data.shape)

                        if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
                            lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv')
                            processed_data=lasso_reg_full(processed_data,lasso_result)
                            print("start8",processed_data.shape)

                        if(("pca_trans" in process_options) & ("cluster_gen" in process_options)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=pca_reduction_cluster_full(processed_data,"common")    # Common specifies same data for all ml techniques 

                        if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=pca_reduction_no_cluster_full(processed_data,"common") # Common specifies same data for all ml techniques

                        print(processed_data.tail())
                        print("model",model,ml_algo_name)

                        # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
                        if(ml_algo_name=="Technique 1" or ml_algo_name=="Technique 5"): # If the selected algorithm is technique 1 or if the selected algorithm is technique 5
                            option_csv_df=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                            options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                            no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
                            options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                            if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                                no_scale_check_lr=1 # atleast one scaling
                            if(no_scale_check_lr==0): # If none present, then do this
                                sc_t1_5=joblib.load('model_files/'+'sc_forT1&5only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                                processed_data=sc_t1_5.transform(processed_data) # Scale and transform the processed full-data
                                print('no scale')
                            
                        if(ml_algo_name=="Technique 4"): # If the selected algorithm is technique 4
                            option_csv_df=pd.read_csv('model_files/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                            options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                            no_scale_check=0 # Initialize a scaling check for nn (lr and svr already done)
                            options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                            if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                                no_scale_check=1 # atleast one scaling
                            if(no_scale_check==0): # If none present, then do this
                                sc_t4=joblib.load('model_files/'+'sc_forT4only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                                processed_data=sc_t4.transform(processed_data) # Scale and transform the processed full-data
                                print('no scale')
                        predi[str(col_name)+'_Pred']=model.predict(processed_data) # Predict depth values for the processed batch-data and etore the prediction values to the dataframe under the column name 'ALGO_NAME_pred' - ALGO_NAME is the chosen ml algorithm name
                        df_preds2=pd.DataFrame(data=predi[str(col_name)+'_Pred'].values,columns= [str(col_name)+'_Pred'], index=to_merge.index) # Copy the same dataframe to a new one - with index from this batch's input full-data dataset - copy the index from this dataframe to this new dataframe so that the values are in correct order while merging all batches at the end 
                        if(actual_status=="Yes"): # Check if there is an actual target variable
                            df_preds2[sel_tar]=predi[sel_tar]    # If actual variable is present and selected by the user, then copy the target values into this new dataframe
                        
                        merged=pd.concat([to_merge,df_preds2],axis=1) # Merge all columns and predicted values into one dataframe
                        merged=merged.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
                        full_river=pd.concat([full_river,merged],axis=0) # Merge the current batch values with the previous batch (if any) values - concats on row base, adds more rows after the previous batches' rows
                        full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
                        del merged # Delete the temporary dataframe
                        del df_preds2 # Delete the temporary dataframe
                        del predi,processed_data # Delete the temporary dataframe

                    del full_data

                    full_river=full_river.sort_values(by=pid) # Sort the values of ID column
                    full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the sorted dataframe and remove the index column
                    # full_river = full_river.sort_values(by = ["North_generated", "East_generated"], ascending = [False, True])
                    # full_river=full_river.reset_index().drop(['index'],axis=1) 

                    print("Sample of the output")
                    print(full_river.head())

                    jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records')) # Save the first 5 rows of the input data with just selected independent features, as json object to show to the user using javascript  
                    preds_file_name='Predictions_'+str(col_name).lower()+'.tif' # get the selected ml_algo name and it's prediction column name to copy that name to the output file's name

                    ## Convert back to raster tif file
                    full_river = full_river.drop(['ID'],axis=1) # Drop the ID column which was created during the process
                    
                    full_river = full_river.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values 
                    full_river = full_river.reset_index().drop(['index'],axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                    # save_this_df = pd.merge(save_this_df, full_river,  how='left', on=['East_generated','North_generated']).fillna(float(nodata_val))
                    mdf=pd.DataFrame()
                    # Now we have the output predictions and east,north coordinates in a dataframe. We need to merge with the save_this_df dataframe - this dataframe contains all east, north coordinates. All coordinates includes the ones we removed because they had no-data values.
                    # The reason to merge it is when we create output tif file, the array values should match the exact number of values as was the input band. After removing nodata values, there will be change in number of values, so we need to add those nodata values back and also make sure we add coordinates at the right indices - should not be random.
                    chks = split_dataframe(save_this_df) # We split the dataframe because merge function is computationally very heavy and splitting into smaller chunks and merging each chunk based on the east and north coordinates will be efficient
                    del save_this_df

                    for i in chks: # For each chunk
                        print(i.shape)
                        mdf1 = pd.merge(i, full_river,  how='left', on=['East_generated',
                                        'North_generated']).fillna(float(nodata_val)) # Merge this chunk of predictions with the east, north data 
                        mdf = pd.concat([mdf, mdf1 ],axis=0) # Merge the updated chunk with the previously merged rows
                        del i
                        del mdf1
                        gc.collect()
                        mdf = mdf.reset_index().drop(['index'],axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                    del chks,full_river
                    gc.collect()
                    print(mdf.shape)
                    print(mdf.head())

                    print('merge done')
                    # Now we have complete dataframe with number of values matching this particular split part's total raster values, so convert to tif and store with output number
                    mdf = mdf.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                    mdf = np.array( mdf[ str(col_name)+'_Pred' ].values, dtype=np.float32 ) # Get the prediction column values and change data type to float32 - uses less memory to store files
                    
                    # Read main rgb file and get its extents to output tif
                    xsize = main_raster_obj.RasterXSize # Get total length in X axis
                    ysize = main_raster_obj.RasterYSize # Get total length in X axis
                    
                    import rasterio as rio   
                    with rio.open("tif_processing/input_"+ str(output_count) +"_part.tif") as src2: # Read the input split file and copy its metadata to store it to output tif file of this specific split part
                        ras_data = src2.read() # Read the raster data
                        ras_meta = src2.profile # Read the meta data of raster

                    # make any necessary changes to raster properties, e.g.:
                    ras_meta['dtype'] = "float32" # Since all values are nodata values, we can just use a float 32 value to store them - higher the precision- more storage the tif file would consume
                    ras_meta['nodata'] = np.float32(nodata_val)
                    ras_meta['count'] = 1 # Specify only one band in output tif file
                    ras_meta['options']=['COMPRESS=DEFLATE']
                    # inDs = gdal.Open("tif_processing/input_"+ str(output_count) +"_part.tif")
                    ras_meta['crs'] = main_raster_obj.GetProjection() # Get CRS on main rgb raster file and save it as the meta data crs of the output raster file
                   
                    print(ras_meta)

                    if(output_count<10): # Store the file in format 001,002 upto 400 - since we divide the main raster into 20x20 filter, there will be 400 parts
                        with rio.open("tif_processing/output_00"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                            dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 001 to 010
                    elif(output_count>10 and output_count<100):
                        with rio.open("tif_processing/output_0"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                            dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 010 to 099
                    else:
                        with rio.open("tif_processing/output_"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                            dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 100 to 400

                    dst = None

                    src2= None
                    main_raster_obj = None
                    other_raster_obj = None
                    
                    import os
                    os.remove("tif_processing/input_"+ str(output_count)+"_part.tif") # Remove the input split file of this iteration

                    #if you get errors with proj.db or with crs - then use this:
                    # inDs = gdal.Open(raster_filename)
                    # ras_meta['crs'] = inDs.GetProjection()
                    del mdf
                    gc.collect()

    src = None 
    dem = None
    del rastfile_warp
    gc.collect()

    # Merge all tif into a single output file
    import glob
    demList = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Use glob to get all files name with matching names

    print(demList)

    vrt = gdal.BuildVRT("merged.vrt",demList) # Build a VRT and pass in the list of all output tif file names to be merged into one single tif file
    gdal.Translate("tif_processing/"+ preds_file_name, vrt, xRes= res_all, yRes=-res_all, creationOptions=["COMPRESS=DEFLATE"] ) # It will merge all files into one file and compress the size using deflate algorithm
    vrt = None               

    # NEW - ADD TO ALL TIF - IF EXTENTS DON'T MATCH DO THIS: USE RGB RASTER TO ADD EXTENTS TO OUTPUT TIF
    # gdal.Translate("input_"+ str('newww') +"_part.tif", src, projWin = ( ulx ,uly, lrx, lry ), xRes = xres, yRes = -xres, creationOptions=["COMPRESS=DEFLATE"])

    # src = gdal.Warp("", ds,format="vrt",
    #                 outputBounds=[ ulx, lry, lrx, uly ], 
    #                 xRes=xres, yRes=-xres,
    #                 resampleAlg=gdal.GRA_NearestNeighbour, options=['COMPRESS=DEFLATE'])

    # Remove all output tifs if present
    import glob
    demList1 = glob.glob("tif_processing/output_[0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file
    demList2 = glob.glob("tif_processing/output_[0-9][0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file
    demList3 = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file

    demList=[] # Initialize empty list
    for i in demList1: # For all files in this list of separate output tif files - this is for 0-10 output numbered files
        demList.append(i) # Append to the new list
    for i in demList2: # For all files in this list of separate output tif files - this is for 10-100 output numbered files
        demList.append(i) # Append to the new list
    for i in demList3: # For all files in this list of separate output tif files - this is for 100-400 output numbered files
        demList.append(i) # Append to the new list

    for i in demList: # For all files in the fully appened list of output tif files
        os.remove(i) # Remove them

    if(actual_status=="No"): # When there is no target variable in full-data
        model_textfile_name="FINAL_MODEL_USED.txt" # Define a filename - this section is for saving last used model name for highlighting
        if(os.path.exists('model_files/'+str(model_textfile_name))): # Check if the file model_files/FINAL_MODEL_USED.txt pre-exists
            os.remove('model_files/'+str(model_textfile_name)) # if it is present, delete it

        model_textfile=pd.DataFrame() # Initialize empty dataframe
        print(ml_algo_name)
        model_textfile['Model Name']=[str(ml_algo_name)] # Under the column 'Model_Name' save the ml algo name which was selected by the user to predict on full-data 
        model_textfile.to_csv('model_files/'+str(model_textfile_name),index=False)   # Save this text file to local path to use it later in the pre-trained module

        if( (model_save_loc != None) and (file_path_model_files != None) ): # We should have already saved this file during the ml training phase (when user saved the model files ) itself because this file is used during the pre-trained module - so get the file location to wherever the user decided to save it and overwrite the 'FINAL_MODEL_USED.txt' contents with the updated contents, if the user did not save it (if both 'file_path_model_files' and 'model_save_loc' are none ) then discard this line
            model_textfile.to_csv(file_path_model_files + model_save_loc + '/' + str(model_textfile_name),index=False)       # Overwrite and save as csv to the user selected location

        # Save columns chosen for full river
        col_textfile_name="FINAL_COLS_USED.txt"
        if(os.path.exists('model_files/'+str(col_textfile_name))):
            os.remove('model_files/'+str(col_textfile_name))
        cols_textfile=pd.DataFrame(columns=['Columns']) # Initialize a new dataframe with column name 'Columns'
        ALL_COLS='' # Initialize empty string
        print("selcols3",sel_cols3)
        for i in sel_cols3: # For every column name used while processing the full-data
            ALL_COLS+= str(i) + ', ' # Append each of those column names to a string - so each column value is separated by a ','
        cols_textfile.loc[0]=ALL_COLS # Add this string as the value of first row of the new dataframe 
        cols_textfile.to_csv('model_files/'+str(col_textfile_name),index=False)  # Save this text file to local path to use it later in the pre-trained module
        if( (model_save_loc != None) and (file_path_model_files != None) ): # We should have already saved this file during the ml training phase (when user saved the model files ) itself because this file is used during the pre-trained module - so get the file location to wherever the user decided to save it and overwrite the 'FINAL_COLS_USED.txt' contents with the updated contents, if the user did not save it (if both 'file_path_model_files' and 'model_save_loc' are none ) then discard this line
            cols_textfile.to_csv(file_path_model_files + model_save_loc + '/' + str(col_textfile_name),index=False)   # Overwrite and save as csv to the user selected location

        return jsonify( { 'data':jsonfiles_full_river_preds, 'output_file_name':preds_file_name } )























    
    # # Warp all other rasters except the first (rgb) raster end

    # # only_band_names - has only band names
    # src = rasterio.open( raster_filename )
    # print(src.meta)
    
    # band1 = src.read()
    # # Don't just read first band as 0 - it could be alpha or other band
    # first_band = band_name_number[sel_cols3[3]] - 1
    # band1=band1[first_band]
    
    # nodata_val = float(src.nodata)
    # print(nodata_val)

    # # Get xstart,ystart, xsize, ysize, res
    # xmin = src.transform[2]
    # ymax = src.transform[5]
    # xsize = src.width
    # ysize = src.height
    # xstart = xmin +src.transform[0]/2
    # ystart = ymax - src.transform[0]/2
    # res = src.transform[0]

    # #xi and yi must describe a regular grid, can be either 1D or 2D, but must be monotonically increasing.
    # x = np.arange(xstart, xstart+xsize*res, res)
    # y = np.arange(ystart, ystart-ysize*res, -res)

    # another_x = np.arange(0,len(x),1,dtype=int)
    # another_y = np.arange(0,len(y),1,dtype=int)

    # del x,y
    # gc.collect()

    # print(another_x.shape)
    # print(another_y.shape)

    # X,Y = np.meshgrid(another_x,another_y,copy=False)

    # del another_x,another_y
    # gc.collect()

    # save_this_df = pd.DataFrame(columns=['East_generated','North_generated'])
    # save_this_df['East_generated'] = X.flatten()
    # save_this_df['North_generated'] = Y.flatten()
    # print('With all nan and values ', save_this_df.shape)
    # gc.collect()
    # del x,y
    
    # # # here get nodata values and remove
    # mr = np.ma.masked_equal(band1,nodata_val)

    # new_x = np.ma.masked_array(X, mr.mask)
    # new_y = np.ma.masked_array(Y, mr.mask)

    # del X,Y
    # gc.collect()
    # new_x = np.ma.compressed(new_x)
    # new_y = np.ma.compressed(new_y)

    # gc.collect()

    # df = pd.DataFrame(columns=['East_generated','North_generated'])
    # df['East_generated']=np.array(new_x)
    # df['North_generated']=np.array(new_y)
    
    # print('With no nan values ', df.shape)
    # del new_x,new_y
    # gc.collect()

    # for i in sel_cols3[3:6]:
    #     band_number = band_name_number[i] - 1
    #     band1 = src.read()[band_number]
    #     new_band1 = np.ma.masked_array(band1, mr.mask)
    #     del band1
    #     new_band1 = np.ma.compressed(new_band1)
    #     df[i]=np.array(new_band1)

    # del new_band1
    # gc.collect()

    # df = df.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first')
    # df = df.reset_index().drop(['index'],axis=1) 
    # print(df.head())

    # ## DONT DO ASTYPE TO FLOAT - CHANGES MERGE SYSTEM
    # ## DIFFERET COMPUTERS HAVE DIFFERENT ARRAY FLOATING PRECISIONS - SO CARFULLY CONVER THEM
    
    # ## Now for rest of the bands
    
    # for i in sel_cols3[6:]:

    #     # for multiple files
    #     band_numb = band_name_number[i]
    #     total_count = 0
    #     determined_band_numb = 0
    #     loop_done=0

    #     # determine which file it belongs to
    #     for fl in full_files_name:
    #         if(loop_done==0):
    #             fl_count = rasterfile_bands_dict[fl]
    #             total_count = total_count + fl_count
    #             print(fl_count, fl,i, band_numb, total_count )
    #             if( (band_numb > total_count) == False ):
    #                 file_name_val = fl

    #                 # determine the band number in that file name
    #                 total_count = total_count - fl_count
    #                 for k in range( fl_count ):
    #                     if( band_numb == ( total_count + (k+1) ) ):
    #                         src = None
    #                         determined_band_numb = k + 1
    #                         print(determined_band_numb,file_name_val)
                            
    #                         ###GDAL WARP WITH BOUND BOXES - to align and make the tif match coordinates of the rgb raster
    #                         ds = gdal.Open(full_files_name[0])
    #                         lt = list(ds.GetGeoTransform())
    #                         ulx, xres, xskew, uly, yskew, yres = ds.GetGeoTransform()
    #                         lrx = ulx + (ds.RasterXSize*xres)
    #                         lry = uly + (ds.RasterYSize*yres)
    #                         prj=str(ds.GetProjection())
    #                         ds= None

    #                         ds = gdal.Open(file_name_val,1)
    #                         src = gdal.Warp("", ds,format="vrt",
    #                           outputBounds=[ ulx, lry, lrx, uly ], 
    #                           xRes=xres, yRes=xres,
    #                           resampleAlg=gdal.GRA_NearestNeighbour, options=['COMPRESS=DEFLATE'])
    #                         ds = None

    #                         band1=src.GetRasterBand(determined_band_numb).ReadAsArray()

    #                         xmin = src.GetGeoTransform()[0]
    #                         ymax = src.GetGeoTransform()[3]
    #                         xsize = src.RasterXSize
    #                         ysize = src.RasterYSize
    #                         xstart = xmin +src.GetGeoTransform()[1]/2
    #                         ystart = ymax - src.GetGeoTransform()[1]/2
    #                         res = src.GetGeoTransform()[1]

    #                         x = np.arange(xstart, xstart+xsize*res, res)
    #                         y = np.arange(ystart, ystart-ysize*res, -res)
    #                         #xi and yi must describe a regular grid, can be either 1D or 2D, but must be monotonically increasing.
                            
    #                         another_x = np.arange(0,len(x),1,dtype=int)
    #                         another_y = np.arange(0,len(y),1,dtype=int)

    #                         del x,y
    #                         gc.collect()

    #                         print(another_x.shape)
    #                         print(another_y.shape)
                            
    #                         X1,Y1 = np.meshgrid(another_x,another_y,copy=False)

    #                         del another_x,another_y
    #                         gc.collect()

    #                         nodata_val = band1[0][0]
    #                         print(nodata_val)

    #                         # # here get nodata values and remove
    #                         mr = np.ma.masked_equal(band1,nodata_val)

    #                         new_x = np.ma.masked_array(X1, mr.mask)
    #                         new_y = np.ma.masked_array(Y1, mr.mask)
    #                         del X1,Y1
    #                         gc.collect()
                            
    #                         new_x = np.ma.compressed(new_x)
    #                         new_y = np.ma.compressed(new_y)
    #                         gc.collect()
    #                         print(new_x.shape)
    #                         print(new_y.shape)

    #                         temp_df = pd.DataFrame(columns=['East_generated','North_generated',i])
    #                         temp_df['East_generated']=np.array(new_x)
    #                         temp_df['North_generated']=np.array(new_x)
    #                         del new_x,new_y
    #                         gc.collect()

    #                         new_band1 = np.ma.masked_array(band1, mr.mask)
    #                         del band1
    #                         new_band1 = np.ma.compressed(new_band1)
    #                         temp_df[i]=np.array(new_band1)

    #                         temp_df = temp_df.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first')
    #                         temp_df=temp_df.reset_index().drop(['index'],axis=1) 

    #                         df = pd.merge(df, temp_df,  how='inner', on=['East_generated','North_generated'])
    #                         print(df.head())
    #                         # new_band1 = np.ma.masked_array(band1, mr.mask)
    #                         # del band1
    #                         # new_band1 = np.ma.compressed(new_band1)
    #                         # df[i]=np.array(new_band1)
    #                         loop_done = 1
    #                         del temp_df
    #                         gc.collect()
                            

    
    # #Just to maintain original values of orthophoto
    # raster_filename = full_files_name[0]
    # src = rasterio.open( raster_filename )
    # nodata_val = float(src.nodata)
    # del src
    # gc.collect()
    
    # df=df.reset_index().drop(['index'],axis=1) 
    # df['ID'] = np.array(df.index)
    # print('After all bands')
    # print(df.head())
    # print(df.shape)

    # ## This df is the final full-data for processing
    # full_data = split_dataframe(df) 
    # del df
    # if("best_options" in process_options):
    #     print("in best options")
    #     full_river=pd.DataFrame()
    #     # data_reference=load_data('data_for_all_models.csv')
    #     if(ml_algo_name=="Technique 1"):
    #         model=joblib.load('model_files/'+'Technique 1.pkl')
    #         col_name="Technique 1"

    #     if(ml_algo_name=="Technique 5"):
    #         model=joblib.load('model_files/'+'Technique 5.pkl')
    #         col_name="Technique 5"

    #     if(ml_algo_name=="Technique 2"):
    #         model=joblib.load('model_files/'+'Technique 2.pkl')
    #         col_name="Technique 2"

    #     if(ml_algo_name=="Technique 3"):
    #         model=joblib.load('model_files/'+'Technique 3.pkl')
    #         col_name="Technique 3"

    #     if(ml_algo_name=="Technique 4"):
    #         model=joblib.load('model_files/'+'Technique 4.pkl')
    #         col_name="Technique 4"
    #     z=0
    #     option_csv=pd.read_csv('model_files/'+'option_csv.csv')
    #     process_options=[]
    #     for i in range(len(option_csv)):
    #         process_options.append(option_csv.iloc[i]['best_option'])
    #     print(process_options)
    #     for jun in full_data:
    #         jun=jun.dropna()
    #         processed_data=jun.copy()
    #         z=z+1
    #         print("cluster ",z)
    #         print(processed_data.index[0],processed_data.index[-1])
    #         if(actual_status=="Yes"):
    #             predi=pd.DataFrame()
    #             predi[sel_tar]=processed_data[sel_tar].copy()
    #         elif(actual_status=="No"):
    #             predi=pd.DataFrame()

    #         to_merge=pd.DataFrame()
    #         merged=pd.DataFrame()
    #         to_merge=pd.DataFrame(processed_data[[pid,east,north]])

    #         # other selected colourspaces add
    #         # check if 0 value is present in r,g,b
    #         r=sel_cols3[3] #3 because of east north corrd input 
    #         g=sel_cols3[4]
    #         b=sel_cols3[5]   

    #         new_sel_cols3 = sel_cols3.copy()

    #         if(processed_data.describe()[r]['min']==0):                   
    #             processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
    #         if(processed_data.describe()[g]['min']==0):                   
    #             processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
    #         if(processed_data.describe()[b]['min']==0):                   
    #             processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

    #         if(processed_data.describe()[r]['min']<0):                   
    #             processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
    #         if(processed_data.describe()[g]['min']<0):                   
    #             processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
    #         if(processed_data.describe()[b]['min']<0):                   
    #             processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

    #         if( 'rgb' in colourspaces and 'hsv' not in colourspaces ):
    #             print('same dataset - no hsv only rgb')

    #         elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ):
    #             print('same dataset - no hsv only rgb')

    #         elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ):
    #             print('change in dataset - no rgb only hsv')
    #             hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1)
    #             processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index )
    #             processed_data[r] = processed_data2['H_generated'].copy()
    #             processed_data[g] = processed_data2['S_generated'].copy()
    #             processed_data[b] = processed_data2['V_generated'].copy()

    #             processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'})
    #             for ix, valx in enumerate(new_sel_cols3):
    #                 if(valx == r):
    #                     new_sel_cols3[ix] = 'H_generated'
    #                 if(valx == g):
    #                     new_sel_cols3[ix] = 'S_generated'
    #                 if(valx == b):
    #                     new_sel_cols3[ix] = 'V_generated'
                
    #             # check if 0 value is present in r,g,b
    #             if(processed_data.describe()['H_generated']['min']==0):                   
    #                 processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
    #             if(processed_data.describe()['S_generated']['min']==0):                   
    #                 processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
    #             if(processed_data.describe()['V_generated']['min']==0):                   
    #                 processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

    #             del processed_data2

    #         elif( 'rgb' in colourspaces and 'hsv' in colourspaces ):
    #             print('change in dataset - rgb and hsv')
    #             hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1)
    #             processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index )
    #             processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy())
    #             processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy())
    #             processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy())

    #             #insert in new_sel_cols3 
    #             new_sel_cols3.insert(6, 'H_generated')
    #             new_sel_cols3.insert(7, 'S_generated')
    #             new_sel_cols3.insert(8, 'V_generated') 

    #             # check if 0 value is present in r,g,b
    #             if(processed_data.describe()['H_generated']['min']==0):                   
    #                 processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
    #             if(processed_data.describe()['S_generated']['min']==0):                   
    #                 processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
    #             if(processed_data.describe()['V_generated']['min']==0):                   
    #                 processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

    #             del processed_data2

    #         r=send_feats_train[3]
    #         g=send_feats_train[4]
    #         b=send_feats_train[5]

    #         sel_cols2=new_sel_cols3[3:]    
    #         processed_data=processed_data[sel_cols2].copy()
    #         old_cols=new_sel_cols3[3:]
    #         print(old_cols, new_cols)
    #         for j in range(len(old_cols)):
    #             processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True)
            

    #         print("start",processed_data.shape)
    #         if("logperm_gen" in process_options):
    #             processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
    #             print("start1",processed_data.shape)

    #         if("greyscale_gen" in process_options):
    #             processed_data=greyscale_gen_full(processed_data,r,g,b)
    #             print("start2",processed_data.shape)

    #         if("cluster_gen" in process_options):
    #             processed_data=cluster_data_full(processed_data,r,g,b)  
    #             print("start3",processed_data.shape)

    #         if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)):
    #             processed_data=poly_creation_cluster_full(processed_data)    
    #             print("start4",processed_data.shape)

    #         if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)):
    #             processed_data=poly_creation_no_cluster_full(processed_data)     
    #             print("start4",processed_data.shape)

    #         if(("ss_scale" in process_options) & ("cluster_gen" in process_options)):
    #             processed_data=standard_scale_cluster_full(processed_data,"common")
    #             print("start5",processed_data.shape)

    #         if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)):
    #             processed_data=standard_scale_no_cluster_full(processed_data,"common")    
    #             print("start5",processed_data.shape)
            
    #         if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)):
    #             processed_data=min_max_scale_cluster_full(processed_data,"common")
    #             print("start6",processed_data.shape)

    #         if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)):
    #             processed_data=min_max_scale_no_cluster_full(processed_data,"common")    

    #         if(("robust_scale" in process_options) & ("cluster_gen" in process_options)):
    #             processed_data=robust_scale_cluster_full(processed_data,"common")

    #         if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)):
    #             processed_data=robust_scale_no_cluster_full(processed_data,"common")    

    #         if(("power_scale" in process_options) & ("cluster_gen" in process_options)):
    #             processed_data=power_scale_cluster_full(processed_data,"common")

    #         if(("power_scale" in process_options) & ("cluster_gen" not in process_options)):
    #             processed_data=power_scale_no_cluster_full(processed_data,"common")  

    #         if("pearson_correl" in process_options):
    #             processed_data=correl_full(processed_data,"common")
    #             print("start7",processed_data.shape)

    #         if("poly_lasso_trans" in process_options):
    #             lasso_result=pd.read_csv('model_files/'+'lasso_reg.csv')
    #             processed_data=lasso_reg_full(processed_data,lasso_result)
    #             print("start8",processed_data.shape)

    #         if(("pca_trans" in process_options) & ("cluster_gen" in process_options)):
    #             processed_data=pca_reduction_cluster_full(processed_data,"common")    

    #         if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)):
    #             processed_data=pca_reduction_no_cluster_full(processed_data,"common")

    #         print(processed_data.tail())
    #         print("model",model,ml_algo_name)
    #         if(ml_algo_name=="Technique 4"):
    #             option_csv_df=pd.read_csv('model_files/'+'option_csv.csv')
    #             options_csv=option_csv_df['best_option']
    #             no_scale_check=0
    #             options_values=list(option_csv_df['best_option'])[0]
    #             if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ):
    #                 no_scale_check=1 # atleast one scaling
    #             if(no_scale_check==0):
    #                 sc_t4=joblib.load('model_files/'+'sc_forT4only.pkl')
    #                 processed_data=sc_t4.transform(processed_data)
    #                 print('no scale')
    #         predi[str(col_name)+'_Pred']=model.predict(processed_data)
    #         df_preds2=pd.DataFrame(data=predi[str(col_name)+'_Pred'].values,columns= [str(col_name)+'_Pred'], index=to_merge.index)
    #         if(actual_status=="Yes"):
    #             df_preds2[sel_tar]=predi[sel_tar]   
            
    #         merged=pd.concat([to_merge,df_preds2],axis=1)
    #         merged=merged.reset_index().drop(['index'],axis=1) 
    #         full_river=pd.concat([full_river,merged],axis=0)
    #         full_river=full_river.reset_index().drop(['index'],axis=1) 
    #         del merged
    #         del df_preds2
    #         del predi,processed_data

    #     del full_data

    #     full_river=full_river.sort_values(by=pid)
    #     full_river=full_river.reset_index().drop(['index'],axis=1) 
    #     # full_river = full_river.sort_values(by = ["North_generated", "East_generated"], ascending = [False, True])
    #     # full_river=full_river.reset_index().drop(['index'],axis=1) 

    #     print("Sample of the output")
    #     print(full_river.head())

    #     jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records'))
    #     preds_file_name='Predictions_'+str(col_name).lower()+'.tif'

    #     ## Convert back to raster tif file
    #     full_river = full_river.drop(['ID'],axis=1)
        
    #     full_river = full_river.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first')
    #     full_river = full_river.reset_index().drop(['index'],axis=1)

    #     # save_this_df = pd.merge(save_this_df, full_river,  how='left', on=['East_generated','North_generated']).fillna(float(nodata_val))
    #     mdf=pd.DataFrame()
    #     chks = split_dataframe(save_this_df)
    #     del save_this_df

    #     for i in chks:
    #       print(i.shape)
    #       mdf1 = pd.merge(i, full_river,  how='left', on=['East_generated','North_generated']).fillna(float(nodata_val))
    #       mdf = pd.concat([mdf, mdf1 ],axis=0)
    #       del i,mdf1
    #       gc.collect()
    #       mdf = mdf.reset_index().drop(['index'],axis=1)

    #     del chks,full_river
    #     gc.collect()
    #     print(mdf.shape)
    #     print(mdf.head())

    #     # del full_river
    #     # gc.collect()
    #     # save_this_df = save_this_df.fillna(float(nodata_val))
    #     print('merge done')
        
    #     # save_this_df[ str(col_name)+'_Pred' ] = save_this_df[ str(col_name)+'_Pred' ].fillna(float(nodata_val))
    #     # print(save_this_df.head())
    #     # print(save_this_df.shape)
    #     # mdf = mdf.sort_values(by = ["North_generated", "East_generated"], ascending = [False, True]) # Crashes for large dataframes
    #     mdf = mdf.reset_index().drop(['index'],axis=1) 
    #     mdf = np.array( mdf[ str(col_name)+'_Pred' ].values, dtype=np.float64 )
    #     # save_this_df = np.array(save_this_df.sort_values(by = ["North_generated", "East_generated"], ascending = [False, True])[ str(col_name)+'_Pred' ].values, dtype=np.float32)

    #     import rasterio as rio   
    #     with rio.open(raster_filename) as src:
    #         ras_data = src.read()
    #         ras_meta = src.profile

    #     # make any necessary changes to raster properties, e.g.:
    #     ras_meta['dtype'] = "float64"
    #     ras_meta['nodata'] = nodata_val
    #     ras_meta['count'] = 1
    #     inDs = gdal.Open(raster_filename)
    #     ras_meta['crs'] = inDs.GetProjection()
    #     print(ras_meta)

    #     with rio.open("model_files/"+preds_file_name, 'w', **ras_meta) as dst:
    #         dst.write(mdf.reshape(ysize,xsize), 1)

    #     dst = None
    #     #if you get errors with proj.db or with crs - then use this:
    #     # inDs = gdal.Open(raster_filename)
    #     # ras_meta['crs'] = inDs.GetProjection()
    #     del mdf
    #     gc.collect()
        
       
    #     if(actual_status=="No"):
    #         model_textfile_name="FINAL_MODEL_USED.txt"
    #         if(os.path.exists('model_files/'+str(model_textfile_name))):
    #             os.remove('model_files/'+str(model_textfile_name))

    #         model_textfile=pd.DataFrame()
    #         print(ml_algo_name)
    #         model_textfile['Model Name']=[str(ml_algo_name)]
    #         model_textfile.to_csv('model_files/'+str(model_textfile_name),index=False)    

    #         # Save columns chosen for full river
    #         col_textfile_name="FINAL_COLS_USED.txt"
    #         if(os.path.exists('model_files/'+str(col_textfile_name))):
    #             os.remove('model_files/'+str(col_textfile_name))
    #         cols_textfile=pd.DataFrame(columns=['Columns'])
    #         ALL_COLS=''
    #         print("selcols3",sel_cols3)
    #         for i in sel_cols3:
    #            ALL_COLS+= str(i) + ', '
    #         cols_textfile.loc[0]=ALL_COLS
    #         cols_textfile.to_csv('model_files/'+str(col_textfile_name),index=False) 

    #         return jsonify( { 'data':jsonfiles_full_river_preds, 'output_file_name':preds_file_name } )

            
@app.route('/save_model_files_tif', methods=['POST','GET'])
def save_model_files_tif():
    model_loc_obj= request.form['model_save_loc']
    model_loc=json.loads(model_loc_obj)

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    algo_choice_obj= request.form['ml_algo_name']
    algo_choice=json.loads(algo_choice_obj)
    print(algo_choice)

    if os.path.exists(model_loc):
        shutil.rmtree(model_loc)
        
    # shutil.copytree('model_files',model_loc)

    file_path=file_path+model_loc
    shutil.copytree('model_files',file_path)
    
    return jsonify("Done")


@app.route('/save_files_tif', methods=['POST','GET'])
def save_files_tif():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    
    file_name_obj= request.form['file_name']
    file_name=json.loads(file_name_obj)

    shutil.copy('tif_processing/'+ file_name,file_path)
    #os.remove( 'model_files/'+ file_name )

    return jsonify("Done")



























#### SECTION 12: PRE-TRAINED MODULE FUNCTIONS - TO PROCESS FULL-DATA AND APPLY PRE-TRAINED MODELS TO GET OUTPUT PREDICTIONS

#### FOR APPLY PRETRAINED FUNCTIONS ####


@app.route('/load_model_files',methods=['POST','GET'])
def load_model_files():
    
    model_files_name_obj=request.form['model_files_name'] # Get the folder path as a json object- the user loads in a folder that contains all pre-trained models and other object files, that are needed to apply on full-data csv
    model_files_name=json.loads(model_files_name_obj) # Convert the json object to list
    
    store_pretrained_path='apply_pretrained_model_storage' # This will the folder we create while initializing the pre-trained module and all files from the user's input folder will be copied here
    print(model_files_name)
    dir_name=model_files_name[0] # First value - there is only one value as input (one input folder at a time)

    if os.path.exists(store_pretrained_path): # If this folder already exists
        shutil.rmtree(store_pretrained_path, ignore_errors=True)   # Delete that folder
    if not os.path.exists(store_pretrained_path): # If the folder doesn't exist
        os.mkdir(store_pretrained_path) # Then create it - this is the folder where all the model files and other object files will be stored
    
    loaded_files=[] # Initialize empty list
    for filename in os.listdir(dir_name): # For all files in the input folder
        if (filename!="full_river_data_software_generated.csv" and filename!="all_models_preds.csv" and filename!="data_for_all_models.csv" and filename!="Predictions_technique 1.csv" and filename!="Predictions_technique 2.csv" and filename!="Predictions_technique 3.csv" and filename!="Predictions_technique 4.csv" and filename!="Predictions_technique 5.csv" and filename!="Technique 1_pred_temp.csv" and filename!="Technique 2_pred_temp.csv" and filename!="Technique 3_pred_temp.csv" and filename!="Technique 4_pred_temp.csv" and filename!="Technique 5_pred_temp.csv" and filename!="temp_pipeline.csv" and filename!="temp_pipeline_2.csv" and filename!="x_y.csv"): # If the file names match with these files - we do not need to copy these files as they are not useful at this stage and also they might sometimes be very large in size
            full_filename= dir_name + '/' + filename  # For all other files, get the full file path
            loaded_files.append(full_filename) # Append those file names to the list
            shutil.copy(full_filename, store_pretrained_path+'/'+filename) # Copy these files from the user location to local app's folder - under 'apply_pretrained_model_storage' folder

    print(len(loaded_files))

    # We perform a second scan to get all the technique model names that are present in the folder by checking the pkls and count them - also get the last used model name from the 'FINAL_MODEL_USED.txt' file and also get the last used feature names in the ml module by checking the 'FINAL_COLS_USED.txt' to highlight these values for the user to know which models they ran in the final step and which columns they used for trianing or full-data prediction in the ml module
    tech="Technique"
    tech_count=0 # Initialize count variable to 0
    tech_present=[] # To hold a list of all pre-trained technique names
    for i in range(1,6): # For all 5 techinques
        tech_name=dir_name + '/' + tech+" "+str(i)+".pkl" # Check if the file name matches with 'Technique_number.pkl' type - for each technique
        if(tech_name in loaded_files): # If there is a match
            tech_present.append(tech+" "+str(i)) # Add this technique name to the list
            tech_count+=1 # Increment the counter

    if(os.path.exists(store_pretrained_path + str("/FINAL_MODEL_USED.txt"))): # If this text file is present
        model_textfile=pd.read_csv( store_pretrained_path + str("/FINAL_MODEL_USED.txt") ) # Read the textfile as csv 
        final_used_ml_algo_name=str(model_textfile['Model Name'].iloc[0]) # Get the used model name from the first row under column 'Model_Name'

    else:
        final_used_ml_algo_name = "None" # If no such text file is found, there will be no highlighting in the technique names - so just keep as None value
    
    if(os.path.exists(store_pretrained_path + str("/FINAL_COLS_USED.txt"))): # If this text file is present
        cols_textfile=pd.read_csv( store_pretrained_path + str("/FINAL_COLS_USED.txt") ) # Read the textfile as csv
        final_used_cols=str(cols_textfile['Columns'].iloc[0]) # Get the used column names from the first row under column 'Columns'
        print(final_used_cols)

    else:
        final_used_cols=['None'] # If no such text file is found, there will be no highlighted column names - so just keep the list as None value

    # send the list of techniques present and their counts back to javascript
    print(tech_count,tech_present)
    return (jsonify({'tech_count':tech_count, 'tech_present':tech_present, 'final_used_ml_algo_name': final_used_ml_algo_name, 'final_used_cols': final_used_cols, 'file_name':loaded_files}))

@app.route('/wait_interval',methods=['GET','POST'])
def wait_interval():
    time.sleep(10)
    return jsonify("done")

        

# full river input and process
@app.route('/apply_data_input',methods=['POST'])
def apply_data_input():
    print("Merging")
    
    full_files_name_obj=request.form['full_files_name'] # get the file name - location of the full-data csv file. We don't copy the file to the local 'apply_pretrained_model_storage' folder because full-data is usually very large and we do not have to consume extra space, we can just get the file location path, and access/run functions on the dataset without having to copy the csv
    full_files_name=json.loads(full_files_name_obj)

    df=pd.read_csv(full_files_name[0], nrows=10)   # read first 10 rows and send to javascript to view in app page
    full_columns=df.columns # Get all the column names of the dataframe
    df_jsonfiles = json.loads(df.head().to_json(orient='records')) # Always load as json and then jsonify the data (next two lines) before sending - only way to communicate between python and javascript
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))


#full river predictions
@app.route('/apply_data_predictions',methods=['POST','GET'])
def apply_data_predictions(): # This function takes in the full-data csv, processes it with the same functions and feature engineering methods that were used on the training dataset. After processing, load the pre-trained model and its parameters, make predictions and store the output csv to the user's selected location
    ml_algo_obj=request.form['apply_ml_algo'] # Get the user's selected ml algorithm (NOT THE MODEL SELECTED IN TRAINING PHASE - A NEW SELECTION JUST FOR FULL-DATA ) to apply on full-data
    ml_algo_name=json.loads(ml_algo_obj) 
    model='' # Initialize an empty string
    sel_feats=request.form['feats'] # Columns selected for full-data
    sel_cols3 = json.loads(sel_feats)

    full_files_name_obj=request.form['full_files_name']
    full_files_name=json.loads(full_files_name_obj) # Get the full-data csv file path

    actual_status=request.form['actual_status'] # If an actual target variable is present or not
    # actual_status=json.loads(actual_status_obj)
    
    target_json=request.form['actual_variable']
    target=json.loads(target_json) # If actual target variable is present, then which variable is it
    print(target)

    original_feats_df=pd.read_csv('apply_pretrained_model_storage/'+'original_feats_train.csv') # To get the original feature names that was used during the training phase - we load this previously saved csv file, which contains all column names used for training. We need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
    original_feats=[] # Initialize empty list
    for i in range(len(original_feats_df)): # For all rows in the dataset
        original_feats.append(original_feats_df.iloc[i]['feats']) # Read and add the original feature names to the list - this should be in the same order as the user's input training features

    send_feats_train=original_feats # Make a copy of the original_feats list - this will be the training features used in the ml module
    new_cols=send_feats_train[3:-1]  # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)

    process_options_obj=request.form['process_options'] # Get the json object of what process_option value is - As mentioned already, as of the current version, the only value process_option can possibly have is 'best_options' - which is to automatically select the best feature engineering methods combination
    process_options=json.loads(process_options_obj) # Convert the json object to list

    pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols3[1] # Get east coordinate column name
    north=sel_cols3[2] # Get north coordinate column name
    
    r=send_feats_train[3] # Get R column name
    g=send_feats_train[4] # Get G column name
    b=send_feats_train[5] # Get B column name
    sel_cols2=sel_cols3[3:]
    
    if(actual_status=="Yes"): # Check if there is an actual target variable
        sel_tar=str(target) # Get the name of the target column
        print("sel tar done")
    if(actual_status=="No"): # Check if there is no actual target variable
        sel_tar="none"  # If no target variable, make this string to none


    # colourspaces_json=request.form['colourspaces']
    # colourspaces  = json.loads(colourspaces_json)
    csp_str = str( pd.read_pickle('apply_pretrained_model_storage/colourspaces.pkl').iloc[0]['colourspaces'] ) # Get what colourspace was used in training - there was a pickle file stored during the ml algorithm training phase - this file contains the user's selected colourspace options
    print("Colourspaces : ", csp_str)

    colourspaces=[] # Create empty list
    if(csp_str == 'rgb'): # If this string value read from the pkl file is 'rgb' - then the colourspace selected was just rgb colourpsace
        colourspaces.append('rgb') # Add the 'rgb' to the list

    if(csp_str == 'hsv'): # If this string value read from the pkl file is 'hsv' - then the colourspace selected was just hsv colourpsace
        colourspaces.append('hsv') # Add the 'hsv' to the list

    if(csp_str == 'rgbhsv'): # If this string value read from the pkl file is 'rgbhsv' - then the colourspace selected was both rgb and hsv colourpsace
        colourspaces.append('rgb') # Add the 'rgb' to the list
        colourspaces.append('hsv') # Add the 'hsv' to the list
        


    print("Process options",process_options)
    print("Actual status",actual_status)
    print(" Model name",ml_algo_name)
    print("features",sel_cols3)

    full_data=pd.read_csv(full_files_name[0], chunksize=1000000) # Read the full-data csv not as whole, but in chunks of size 10000000 each - batch processing by performing multiple iterations to complete all batches and then finally merge prediction values of all batches into one single csv and output to user
    
    if("best_options" in process_options): # When the value in process_options list is 'best_options'
        print("in best options")
        full_river=pd.DataFrame() # Initialize an empty dataframe
        # data_reference=load_data_apply_module('data_for_all_models.csv')
        if(ml_algo_name=="Technique 1"): # If the selected algorithm to apply on full-data is technique 1
            model=joblib.load('apply_pretrained_model_storage/'+'Technique 1.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 1" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 5"): # If the selected algorithm to apply on full-data is technique 5
            model=joblib.load('apply_pretrained_model_storage/'+'Technique 5.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 5" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 2"): # If the selected algorithm to apply on full-data is technique 2
            model=joblib.load('apply_pretrained_model_storage/'+'Technique 2.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 2" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 3"): # If the selected algorithm to apply on full-data is technique 3
            model=joblib.load('apply_pretrained_model_storage/'+'Technique 3.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 3" # Define a column name to add to the final dataframe

        if(ml_algo_name=="Technique 4"): # If the selected algorithm to apply on full-data is technique 4
            model=joblib.load('apply_pretrained_model_storage/'+'Technique 4.pkl')  # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
            col_name="Technique 4" # Define a column name to add to the final dataframe
        
        z=0
        option_csv=pd.read_csv('apply_pretrained_model_storage/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
        process_options=[] # Initialze an empty list
        for i in range(len(option_csv)): # For all the values in the option_csv dataset
            process_options.append(option_csv.iloc[i]['best_option']) # Append the values into the new list
        print(process_options)
        for jun in full_data: # For every batch ( chunk ) of the full-data, do all the below codes
            jun=jun.dropna() # Drop nan values if any
            processed_data=jun.copy() # Make a copy of the batch full-data
            z=z+1 # Keep track of the chunk or cluster or current batch number
            print("cluster ",z)
            print(processed_data.index[0],processed_data.index[-1])
            if(actual_status=="Yes"): # Check if there is an actual target variable
                predi=pd.DataFrame() # Initiate empty dataframe
                predi[sel_tar]=processed_data[sel_tar].copy() # If actual variable is present and selected by the user, then copy the target values and keep this as a separate dataframe and merge along with the predictions dataframe at the end
            elif(actual_status=="No"): # Check if there is no actual target variable
                predi=pd.DataFrame() # If no actual target variable, then just initiate empty dataframe

            to_merge=pd.DataFrame() # Empty dataframe to merge later
            merged=pd.DataFrame() # Empty dataframe to merge predictions of every batch processing iteration and the id, east and north of every batch-processing iteration - there is no batch processing of full-data in curve-fitting, but there is batch processing in simplified workflow since its computation load is huge
            to_merge=pd.DataFrame(processed_data[[pid,east,north]]) # Copy the id, east and north columns from the main dataframe to be processed, into the 'to_merge' dataframe - use this dataframe to merge with the  predictions dataframe at the end
            
            # other selected colourspaces add
            # check if 0 value is present in r,g,b
            r=sel_cols3[3] #3 because of east north corrd input. Get R column name 
            g=sel_cols3[4] # Get G column name
            b=sel_cols3[5] # Get B column name 
            
            new_sel_cols3 = sel_cols3.copy() # Keep a copy of the original sel_cols3 - which has the selected full-data features. The reason to keep a copy is in the next few steps - new features will be added to the list and hence we need to have the original features to extract initial columns from the next batch of the batch-processed full-data

            if(processed_data.describe()[r]['min']==0):     # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                
                processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
            if(processed_data.describe()[g]['min']==0):     # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                               
                processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
            if(processed_data.describe()[b]['min']==0):     # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                              
                processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

            if(processed_data.describe()[r]['min']<0):      # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                             
                processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
            if(processed_data.describe()[g]['min']<0):      # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                            
                processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
            if(processed_data.describe()[b]['min']<0):      # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                            
                processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

            if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
                print('same dataset - no hsv only rgb')

            elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
                print('same dataset - no hsv only rgb')

            elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
                print('change in dataset - no rgb only hsv')
                hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                processed_data[r] = processed_data2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data[g] = processed_data2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data[b] = processed_data2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
                for ix, valx in enumerate(new_sel_cols3): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
                    if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                        new_sel_cols3[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
                    if(valx == g):
                        new_sel_cols3[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
                    if(valx == b):
                        new_sel_cols3[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
                
                # check if 0 value is present in r,g,b
                if(processed_data.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                
                    processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                if(processed_data.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                              
                    processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                if(processed_data.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                             
                    processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                del processed_data2 # Delete the temporary dataframe

            elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
                print('change in dataset - rgb and hsv')
                hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                #insert in new_sel_cols3  - since we created new features, they should be added for further use in the next parts of the code
                new_sel_cols3.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                new_sel_cols3.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                new_sel_cols3.insert(8, 'V_generated')  # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

                # check if 0 value is present in r,g,b
                if(processed_data.describe()['H_generated']['min']==0):     # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                    
                    processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                if(processed_data.describe()['S_generated']['min']==0):     # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                  
                    processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                if(processed_data.describe()['V_generated']['min']==0):     # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                  
                    processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                del processed_data2 # Delete the temporary dataframe

            r=send_feats_train[3] # Get R column name - the R feature name in the INPUT TRAINING DATA
            g=send_feats_train[4] # Get G column name - the G feature name in the INPUT TRAINING DATA
            b=send_feats_train[5] # Get B column name - the B feature name in the INPUT TRAINING DATA

            sel_cols2=new_sel_cols3[3:]   # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
            processed_data=processed_data[sel_cols2].copy() # Create the main subset from the initial dataframe - this new dataframe would have only features from R,G,B, to other variables - NOT east, north, id etc, - note that there is no target variable here
            old_cols=sel_cols3[3:] # To rename the full-data column names to be same as the input training-data column names, get all the current full-data column names from r,g,b, till the last selected feature
            print(old_cols, new_cols)
            for j in range(len(old_cols)): # For every current full-data column name
                processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True) # Like mentioned before, need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
                
            print("start",processed_data.shape)
            if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
                processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                print("start1",processed_data.shape)

            if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
                processed_data=greyscale_gen_full(processed_data,r,g,b)
                print("start2",processed_data.shape)

            if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
                processed_data=cluster_data_full_pt(processed_data,r,g,b)  
                print("start3",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=poly_creation_cluster_full(processed_data)    
                print("start4",processed_data.shape)

            if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=poly_creation_no_cluster_full(processed_data)     
                print("start4",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=standard_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques
                print("start5",processed_data.shape)

            if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=standard_scale_no_cluster_full_pt(processed_data,"common")   # Common specifies same data for all ml techniques  
                print("start5",processed_data.shape)
            
            if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=min_max_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques
                print("start6",processed_data.shape)

            if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=min_max_scale_no_cluster_full_pt(processed_data,"common")     # Common specifies same data for all ml techniques

            if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=robust_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques

            if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=robust_scale_no_cluster_full_pt(processed_data,"common")    # Common specifies same data for all ml techniques 

            if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=power_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques

            if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=power_scale_no_cluster_full_pt(processed_data,"common")   # Common specifies same data for all ml techniques

            if("pearson_correl" in process_options): # This line checks if pearson_correl is in the process_options list 
                processed_data=correl_full_pt(processed_data,"common") # Common specifies same data for all ml techniques
                print("start7",processed_data.shape)

            if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
                lasso_result=pd.read_csv('apply_pretrained_model_storage/'+'lasso_reg.csv') # Load this csv which was stored when the poly_lasso_trans function ran while finding the best preprocessing methods 
                processed_data=lasso_reg_full(processed_data,lasso_result) # We pass the csv to copy the column names present in the lasso_result dataset - lasso_result dataset was saved after computing lasso reg during feature engineering phase
                print("start8",processed_data.shape)

            if(("pca_trans" in process_options) & ("cluster_gen" in process_options)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                processed_data=pca_reduction_cluster_full_pt(processed_data,"common")     # Common specifies same data for all ml techniques

            if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                processed_data=pca_reduction_no_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques

            print(processed_data.tail())
            print("model",model,ml_algo_name)

            # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
            if(ml_algo_name=="Technique 1" or ml_algo_name=="Technique 5"): # If the selected algorithm is technique 1 or if the selected algorithm is technique 5
                option_csv_df=pd.read_csv('apply_pretrained_model_storage/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
                options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                    no_scale_check_lr=1 # atleast one scaling
                if(no_scale_check_lr==0): # If none present, then do this
                    sc_t1_5=joblib.load('apply_pretrained_model_storage/'+'sc_forT1&5only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                    processed_data=sc_t1_5.transform(processed_data) # Scale and transform the processed full-data
                    print('no scale')

            if(ml_algo_name=="Technique 4"): # If the selected algorithm is technique 4
                option_csv_df=pd.read_csv('apply_pretrained_model_storage/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                no_scale_check=0 # Initialize a scaling check for nn (lr and svr already done)
                options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                    no_scale_check=1 # atleast one scaling
                if(no_scale_check==0): # If none present, then do this
                    sc_t4=joblib.load('apply_pretrained_model_storage/'+'sc_forT4only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                    processed_data=sc_t4.transform(processed_data) # Scale and transform the processed full-data
                    print('no scale')
            predi[str(col_name)+'_Pred']=model.predict(processed_data) # Predict depth values for the processed batch-data and etore the prediction values to the dataframe under the column name 'ALGO_NAME_pred' - ALGO_NAME is the chosen ml algorithm name
            df_preds2=pd.DataFrame(data=predi[str(col_name)+'_Pred'].values,columns= [str(col_name)+'_Pred'], index=to_merge.index) # Copy the same dataframe to a new one - with index from this batch's input full-data dataset - copy the index from this dataframe to this new dataframe so that the values are in correct order while merging all batches at the end 
            if(actual_status=="Yes"): # Check if there is an actual target variable
                df_preds2[sel_tar]=predi[sel_tar]    # If actual variable is present and selected by the user, then copy the target values into this new dataframe 
            
            merged=pd.concat([to_merge,df_preds2],axis=1) # Merge all columns and predicted values into one dataframe
            merged=merged.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
            full_river=pd.concat([full_river,merged],axis=0) # Merge the current batch values with the previous batch (if any) values - concats on row base, adds more rows after the previous batches' rows
            full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
            del merged # Delete the temporary dataframe
            del df_preds2 # Delete the temporary dataframe
            del predi # Delete the temporary dataframe

        del full_data # Delete the temporary dataframe

        full_river=full_river.sort_values(by=pid) # Sort the values of ID column
        full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the sorted dataframe and remove the index column
        print("Sample of the output")
        print(full_river.head())
        preds_file_name='Pretrained_Model_Predictions_'+str(col_name).lower()+'.csv' # This is the output file name - 'predictions' + ml algo name, and save as csv file 
        full_river.to_csv('apply_pretrained_model_storage/'+preds_file_name,index=False) # Save the final all merged output predictions csv file to the 'apply_pretrained_model_storage' folder
        full_river=full_river.round(3) # Round all values to 3 decimals - this is just for showing sample data - does not affect the decimal values in the stored csv file
        jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records')) # Save the first 5 rows of the input data with just selected independent features, as json object to show to the user using javascript  
        if(actual_status=='Yes'): # If actual target is present calculate the metrics between predicted and the actual target variables
            print(sel_tar)
            print(full_river.head())
            full_river_preds_metrics=calculate_metrics(ml_algo_name, full_river, sel_tar, str(col_name)) # Get the metric scores by passing in the prediction values and the actual target values
            full_river_preds_metrics_temp=calculate_metrics_temp(full_river, sel_tar, str(col_name)) # Get the metric scores by passing in the prediction values and the actual target values
            full_river_preds_metrics_temp.to_csv('apply_pretrained_model_storage/'+'pretrained_model_statistics.txt',index=False) # Save the final full-data metric scores as text file
            final_preds_full_concat=pd.concat([full_river_preds_metrics,full_river_preds_metrics_temp],axis=0) # Sometimes json object has problems sending data with just one row, so we add another row with same values as the first row
            final_preds_full_concat=final_preds_full_concat.round(3) # Round the metrics to 3 decimals
            
            jsonfiles_full_river_preds_metrics = json.loads(final_preds_full_concat[full_river_preds_metrics.columns].to_json(orient='records')) # Now create and send a json object for the metric scores - since there is an extra row with same values as first row, just send them both - and in javascript we show only one row
            del full_river # Delete the temporary dataframe
            return (jsonify({'preds':jsonfiles_full_river_preds,'metrics':jsonfiles_full_river_preds_metrics, 'preds_file_name':preds_file_name}))    
        if(actual_status=="No"): # If actual target is not present then just send the sample predictions to javascipt for the user to see the sample top 5 rows of the predictions dataset
            del full_river
            return (jsonify({ 'preds': jsonfiles_full_river_preds, 'preds_file_name': preds_file_name }))



@app.route('/save_model_files_pretrained', methods=['POST','GET'])
def save_model_files_pretrained(): # Copy the output predictions file of apply-pre-trained module to the user's preferred location

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)

    preds_file_name_obj= request.form['preds_file_name']
    preds_file_name=json.loads(preds_file_name_obj)
    
    shutil.copy('apply_pretrained_model_storage/'+ preds_file_name, file_path+'Pre-trained-Model-Predictions.csv')
    
    return jsonify("Done")















########## FOR TIF BASED INPUT FILE

# full river input and process
@app.route('/full_tif_input_pt',methods=['POST'])
def full_tif_input_pt(): # This function is to get the input raster files and send json object with details about raster band names, number of bands etc
    print("tif files mode")
    
    full_files_name_obj=request.form['full_files_name'] # Json object containing the tif file path name - it is a list of names - the first value is the rgb raster, the subsequent values are extra raster files
    full_files_name=json.loads(full_files_name_obj) # Convert the json list object to list of values
    count_raster = 0 # Initialize counter for number of raster files
    tif_files = [] # Initialize empty list to store all tif files
    rasterfile_bands_dict = {} # Initialize dict to store tif files info

    if os.path.exists('tif_processing'): # If this folder already exists
        shutil.rmtree('tif_processing', ignore_errors=True) # Delete that folder
    if not os.path.exists('tif_processing'): # If the folder doesn't exist
        os.mkdir('tif_processing') # Then create it - this is the folder where all the tif files and its split parts will be stored

    # Here full_files_name[0] is the rgb raster and the rest are additional feature rasters
  
    for file in full_files_name: # For every file in this list
        file_ext=str(str(file).split('.')[-1])   # Get its extension
        if(file_ext=="tif"): # Check if the extension is tif, if yes then continue
            count_raster+=1  # Keeps count of number of tif files imported
            tif_files.append( file ) # Append the tif file to the list
            #for each raster, get number of bands present
            if(len(tif_files)>0 ):
                raster=rasterio.open( file ) # Use rasterio to open and read a tif file
                bands= raster.count # get the band count of the tif file

                rasterfile_bands_dict[file] = int(bands) # Store the band count as the value and the filename as the key, to this dict

    print(rasterfile_bands_dict)
    return jsonify( { 'count_raster':count_raster, 'rasterfile_bands_dict':rasterfile_bands_dict, 'tif_files':tif_files } )

# # input - df: a Dataframe, chunkSize: the chunk size
# # output - a list of DataFrame
# # purpose - splits the DataFrame into smaller chunks 
# # FOR TIF BASED FILES
# def split_dataframe(df, chunk_size = 10000000): 
#     chunks = list()
#     num_chunks = len(df) // chunk_size + 1
#     for i in range(num_chunks):
#         chunks.append(df[i*chunk_size:(i+1)*chunk_size])
#     return chunks


@app.route('/full_river_predictions_tif_pt',methods=['POST','GET'])
def full_river_predictions_tif_pt(): # This function takes in the full-data tif, splits into several tif files, batch processes them with the same functions and feature engineering methods that were used on the training dataset. After processing, load the trained model / trained curve-fit parameters, make predictions on each tif, and after all batches, merge all separate output tif files into one tif file and store that output tif to the user's selected location

    # Remove all output tifs if present
    import glob,os
    demList1 = glob.glob("tif_processing/output_[0-9]_part.tif") # Get all the list of files with [0-9] (1-10) names
    demList2 = glob.glob("tif_processing/output_[0-9][0-9]_part.tif") # Get all the list of files with [0-9][0-9] (10-100) names
    demList3 = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Get all the list of files with [0-9][0-9][0-9] (100-400) names

    demList=[] # Initialize empty list
    for i in demList1: # For all file names in this list
        demList.append(i) # Append to the main list
    for i in demList2: # For all file names in this list
        demList.append(i) # Append to the main list
    for i in demList3: # For all file names in this list
        demList.append(i) # Append to the main list

    for i in demList: # For all file names in the final list
        os.remove(i) # If there are any files with the names present in this list, remove them files


    ml_algo_obj=request.form['full_ml_algo'] # Get what ml algorithm the user has selected to apply on full-data - this is a json object
    ml_algo_name=json.loads(ml_algo_obj) # Convert the json object to string
    model=''
    
    full_files_name_obj=request.form['tif_files'] # Get the tif files path as json list object
    full_files_name=json.loads(full_files_name_obj) # Convert the json object to list
    
    sel_feats=request.form['feats'] # Columns selected for full-data as json object
    sel_cols3 = json.loads(sel_feats) # Convert the json object to list
    
    actual_status=request.form['actual_status'] # If an actual target variable is present or not - for TIF based full-data, we have kept no actual status value by default
    # actual_status=json.loads(actual_status_obj)

    original_feats_df=pd.read_csv('apply_pretrained_model_storage/'+'original_feats_train.csv') # To get the original feature names that was used during the training phase - we load this previously saved csv file, which contains all column names used for training. We need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
    original_feats=[] # Initialize empty list
    for i in range(len(original_feats_df)): # For all rows in the dataset
        original_feats.append(original_feats_df.iloc[i]['feats']) # Read and add the original feature names to the list - this should be in the same order as the user's input training features


    send_feats_train=original_feats # Make a copy of the original_feats list - this will be the training features used in the ml module
    new_cols=send_feats_train[3:-1] # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)

    process_options_obj=request.form['process_options'] # Get the json object of what process_option value is - As mentioned already, as of the current version, the only value process_option can possibly have is 'best_options' - which is to automatically select the best feature engineering methods combination
    process_options=json.loads(process_options_obj) # Convert the json object to list

    pid=sel_cols3[0] # Get the ID column name (first item in the column names' list)
    east=sel_cols3[1] # Get east coordinate column name
    north=sel_cols3[2] # Get north coordinate column name
    
    r=send_feats_train[3] # Get R column name
    g=send_feats_train[4] # Get G column name
    b=send_feats_train[5] # Get B column name
    
    if(actual_status=="Yes"): # Check if there is any actual target variable - FOR TIF based full-data functions, there will be no option for the user to enter target variable
        sel_tar=str(target)
        print("sel tar done")
    if(actual_status=="No"): # Check if there is no actual target variable
        sel_tar="none"  # If no target variable, make this string to none

    # colourspaces_json=request.form['colourspaces']
    # colourspaces  = json.loads(colourspaces_json)
    csp_str = str( pd.read_pickle('apply_pretrained_model_storage/colourspaces.pkl').iloc[0]['colourspaces'] ) # Get what colourspace was used in training - there was a pickle file stored during the ml algorithm training phase - this file contains the user's selected colourspace options
    print("Colourspaces : ", csp_str)

    colourspaces=[] # Create empty list
    if(csp_str == 'rgb'): # If this string value read from the pkl file is 'rgb' - then the colourspace selected was just rgb colourpsace
        colourspaces.append('rgb') # Add the 'rgb' to the list

    if(csp_str == 'hsv'): # If this string value read from the pkl file is 'hsv' - then the colourspace selected was just hsv colourpsace
        colourspaces.append('hsv') # Add the 'hsv' to the list

    if(csp_str == 'rgbhsv'): # If this string value read from the pkl file is 'rgbhsv' - then the colourspace selected was both rgb and hsv colourpsace
        colourspaces.append('rgb') # Add the 'rgb' to the list
        colourspaces.append('hsv') # Add the 'hsv' to the list
        

    print("Process options",process_options)
    print("Actual status",actual_status)
    print(" Model name",ml_algo_name)
    print("features",sel_cols3)

    # Get band names
    band_name_number_json=request.form['band_name_number'] # Get json object for the band details dict that contains the band names and band number in the order of input files - first would be the rgb raster band names and then band name and its number for other rasters
    band_name_number  = json.loads(band_name_number_json) # Convert the json object to dict

    print(band_name_number)

    rasterfile_bands_dict_json=request.form['rasterfile_bands_dict'] # This variable has all the details of each band of every raster file
    rasterfile_bands_dict  = json.loads(rasterfile_bands_dict_json)

    print(rasterfile_bands_dict)


    # full_data=pd.read_csv(full_files_name[0], chunksize=1000000)
    #### Create csv from tif file
    import os,gc
    import gdal
    import geopandas as gpd
    from osgeo import gdal
    import rasterio

    # Read only RGB raster first
    raster_filename = full_files_name[0]
    print(raster_filename)
    
    output_count=0 # Initialze this to 0 - this is a counter to keep count of the output file number for every split


    # Warp all other rasters except the first (rgb) raster
    from osgeo import gdal
    dem = gdal.Open(raster_filename) # get raster file dem
    gt = dem.GetGeoTransform() # get extents
    print(gt)
    # get coordinates of upper left corner
    xmin = gt[0]
    ymax = gt[3]
    res_all = gt[1]

    ulx, xres, xskew, uly, yskew, yres = dem.GetGeoTransform()
    lrx = ulx + (dem.RasterXSize*xres) # Calculate lower right and bottom coordinates
    lry = uly + (dem.RasterYSize*yres)

    rastfile_warp = {}
    # Save the raster object in a dict with filename as key
    for rast_file in full_files_name: # For all files in the list
        if(rast_file!=raster_filename): # Proceed only if the file name is not the first raster file - which is the rgb raster
            print(" Warping raster file ", rast_file)
            # Warp the chainage raster
            # We are warping the chainage raster to match its extents and projection to that of the main rgb raster - only if they match, we can extract band values on those matching coordinates
            ds = gdal.Open(rast_file,1) # Use gdal to open and read the raster file
            src=None # Make this to none first
            src = gdal.Warp("", ds,format="vrt",
                outputBounds=[ ulx, lry, lrx, uly ], 
                xRes=res_all, yRes=-res_all,
                resampleAlg=gdal.GRA_NearestNeighbour, options=['COMPRESS=DEFLATE']) # This function warps the raster to match with the given output bounds and resolution. 
            print(src.GetGeoTransform())

            rastfile_warp[str(rast_file)] = src # Save each dem object of each raster file to a list and use it later
            src = None # Make it none and use this for next raster file in next loop
            ds = None # Make it none and use this for next raster file in next loop

    # determine total length of raster
    xlen = res_all * dem.RasterXSize
    ylen = res_all * dem.RasterYSize

    # number of tiles in x and y direction
    div = 20 # Time taken for the whole process till end:  For large data (500mb input raster file) - 30 div = 3590 sec, 15 div = 3000 sec, 10 div = 3300 sec. For small data (50mb input raster file) - 5 div = 343 sec, 15 div = 299 sec, 50 div = 817 sec, 30 div = 400 sec
    # ydiv = 2

    # size of a single tile
    xsize = xlen/div
    ysize = ylen/div
    
    # create lists of x and y coordinates
    xsteps = [xmin + xsize * iq for iq in range(div+1)]
    ysteps = [ymax - ysize * iq for iq in range(div+1)]
    
    
    # loop over min and max x and y coordinates
    for ii in range(div):
        for jj in range(div):
            xmin_all = xsteps[ii] # Get initial end of the current split-square x coordinate (left-top corner)
            xmax_all = xsteps[ii+1] # Get far end of the current split-square x coordinate (right-top corner)
            ymax_all = ysteps[jj] # Get initial end of the current split-square y coordinate (left-bottom corner)
            ymin_all = ysteps[jj+1] # Get far end of the current split-square y coordinate (right-bottom corner)
            
            print("xmin: "+str(xmin_all))
            print("xmax: "+str(xmax_all))
            print("ymin: "+str(ymin_all))
            print("ymax: "+str(ymax_all))
            print("\n")

            output_count += 1 # For every square (split) or for every loop, increment this counter - represents the output number of each tif split part
            main_raster_obj = None # Main raster - rgb raster
            other_raster_obj = None # Other rasters - chainage, weight etc.  rasters

            main_raster_obj = gdal.Translate("tif_processing/input_"+ str(output_count) +"_part.tif", dem, projWin = (xmin_all, ymax_all, xmax_all, ymin_all), xRes = res_all, yRes = -res_all, creationOptions=["COMPRESS=DEFLATE"]) # Split into a small part
            # First get RGB values from main raster (first input raster is always the main raster)

            nodata_val = main_raster_obj.GetRasterBand(1).GetNoDataValue() # Get the nodata value of the rgb raster
            print(nodata_val)
            if(nodata_val==None): # If nodata value is none then make it 256 for rgb raster
                print('None nodata value')
                nodata_val =  np.float(256) # 256 - nodatavalue for rgb raster
                print(nodata_val)
            band1=main_raster_obj.GetRasterBand(1).ReadAsArray() # Read the first band of the main rgb raster as array
            print(band1.shape)
            xmin = main_raster_obj.GetGeoTransform()[0] # Get lower extents
            ymax = main_raster_obj.GetGeoTransform()[3] # Get top extents
            xsize = main_raster_obj.RasterXSize # Get total length in X axis
            ysize = main_raster_obj.RasterYSize # Get total length in X axis
            xstart = xmin +main_raster_obj.GetGeoTransform()[1]/2 # Get the start point - this should be the mid-point of the left bottom most grid - that is why we add ( for X-axis go right side from min X ) half of the resolution value to the initial start point - to get the mid-point 
            ystart = ymax - main_raster_obj.GetGeoTransform()[1]/2 # Get the start point - this should be the mid-point of the right topmost grid - that is why we subtract ( for Y-axis go down side from max Y ) half of the resolution value to the initial start point - to get the mid-point 
            res = main_raster_obj.GetGeoTransform()[1] # Save the resolution value

            x = np.arange(xstart, xstart+xsize*res, res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the initial grid's mid-point and add half resolution values all the way till topmost grid
            y = np.arange(ystart, ystart-ysize*res, -res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the topmost grid's mid-point and subtract half resolution values all the way till bottom most grid

            # The X values should in ascending order and Y values in descending order

            # Sometimes translating a raster leaves some pixels in the corners behind and so the shape of the raster slightly changes. To match with the original shape, we compare the original shape and translated raster and add/remove pixel values to match them both
            print(x.shape,y.shape)
            
            if(x.shape[0]>band1.shape[1]): # If the created x array has more values than the band's x values
                x_diff = x.shape[0]-band1.shape[1] # Then remove the extra values. This line gets the count of extra values
                x=x[:-x_diff] # Slice the array by removing the extra value's indices

            if(x.shape[0]<band1.shape[1]): # If the created x array has less values than the band's x values
                x_diff = band1.shape[1] - x.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                for idf in range(x_diff): # For each extra value needed
                    x = np.insert(x, x[-1]+res) # Adding one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

            if(y.shape[0]>band1.shape[0]): # If the created x array has more values than the band's y values
                y_diff = y.shape[0]-band1.shape[0] # Then remove the extra values. This line gets the count of extra values
                y=y[:-y_diff] # Slice the array by removing the extra value's indices

            if(y.shape[0]<band1.shape[0]): # If the created x array has less values than the band's y values
                y_diff = band1.shape[0] - y.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                for idf in range(y_diff): # For each extra value needed
                    y = np.insert(y, y[-1]-res) # Subtracting one more coordinate value with value equal to the previous value subtracted by the resolution value - this will be the center point of next below grid

            print(x.shape,y.shape)
            X1,Y1 = np.meshgrid(x,y,copy=False) # Create a meshgrid  - this will output two variables which have same format as the band values from raster - look up meshgrid to know more. Important - meshgrid should be in this format : X values should in ascending order and Y values in descending order

            del x,y # Delete this x and y
            gc.collect()
            save_this_df = pd.DataFrame(columns=['East_generated','North_generated']) # Create a new dataframe with east and north column names as specified
            save_this_df['East_generated'] = X1.flatten() # Flattening will convert to 1-d array from n-d array
            save_this_df['North_generated'] = Y1.flatten() # Flattening will convert to 1-d array from n-d array
            print('With all nan and values ', save_this_df.shape)
            gc.collect()   

            # # here get nodata values and remove
            mr = np.ma.masked_equal(band1,nodata_val) ## Try to get new mask spcific to each band/file - because a lot of times they might not be the same as the rgb raster and using the same maks as the rgb raster might give nodata values 
            new_x = np.ma.masked_array(X1, mr.mask) # Apply the mask to x and y meshgrid values
            new_y = np.ma.masked_array(Y1, mr.mask) # Apply the mask to x and y meshgrid values
            del X1,Y1
            gc.collect()
            new_x = np.ma.compressed(new_x) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
            new_y = np.ma.compressed(new_y) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
            gc.collect()
            print(new_x.shape)
            print(new_y.shape)

            df = pd.DataFrame(columns=['East_generated','North_generated']) # Create another dataframe with these east and north column names
            df['East_generated']=np.array(new_x) # Copy the compressed east coordinate array values into the east column # Store x and y to dataframe- these do not have any nan or no-data values
            df['North_generated']=np.array(new_y) # Copy the compressed north coordinate array values into the north column # Store x and y to dataframe- these do not have any nan or no-data values
            del new_x,new_y
            gc.collect()

            for i in sel_cols3[3:6]: # Now using that mask to apply on three bands of the raster - to get r,g,b. Also, find the band number of r,g,b instead of taking first three bands directly - they might be in different order
                band_number = band_name_number[i] # Get band number of each feature (r,g,b) from this list
                band1 = main_raster_obj.GetRasterBand(band_number).ReadAsArray() # Get the band values as array
                new_band1 = np.ma.masked_array(band1, mr.mask) #Apply mask for band values
                del band1
                new_band1 = np.ma.compressed(new_band1) # Compress to get only values without nodata values
                df[i]=np.array(new_band1) # Copy the compressed band values into a new column named after the Ith feature name # Store to dataframe

            del new_band1
            gc.collect()


            df = df.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values
            df=df.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
            print( " Main raster df ")
            print(df.shape)
            print(df.describe())
            print(df.head())

            print("  THESE MANY ROWS ARE AVAILABLE",len(df))

            if(len(df) == 0 ): # If there are no proper values - when we split tif files into many parts, some parts may not have any band values - just nodata values. So once we remove all no-data values, there will be no other points left - in that case just write this tif file back to output tif and merge it at the end with all other output split parts
                save_this_df['NODATA']=np.float32(nodata_val) # Get the nodata values and store to this dataframe
                mdf = np.array(save_this_df['NODATA'].values, dtype=np.float32) # Convert to array and change data type to float32 - uses less memory to store files
                import rasterio as rio   
                with rio.open("tif_processing/input_"+ str(output_count) +"_part.tif") as src2: # Read the input split file and copy its metadata to store it to output tif file of this specific split part
                    ras_data = src2.read() # Read the raster data
                    ras_meta = src2.profile # Read the meta data of raster
                # make any necessary changes to raster properties, e.g.:
                
                ras_meta['dtype'] = "float32" # Since all values are nodata values, we can just use a float 32 value to store them - higher the precision- more storage the tif file would consume
                ras_meta['nodata'] = np.float32(nodata_val)
                ras_meta['count'] = 1 # Specify only one band in output tif file
                ras_meta['options']=['COMPRESS=DEFLATE']
                # inDs = gdal.Open("input_"+ str(output_count) +"_part.tif")
                ras_meta['crs'] = main_raster_obj.GetProjection() # Get CRS on main rgb raster file and save it as the meta data crs of the output raster file
                
                if(output_count<10): # Store the file in format 001,002 upto 400 - since we divide the main raster into 20x20 filter, there will be 400 parts
                    with rio.open("tif_processing/output_00"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 001 to 010
                elif(output_count>10 and output_count<100):
                    with rio.open("tif_processing/output_0"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 010 to 099
                else:
                    with rio.open("tif_processing/output_"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                        dst.write(mdf.reshape(ysize,xsize), 1)  # Store for output number values between 100 to 400 

                dst = None
                src2= None
                main_raster_obj =None
                inDs =None

                # gdal.Translate('output_' + str(output_count) +'_part.tif', dem, projWin = (xmin, ymax, xmax, ymin), xRes = res, yRes = -res, dtype='float32' ,options=['COMPRESS=DEFLATE'] )
                # main_raster_obj = None
                import os
                os.remove("tif_processing/input_"+ str(output_count)+"_part.tif") # Remove the input split file of this iteration

            # Now if there are actual values present, which are not nodata values - then get other raster values like chainage etc. by matching the coordiante x and y values 
            if(len(df)>0):
                for i in sel_cols3[6:]: # For rest of the columns other than r,g,b
                    # for multiple files
                    band_numb = band_name_number[i] # Get the band number of the Ith feature name
                    total_count = 0
                    determined_band_numb = 0
                    loop_done=0

                    # determine which file it belongs to
                    # For every file do the same as done earlier - get the specific split part( we already computed xmin xmax ymin ymax above), get this small part, check for proper values and merge with dataframe
                    for fl in full_files_name: # Go through all files and check which file has the band values / band name of the particular iteration (Ith feature name)
                        if(loop_done==0): # As long as loop is active
                            fl_count = rasterfile_bands_dict[fl] # For every file , get the total band count from this dict
                            total_count = total_count + fl_count # Add the total count to band count
                            print(fl_count, fl,i, band_numb, total_count )
                            if( (band_numb > total_count) == False ): # Compare with the values from javascript - if the band number of this feature is not greater than the total count - it means this band must be in this file. So we search more in this particular file 
                                file_name_val = fl # We set the file name to the current iteration's filename

                                # determine the band number in that file name
                                total_count = total_count - fl_count # Subtract the total count from the file's total band count - this is to reset the count to the start value of this band. So now, we can have a loop over the total band count of the file, keep adding 1 to the total count and see if the values match. If they match - that Kth value is the band number of the Ith feature in that raster file
                                for k in range( fl_count ): # K is the iteraction count for the total band count of this raster file 'fl'
                                    if( band_numb == ( total_count + (k+1) ) ): # If the band number from the list matches with the total count + Kth value+1 - then K+1 is the  band number
                                        src = None
                                        determined_band_numb = k + 1 # Set band number of the Ith feature
                                        print(determined_band_numb,file_name_val)
                                        
                                        # Get the raster object of the file
                                        src = rastfile_warp[str(file_name_val)] # Get the dem object saved earlier, for the raster filename 'file_name_val' 

                                        # For every tif split (tile), get the same split from other files 
                                        other_raster_obj= gdal.Translate("", src, format='vrt', projWin = (xmin_all, ymax_all, xmax_all, ymin_all), xRes = res_all, yRes = -res_all)

                                        nodata_val_ch = other_raster_obj.GetRasterBand(determined_band_numb).GetNoDataValue()  # Get the nodata value of the raster
                                        print(nodata_val_ch)
                                        if(nodata_val_ch==None): # If nodata value is none then make it -3.402823e+38 for non-rgb raster
                                            print('None nodata')
                                            nodata_val_ch= np.float(-3.402823e+38) # nodatavalue for non-rgb raster
                                            print(nodata_val_ch)
                                        
                                        band1 = other_raster_obj.GetRasterBand(determined_band_numb).ReadAsArray() # Read the first band of the main rgb raster as array
                                        xmin = other_raster_obj.GetGeoTransform()[0] # Get lower extents
                                        ymax = other_raster_obj.GetGeoTransform()[3] # Get top extents
                                        xsize = other_raster_obj.RasterXSize # Get total length in X axis
                                        ysize = other_raster_obj.RasterYSize # Get total length in Y axis
                                        xstart = xmin + other_raster_obj.GetGeoTransform()[1] / 2 # Get the start point - this should be the mid-point of the left bottom most grid - that is why we add ( for X-axis go right side from min X ) half of the resolution value to the initial start point - to get the mid-point 
                                        ystart = ymax - other_raster_obj.GetGeoTransform()[1] / 2 # Get the start point - this should be the mid-point of the right topmost grid - that is why we subtract ( for Y-axis go down side from max Y ) half of the resolution value to the initial start point - to get the mid-point 
                                        res = other_raster_obj.GetGeoTransform()[1] # Save the resolution value

                                        x = np.arange(xstart, xstart + xsize * res, res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the initial grid's mid-point and add half resolution values all the way till topmost grid
                                        y = np.arange(ystart, ystart - ysize * res, -res) # This is one way to get all coordinates in the raster - to create a list of coordinates starting from the topmost grid's mid-point and subtract half resolution values all the way till bottom most grid

                                        print(band1.shape)
                                        print(x.shape,y.shape)

                                        # The X values should in ascending order and Y values in descending order

                                        # Sometimes translating a raster leaves some pixels in the corners behind and so the shape of the raster slightly changes. To match with the original shape, we compare the original shape and translated raster and add/remove pixel values to match them both
                                        if(x.shape[0]>band1.shape[1]): # If the created x array has more values than the band's x values
                                            x_diff = x.shape[0]-band1.shape[1] # Then remove the extra values. This line gets the count of extra values
                                            x=x[:-x_diff] # Slice the array by removing the extra value's indices

                                        if(x.shape[0]<band1.shape[1]): # If the created x array has less values than the band's x values
                                            x_diff = band1.shape[1] - x.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                                            for i in range(x_diff): # For each extra value needed
                                                x = np.insert(x, x[-1]+res) # Adding one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

                                        if(y.shape[0]>band1.shape[0]): # If the created x array has more values than the band's y values
                                            y_diff = y.shape[0]-band1.shape[0] # Then remove the extra values. This line gets the count of extra values
                                            y=y[:-y_diff] # Slice the array by removing the extra value's indices

                                        if(y.shape[0]<band1.shape[0]): # If the created x array has less values than the band's y values
                                            y_diff = band1.shape[0] - y.shape[0] # Then add more values at the end of the list. This line gets the count of extra values needed
                                            for i in range(y_diff): # For each extra value needed
                                                y = np.insert(y, y[-1]-res) # Subtracting one more coordinate value with value equal to the previous value plus resolution value - this will be the center point of next grid

                                        X1, Y1 = np.meshgrid(x, y, copy=False) # Create a meshgrid  - this will output two variables which have same format as the band values from raster - look up meshgrid to know more. Important - meshgrid should be in this format : X values should in ascending order and Y values in descending order
                                        del x
                                        del y
                                        gc.collect()

                                        
                                        mr = np.ma.masked_equal(band1, nodata_val_ch) ## Try to get new mask spcific to each band/file - because a lot of times they might not be the same as the rgb raster and using the same maks as the rgb raster might give nodata values 
                                        new_x = np.ma.masked_array(X1, mr.mask) # Apply the mask to x and y meshgrid values
                                        new_y = np.ma.masked_array(Y1, mr.mask) # Apply the mask to x and y meshgrid values

                                        del X1,Y1
                                        gc.collect()
                                        new_x = np.ma.compressed(new_x) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 
                                        new_y = np.ma.compressed(new_y) # Compressing will convert remove masked values and convert the masked format( n-d ) to 1-d array format 

                                        temp_df = pd.DataFrame(columns=['East_generated', 'North_generated', i]) # Create another dataframe with these east and north column names
                                        temp_df['East_generated'] = np.array(new_x) # Copy the compressed east coordinate array values into the east column
                                        temp_df['North_generated'] = np.array(new_y) # Copy the compressed north coordinate array values into the north column

                                        del new_x
                                        del new_y
                                        gc.collect()

                                        new_band1 = np.ma.masked_array(band1, mr.mask) #Apply mask for band values
                                        del band1
                                        new_band1 = np.ma.compressed(new_band1) # Compress to get only values without nodata values

                                        temp_df[i] = np.array(new_band1) # Copy the compressed band values into a new column named after the Ith feature name # Store to dataframe
                                        temp_df = temp_df.drop_duplicates(subset=['East_generated','North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values
                                        temp_df = temp_df.reset_index().drop(['index'], axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                                        df = pd.merge(df, temp_df, how='inner', on=['East_generated','North_generated']) # Now merge on common values from df and temp_df - this appends all feature columns from different iteration of features, into one single dataframe
                                        print (' Adding other raster df ')
                                        print(df.head())
                                        print(df.shape)


                gc.collect()
    
                df=df.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                df['ID'] = np.array(df.index) # Add an ID column
                print('After all bands')
                print(df.head())
                print(df.shape)

                process_options=json.loads(process_options_obj) # Get the list of the process options value- as we know by default it is best_options - the only value as of the current app version
                
                ## This df is the final full-data for processing
                full_data = split_dataframe(df) # Read the full-data csv not as whole, but in chunks of size 10000000 each : Check the split_dataframe function - batch processing by performing multiple iterations to complete all batches and then finally merge prediction values of all batches into one single csv and output to user 
                del df

                if("best_options" in process_options): # When the value in process_options list is 'best_options'
                    print("in best options")
                    full_river=pd.DataFrame() # Initialize an empty dataframe
                    # data_reference=load_data('data_for_all_models.csv')
                    if(ml_algo_name=="Technique 1"): # If the selected algorithm to apply on full-data is technique 1
                        model=joblib.load('apply_pretrained_model_storage/'+'Technique 1.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 1" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 5"): # If the selected algorithm to apply on full-data is technique 5
                        model=joblib.load('apply_pretrained_model_storage/'+'Technique 5.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 5" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 2"): # If the selected algorithm to apply on full-data is technique 5
                        model=joblib.load('apply_pretrained_model_storage/'+'Technique 2.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 2" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 3"): # If the selected algorithm to apply on full-data is technique 5
                        model=joblib.load('apply_pretrained_model_storage/'+'Technique 3.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 3" # Define a column name to add to the final dataframe

                    if(ml_algo_name=="Technique 4"): # If the selected algorithm to apply on full-data is technique 5
                        model=joblib.load('apply_pretrained_model_storage/'+'Technique 4.pkl') # Load the previously dumped pickle file of model - this pickle file contains all pre-trained parameters and other info about the trained models 
                        col_name="Technique 4" # Define a column name to add to the final dataframe
                    z=0
                    option_csv=pd.read_csv('apply_pretrained_model_storage/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                    process_options=[] # Initialze an empty list
                    for i in range(len(option_csv)): # For all the values in the option_csv dataset
                        process_options.append(option_csv.iloc[i]['best_option']) # Append the values into the new list
                    print(process_options)
                    for jun in full_data: # For every batch ( chunk ) of the full-data, do all the below codes
                        jun=jun.dropna() # Drop nan values if any
                        processed_data=jun.copy() # Make a copy of the batch full-data
                        z=z+1 # Keep track of the chunk or cluster or current batch number
                        print("cluster ",z)
                        print(processed_data.index[0],processed_data.index[-1])
                        if(actual_status=="Yes"): # Check if there is an actual target variable
                            predi=pd.DataFrame() # Initiate empty dataframe
                            predi[sel_tar]=processed_data[sel_tar].copy() # If actual variable is present and selected by the user, then copy the target values and keep this as a separate dataframe and merge along with the predictions dataframe at the end
                        elif(actual_status=="No"): # Check if there is no actual target variable
                            predi=pd.DataFrame() # If no actual target variable, then just initiate empty dataframe

                        to_merge=pd.DataFrame() # Empty dataframe to merge later
                        merged=pd.DataFrame() # Empty dataframe to merge predictions of every batch processing iteration and the id, east and north of every batch-processing iteration - there is no batch processing of full-data in curve-fitting, but there is batch processing in simplified workflow since its computation load is huge
                        to_merge=pd.DataFrame(processed_data[[pid,east,north]]) # Copy the id, east and north columns from the main dataframe to be processed, into the 'to_merge' dataframe - use this dataframe to merge with the  predictions dataframe at the end

                        # other selected colourspaces add
                        # check if 0 value is present in r,g,b
                        r=sel_cols3[3] #3 because of east north corrd input. Get R column name  
                        g=sel_cols3[4] # Get G column name
                        b=sel_cols3[5] # Get B column name  

                        new_sel_cols3 = sel_cols3.copy() # Keep a copy of the original sel_cols3 - which has the selected full-data features. The reason to keep a copy is in the next few steps - new features will be added to the list and hence we need to have the original features to extract initial columns from the next batch of the batch-processed full-data

                        if(processed_data.describe()[r]['min']==0):   # If the minimum R value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                
                            processed_data[r]=np.where(processed_data[r]==0,0.1,processed_data[r])
                        if(processed_data.describe()[g]['min']==0):   # If the minimum G value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                
                            processed_data[g]=np.where(processed_data[g]==0,0.1,processed_data[g])
                        if(processed_data.describe()[b]['min']==0):   # If the minimum B value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                
                            processed_data[b]=np.where(processed_data[b]==0,0.1,processed_data[b])

                        if(processed_data.describe()[r]['min']<0):    # If the minimum R value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                            
                            processed_data[r]=np.where(processed_data[r]<0,0.1,processed_data[r])
                        if(processed_data.describe()[g]['min']<0):    # If the minimum G value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                           
                            processed_data[g]=np.where(processed_data[g]<0,0.1,processed_data[g])
                        if(processed_data.describe()[b]['min']<0):    # If the minimum B value is less than 0, then also change it to 0.1 - log calculations don't work with values less than 0 in denominator                                                                           
                            processed_data[b]=np.where(processed_data[b]<0,0.1,processed_data[b])

                        if( 'rgb' in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If only rgb is selected in the options
                            print('same dataset - no hsv only rgb')

                        elif( 'rgb' not in colourspaces and 'hsv' not in colourspaces ): # Feats variable contains what colourspace has been selected. If none selected - use only rgb colourspace
                            print('same dataset - no hsv only rgb')

                        elif( 'rgb' not in colourspaces and 'hsv' in colourspaces ): # If only hsv present, then check zero values for hsv and then generate logarthmic combinations based on hsv values - log(v/h) and log(v/s)
                            print('change in dataset - no rgb only hsv')
                            hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                            processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                            processed_data[r] = processed_data2['H_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data[g] = processed_data2['S_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data[b] = processed_data2['V_generated'].copy() # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                            processed_data = processed_data.rename(columns={r: 'H_generated', g: 'S_generated', b: 'V_generated'}) # When rgb is not selected, then rename the rgb columns to hsv and copy the hsv values in place of the rgb values 
                            for ix, valx in enumerate(new_sel_cols3): # Feats is the variable which holds all the selected columns. So we need to substitue rgb names with hsv names since our new dataframe will not have rgb values as it was not selected as the colourspace
                                if(valx == r): #if list value matches with r, then rename it to h. Same for g and b.
                                    new_sel_cols3[ix] = 'H_generated' # Rename R to H_generated - ix is the index of R pixel feature name in the list
                                if(valx == g):
                                    new_sel_cols3[ix] = 'S_generated' # Rename G to S_generated - ix is the index of G pixel feature name in the list
                                if(valx == b):
                                    new_sel_cols3[ix] = 'V_generated' # Rename B to V_generated - ix is the index of B pixel feature name in the list
                            
                            # check if 0 value is present in r,g,b
                            if(processed_data.describe()['H_generated']['min']==0):  # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                 
                                processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                            if(processed_data.describe()['S_generated']['min']==0):  # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                 
                                processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                            if(processed_data.describe()['V_generated']['min']==0):  # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                 
                                processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                            del processed_data2

                        elif( 'rgb' in colourspaces and 'hsv' in colourspaces ): # If both hsv and rgb are present then check zeros for both and then create logarthmic combination for rgb features only
                            print('change in dataset - rgb and hsv')
                            hsv_vals = processed_data[[r,g,b]].apply( lambda x: rgb_to_hsv( x[r],x[g], x[b] ), axis=1) # Use lambda function to quickly convert rgb to hsv values 
                            processed_data2 = pd.DataFrame( list(hsv_vals) , columns= ['H_generated','S_generated','V_generated'], index=processed_data.index ) # Create new dataframe with just hsv values
                            processed_data.insert(6, 'H_generated', processed_data2['H_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data.insert(7, 'S_generated', processed_data2['S_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe
                            processed_data.insert(8, 'V_generated', processed_data2['V_generated'].copy()) # Since there is no rgb, rename the r,g,b columns to H_generated, S_generated and V_generated and copy the hsv values to replace the rgb values in the dataframe

                            #insert in new_sel_cols3  - since we created new features, they should be added for further use in the next parts of the code
                            new_sel_cols3.insert(6, 'H_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                            new_sel_cols3.insert(7, 'S_generated') # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions
                            new_sel_cols3.insert(8, 'V_generated')  # Insert into both, the dataframe and feature list - at positions 6,7,8 - right after r,g,b positions

                            # check if 0 value is present in r,g,b
                            if(processed_data.describe()['H_generated']['min']==0):   # If the minimum H value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                                    
                                processed_data['H_generated']=np.where(processed_data['H_generated']==0,0.1,processed_data['H_generated'])
                            if(processed_data.describe()['S_generated']['min']==0):   # If the minimum S value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                                   
                                processed_data['S_generated']=np.where(processed_data['S_generated']==0,0.1,processed_data['S_generated'])
                            if(processed_data.describe()['V_generated']['min']==0):   # If the minimum V value is 0, then change it to 0.1 - log calculations don't work with 0 values in denominator                                                                                                   
                                processed_data['V_generated']=np.where(processed_data['V_generated']==0,0.1,processed_data['V_generated'])       

                            del processed_data2 # Delete the temporary dataframe

                        r=send_feats_train[3] # Get R column name - the R feature name in the INPUT TRAINING DATA
                        g=send_feats_train[4] # Get G column name - the G feature name in the INPUT TRAINING DATA
                        b=send_feats_train[5] # Get B column name - the B feature name in the INPUT TRAINING DATA

                        sel_cols2=new_sel_cols3[3:]     # Only those features used for model training/ curve fitting - exclude the target column (last column name in the list is the target column name)
                        processed_data=processed_data[sel_cols2].copy() # Create the main subset from the initial dataframe - this new dataframe would have only features from R,G,B, to other variables - NOT east, north, id etc, - note that there is no target variable here
                        old_cols=new_sel_cols3[3:] # To rename the full-data column names to be same as the input training-data column names, get all the current full-data column names from r,g,b, till the last selected feature
                        print(old_cols, new_cols)
                        for j in range(len(old_cols)): # For every current full-data column name
                            processed_data.rename(columns={old_cols[j]: new_cols[j]},inplace=True) # Like mentioned before, need to rename columns to same name - some algorithms in the processing stage need the feature names to be same as the data columns they were trained on
                        

                        print("start",processed_data.shape)
                        if("logperm_gen" in process_options): # This line checks if logperm_gen is in the process_options list and if it's there, then the logperm_gen function is called and the data is updated
                            processed_data=logperm_gen_full(processed_data,r,g,b, colourspaces)
                            print("start1",processed_data.shape)

                        if("greyscale_gen" in process_options): # This line checks if greyscale_gen is in the process_options list and if it's there, then the greyscale_gen function is called and the data is updated
                            processed_data=greyscale_gen_full(processed_data,r,g,b)
                            print("start2",processed_data.shape)

                        if("cluster_gen" in process_options): # This line checks if cluster_gen is in the process_options list and if it's there, then the cluster_data function is called and the data is updated
                            processed_data=cluster_data_full_pt(processed_data,r,g,b)  
                            print("start3",processed_data.shape)

                        if(("poly_corr_gen" in process_options) & ("cluster_gen" in process_options)): # This line checks if poly_corr_gen and cluster_gen is in the process_options list and if it's true, then the poly_creation_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=poly_creation_cluster_full(processed_data)    
                            print("start4",processed_data.shape)

                        if(("poly_corr_gen" in process_options) & ("cluster_gen" not in process_options)): # This line checks if poly_corr_gen is there but cluster_gen is not in the process_options list and if it's true, then the poly_creation_cluster_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=poly_creation_no_cluster_full(processed_data)     
                            print("start4",processed_data.shape)

                        if(("ss_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if ss_scale and cluster_gen is in the process_options list and if it's true, then the standard_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=standard_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques
                            print("start5",processed_data.shape)

                        if(("ss_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if ss_scale is there but cluster_gen is not in the process_options list and if it's true, then the standard_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=standard_scale_no_cluster_full_pt(processed_data,"common")   # Common specifies same data for all ml techniques  
                            print("start5",processed_data.shape)
                        
                        if(("minmax_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if minmax_scale and cluster_gen is in the process_options list and if it's true, then the min_max_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=min_max_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques
                            print("start6",processed_data.shape)

                        if(("minmax_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if minmax_scale is there but cluster_gen is not in the process_options list and if it's true, then the min_max_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=min_max_scale_no_cluster_full_pt(processed_data,"common")     # Common specifies same data for all ml techniques

                        if(("robust_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if robust_scale and cluster_gen is in the process_options list and if it's true, then the robust_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=robust_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques

                        if(("robust_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if robust_scale is there but cluster_gen is not in the process_options list and if it's true, then the robust_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=robust_scale_no_cluster_full_pt(processed_data,"common")    # Common specifies same data for all ml techniques 

                        if(("power_scale" in process_options) & ("cluster_gen" in process_options)): # This line checks if power_scale and cluster_gen is in the process_options list and if it's true, then the power_scale_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=power_scale_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques

                        if(("power_scale" in process_options) & ("cluster_gen" not in process_options)): # This line checks if power_scale is there but cluster_gen is not in the process_options list and if it's true, then the power_scale_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=power_scale_no_cluster_full_pt(processed_data,"common")   # Common specifies same data for all ml techniques

                        if("pearson_correl" in process_options): # This line checks if pearson_correl is in the process_options list 
                            processed_data=correl_full_pt(processed_data,"common") # Common specifies same data for all ml techniques
                            print("start7",processed_data.shape)

                        if("poly_lasso_trans" in process_options): # This line checks if poly_lasso_trans is in the process_options list and if it's there, then the lasso_reg_bestop function is called and the data is updated
                            lasso_result=pd.read_csv('apply_pretrained_model_storage/'+'lasso_reg.csv') # Load this csv which was stored when the poly_lasso_trans function ran while finding the best preprocessing methods 
                            processed_data=lasso_reg_full(processed_data,lasso_result) # We pass the csv to copy the column names present in the lasso_result dataset - lasso_result dataset was saved after computing lasso reg during feature engineering phase
                            print("start8",processed_data.shape)

                        if(("pca_trans" in process_options) & ("cluster_gen" in process_options)): # This line checks if pca_trans and cluster_gen is in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_cluster function is called and the data is updated. #If clustering data was an option in the combination, then run this code
                            processed_data=pca_reduction_cluster_full_pt(processed_data,"common")     # Common specifies same data for all ml techniques

                        if(("pca_trans" in process_options) & ("cluster_gen" not in process_options)): # This line checks if pearson_correl is there but cluster_gen is not in the process_options list and also checks if the number of columns in the dataset up until this point is atleast 3 - because after passing through all the before feature reduction processes, the number of columns might have reduced to less than 3. In that case PCA can not work since there are already less than 3 columns. Only if all these conditions are true, then the pca_reduction_no_cluster is called and the data is updated. #If clustering data was not an option in the combination, then run this code
                            processed_data=pca_reduction_no_cluster_full_pt(processed_data,"common") # Common specifies same data for all ml techniques

                        print(processed_data.tail())
                        print("model",model,ml_algo_name)

                        # To make sure lr nn and svr recieves scaled data - the reason is for lr nn and svr algorithms, non-scaled values hugely affect the predictions - so to make sure they give valid predictions, we scale the data values if scaling was not inluded as an option while computing the best_options. If scaling not done already , scale it now.
                        if(ml_algo_name=="Technique 1" or ml_algo_name=="Technique 5"): # If the selected algorithm is technique 1 or if the selected algorithm is technique 5
                            option_csv_df=pd.read_csv('apply_pretrained_model_storage/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                            options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                            no_scale_check_lr=0 # Initialize a scaling check for lr nn and svr 
                            options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                            if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                                no_scale_check_lr=1 # atleast one scaling
                            if(no_scale_check_lr==0): # If none present, then do this
                                sc_t1_5=joblib.load('apply_pretrained_model_storage/'+'sc_forT1&5only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                                processed_data=sc_t1_5.transform(processed_data) # Scale and transform the processed full-data
                                print('no scale')
                            
                        if(ml_algo_name=="Technique 4"): # If the selected algorithm is technique 4
                            option_csv_df=pd.read_csv('apply_pretrained_model_storage/'+'option_csv.csv') # Load the options_csv file which contains the best processing methods list
                            options_csv=option_csv_df['best_option'] # Get the value of the best_option column, in the options_csv dataframe
                            no_scale_check=0 # Initialize a scaling check for nn (lr and svr already done)
                            options_values=list(option_csv_df['best_option'])[0] # Get the first row of the best processing options dataframe and convert to a list
                            if( ("ss_scale" in options_values) | ("minmax_scale" in options_values) | ("robust_scale" in options_values) ): # Check if any of the three scaling options are present in the best options list 
                                no_scale_check=1 # atleast one scaling
                            if(no_scale_check==0): # If none present, then do this
                                sc_t4=joblib.load('apply_pretrained_model_storage/'+'sc_forT4only.pkl') # Load this pickle file which contains the info of the scaling object which was fit on the training data during the ml trainng phase, load this object and apply the same on the processed full-data
                                processed_data=sc_t4.transform(processed_data) # Scale and transform the processed full-data
                                print('no scale')
                        predi[str(col_name)+'_Pred']=model.predict(processed_data) # Predict depth values for the processed batch-data and etore the prediction values to the dataframe under the column name 'ALGO_NAME_pred' - ALGO_NAME is the chosen ml algorithm name
                        df_preds2=pd.DataFrame(data=predi[str(col_name)+'_Pred'].values,columns= [str(col_name)+'_Pred'], index=to_merge.index) # Copy the same dataframe to a new one - with index from this batch's input full-data dataset - copy the index from this dataframe to this new dataframe so that the values are in correct order while merging all batches at the end 
                        if(actual_status=="Yes"): # Check if there is an actual target variable
                            df_preds2[sel_tar]=predi[sel_tar]   # If actual variable is present and selected by the user, then copy the target values into this new dataframe 
                        
                        merged=pd.concat([to_merge,df_preds2],axis=1) # Merge all columns and predicted values into one dataframe
                        merged=merged.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
                        full_river=pd.concat([full_river,merged],axis=0) # Merge the current batch values with the previous batch (if any) values - concats on row base, adds more rows after the previous batches' rows
                        full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the merged dataframe and remove the index column
                        del merged # Delete the temporary dataframe
                        del df_preds2 # Delete the temporary dataframe
                        del predi,processed_data # Delete the temporary dataframe

                    del full_data

                    full_river=full_river.sort_values(by=pid) # Sort the values of ID column
                    full_river=full_river.reset_index().drop(['index'],axis=1)  # Reset the sorted dataframe and remove the index column
                    
                    print("Sample of the output")
                    print(full_river.head())
                    

                    jsonfiles_full_river_preds = json.loads(full_river.head().to_json(orient='records')) # Save the first 5 rows of the input data with just selected independent features, as json object to show to the user using javascript  
                    preds_file_name='Pretrained_Model_Predictions_'+str(col_name).lower()+'.tif' # get the selected ml_algo name and it's prediction column name to copy that name to the output file's name

                    ## Convert back to raster tif file
                    full_river = full_river.drop(['ID'],axis=1) # Drop the ID column which was created during the process
                    
                    full_river = full_river.drop_duplicates(subset=['East_generated', 'North_generated'], keep='first') # More often there are duplicates - so reduce computation load by removing duplicate values 
                    full_river = full_river.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                    
                    #save_this_df = pd.merge(save_this_df, full_river,  how='left', on=['East_generated','North_generated']).fillna(float(nodata_val))

                    # save_this_df = pd.merge(save_this_df, full_river,  how='left', on=['East_generated','North_generated']).fillna(float(nodata_val))
                    mdf=pd.DataFrame()
                    # Now we have the output predictions and east,north coordinates in a dataframe. We need to merge with the save_this_df dataframe - this dataframe contains all east, north coordinates. All coordinates includes the ones we removed because they had no-data values.
                    # The reason to merge it is when we create output tif file, the array values should match the exact number of values as was the input band. After removing nodata values, there will be change in number of values, so we need to add those nodata values back and also make sure we add coordinates at the right indices - should not be random.
                    chks = split_dataframe(save_this_df) # We split the dataframe because merge function is computationally very heavy and splitting into smaller chunks and merging each chunk based on the east and north coordinates will be efficient
                    del save_this_df

                    for i in chks: # For each chunk
                        print(i.shape)
                        mdf1 = pd.merge(i, full_river,  how='left', on=['East_generated',
                                        'North_generated']).fillna(float(nodata_val)) # Merge this chunk of predictions with the east, north data 
                        mdf = pd.concat([mdf, mdf1 ],axis=0) # Merge the updated chunk with the previously merged rows
                        del i
                        del mdf1
                        gc.collect()
                        mdf = mdf.reset_index().drop(['index'],axis=1) # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index

                    del chks,full_river
                    gc.collect()
                    print(mdf.shape)
                    print(mdf.head())

                    print('merge done')
                    # Now we have complete dataframe with number of values matching this particular split part's total raster values, so convert to tif and store with output number
                    mdf = mdf.reset_index().drop(['index'],axis=1)  # Reset the index to make sure everything is in order and remove the index column that gets created automatically while resetting index
                    mdf = np.array( mdf[ str(col_name)+'_Pred' ].values, dtype=np.float32 ) # Get the prediction column values and change data type to float32 - uses less memory to store files
                    
                    # Read main rgb file and get its extents to output tif
                    xsize = main_raster_obj.RasterXSize # Get total length in X axis
                    ysize = main_raster_obj.RasterYSize # Get total length in Y axis

                    import rasterio as rio   
                    with rio.open("tif_processing/input_"+ str(output_count) +"_part.tif") as src2: # Read the input split file and copy its metadata to store it to output tif file of this specific split part
                        ras_data = src2.read() # Read the raster data
                        ras_meta = src2.profile # Read the meta data of raster

                    # make any necessary changes to raster properties, e.g.:
                    ras_meta['dtype'] = "float32" # Since all values are nodata values, we can just use a float 32 value to store them - higher the precision- more storage the tif file would consume
                    ras_meta['nodata'] = np.float32(nodata_val)
                    ras_meta['count'] = 1 # Specify only one band in output tif file
                    ras_meta['options']=['COMPRESS=DEFLATE']
                    # inDs = gdal.Open("input_"+ str(output_count) +"_part.tif")
                    ras_meta['crs'] = main_raster_obj.GetProjection() # Get CRS on main rgb raster file and save it as the meta data crs of the output raster file
                   
                    print(ras_meta)

                    if(output_count<10): # Store the file in format 001,002 upto 400 - since we divide the main raster into 20x20 filter, there will be 400 parts
                        with rio.open("tif_processing/output_00"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                            dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 001 to 010
                    elif(output_count>10 and output_count<100):
                        with rio.open("tif_processing/output_0"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                            dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 010 to 099
                    else:
                        with rio.open("tif_processing/output_"+ str(output_count) +"_part.tif", 'w', **ras_meta) as dst:
                            dst.write(mdf.reshape(ysize,xsize), 1) # Store for output number values between 100 to 400

                    dst = None

                    src2= None
                    main_raster_obj = None
                    other_raster_obj = None
                    
                    import os
                    os.remove("tif_processing/input_"+ str(output_count)+"_part.tif") # Remove the input split file of this iteration

                    #if you get errors with proj.db or with crs - then use this:
                    # inDs = gdal.Open(raster_filename)
                    # ras_meta['crs'] = inDs.GetProjection()
                    del mdf
                    gc.collect()
        
    src = None
    dem = None
    del rastfile_warp
    gc.collect()

    # Merge all tif into a single output file
    import glob
    demList = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Use glob to get all files name with matching names

    print(demList)

    vrt = gdal.BuildVRT("merged.vrt",demList) # Build a VRT and pass in the list of all output tif file names to be merged into one single tif file
    gdal.Translate("tif_processing/"+ preds_file_name, vrt,  xRes= res_all, yRes=-res_all, creationOptions=["COMPRESS=DEFLATE"] ) # It will merge all files into one file and compress the size using deflate algorithm
    vrt = None               

    # Remove all output tifs if present
    import glob
    demList1 = glob.glob("tif_processing/output_[0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file
    demList2 = glob.glob("tif_processing/output_[0-9][0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file
    demList3 = glob.glob("tif_processing/output_[0-9][0-9][0-9]_part.tif") # Remove all separate output tif files - they are not needed as they are merged into one single file

    demList=[] # Initialize empty list
    for i in demList1: # For all files in this list of separate output tif files - this is for 0-10 output numbered files
        demList.append(i) # Append to the new list
    for i in demList2: # For all files in this list of separate output tif files - this is for 10-100 output numbered files
        demList.append(i) # Append to the new list
    for i in demList3: # For all files in this list of separate output tif files - this is for 100-400 output numbered files
        demList.append(i) # Append to the new list

    for i in demList: # For all files in the fully appened list of output tif files
        os.remove(i) # Remove them

    if(actual_status=="No"): # Send back the sample predictions and the output file name (predictions csv file name)
        return jsonify( { 'data':jsonfiles_full_river_preds, 'output_file_name':preds_file_name } )

            

@app.route('/save_files_tif_pt', methods=['POST','GET'])
def save_files_tif_pt(): # Copy the output predictions file of apply-pre-trained module to the user's preferred location

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    
    file_name_obj= request.form['file_name']
    file_name=json.loads(file_name_obj)

    shutil.copy('tif_processing/'+ file_name,file_path)
    #os.remove( 'model_files/'+ file_name )

    return jsonify("Done")







































###################################################  MODULE 1 - DATA PREPARATION FUNCTIONS ####################################################

app.config['FILE_SAVE_DPR']="data_prep_storage/"
@app.route('/upload_file_dpr',methods=['POST','GET'])
def upload_file_dpr():
    print("in file present")
    if( request.method=="POST" ):
        if( request.files ):

            if os.path.exists('data_prep_storage'):
                shutil.rmtree('data_prep_storage', ignore_errors=True)
            if not os.path.exists('data_prep_storage'):
                os.mkdir('data_prep_storage')

            file=request.files["file"]
            print(file)
            file.save(os.path.join(app.config['FILE_SAVE_DPR'], file.filename))
            print(" file saved ")
            #f"{file.filename}
            if(os.path.exists( os.path.join(app.config['FILE_SAVE_DPR'],"temp_pipeline.csv") )):
                os.remove( os.path.join(app.config['FILE_SAVE_DPR'],"temp_pipeline.csv") )
            os.rename( os.path.join(app.config['FILE_SAVE_DPR'], file.filename), os.path.join(app.config['FILE_SAVE_DPR'],"temp_pipeline.csv") )
            if(os.path.exists( os.path.join(app.config['FILE_SAVE_DPR'], file.filename)) ):
                os.remove( os.path.join(app.config['FILE_SAVE_DPR'], file.filename) )
            
            res = make_response(jsonify({"message":  f"File uploaded successfully "}),200)

            return res
        return render_template('simplified_workflow.html/upload_file.html')



# full river input and process
@app.route('/train_river_input_dpr',methods=['POST'])
def train_river_input_dpr():
        
    df=pd.read_csv('data_prep_storage/'+'temp_pipeline.csv', nrows=10) 
    full_columns=df.columns
    df_jsonfiles = json.loads(df.head().to_json(orient='records'))
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))


@app.route('/calculate_distance_dpr',methods=['GET','POST'])
def calculate_distance_dpr():
    df=pd.read_csv('data_prep_storage/'+'temp_pipeline.csv')
    yorn=request.form['yorn']
    sel_feats=request.form['sel_feat']
    print(" Selected feats from ajax: ",sel_feats)
    print(yorn,type(yorn))
    #sel_feats is already in json format
    feats  = json.loads(sel_feats)
    print(feats)
    df=df[feats].copy()
    df=df.dropna()
    df=df.sort_values(by=feats[0])
    df=df.reset_index().drop(['index'],axis=1)
    if(yorn=="no_distance"):
        east=feats[1]
        north=feats[2]
        agg=0
        df['Distance_new']=0
        for i in range(1,len(df)):
            diff=distance_cal(df[east].iloc[i],df[north].iloc[i],df[east].iloc[i-1],df[north].iloc[i-1])
            agg=agg+diff
            df['Distance_new'].iloc[i]=agg
        print(df.head())
        df=df.round(3)
        print(df.head())       
        jsonfiles = json.loads(df.head().to_json(orient='records'))
        
        if(True in list(df.iloc[-1].isnull()[:])):
            drop_ind=df.index[-1]
            print(drop_ind)
            df.drop(drop_ind,axis=0,inplace=True)
        df.sort_index(inplace=True)

        r=feats[3] #3 because of east north corrd input 
        g=feats[4]
        b=feats[5]   
        # check if 0 value is present in r,g,b
        if(df.describe()[r]['min']==0):                   
            df[r]=np.where(df[r]==0,0.1,df[r])
        if(df.describe()[g]['min']==0):                   
            df[g]=np.where(df[g]==0,0.1,df[g])
        if(df.describe()[b]['min']==0):                   
            df[b]=np.where(df[b]==0,0.1,df[b])

        if(df.describe()[r]['min']<0):                   
            df[r]=np.where(df[r]<0,0.1,df[r])
        if(df.describe()[g]['min']<0):                   
            df[g]=np.where(df[g]<0,0.1,df[g])
        if(df.describe()[b]['min']<0):                   
            df[b]=np.where(df[b]<0,0.1,df[b])

        df.to_csv('data_prep_storage/'+'temp_pipeline_2.csv',index=False)
        del df
        return (jsonify(jsonfiles))
        #return jsonify("parsed")
        
    else:
        return jsonify("Could not parse")

@app.route('/pre_distance_dpr',methods=['GET','POST'])
def pre_calc_distance_dpr():
    df=pd.read_csv('data_prep_storage/'+'temp_pipeline.csv')
    sel_feats=request.form['sel_feat']
    feats  = json.loads(sel_feats)
    print(feats)
    cdist_get=request.form['cdist_send']
    cdist = json.loads(cdist_get)
    df=df.sort_values(by=feats[0])
    df=df.reset_index().drop(['index'],axis=1)
    if(("Distance_new" in df.columns) & (cdist!="Distance_new")):
        df.drop(['Distance_new'],axis=1,inplace=True)
    df.rename(columns={cdist: "Distance_new"},inplace=True)
    new_df = pd.concat([df[feats].copy(),df['Distance_new']],axis=1)  
    new_df=new_df.dropna()
    new_df=new_df.reset_index().drop(['index'],axis=1)
    if(True in list(new_df.iloc[-1].isnull()[:])):
        drop_ind=new_df.index[-1]
        new_df.drop(drop_ind,axis=0,inplace=True)
    
    new_df.sort_index(inplace=True)

    r=feats[3] #3 because of east north corrd input 
    g=feats[4]
    b=feats[5]   
    # check if 0 value is present in r,g,b
    if(new_df.describe()[r]['min']==0):                   
        new_df[r]=np.where(new_df[r]==0,0.1,new_df[r])
    if(new_df.describe()[g]['min']==0):                   
        new_df[g]=np.where(new_df[g]==0,0.1,new_df[g])
    if(new_df.describe()[b]['min']==0):                   
        new_df[b]=np.where(new_df[b]==0,0.1,new_df[b])

    if(new_df.describe()[r]['min']<0):                   
        new_df[r]=np.where(new_df[r]<0,0.1,new_df[r])
    if(new_df.describe()[g]['min']<0):                   
        new_df[g]=np.where(new_df[g]<0,0.1,new_df[g])
    if(new_df.describe()[b]['min']<0):                   
        new_df[b]=np.where(new_df[b]<0,0.1,new_df[b])

    new_df.to_csv('data_prep_storage/'+'temp_pipeline_2.csv',index=False)
    new_df=new_df.round(3)   
    jsonfiles = json.loads(new_df.head().to_json(orient='records'))
    del new_df
    return (jsonify(jsonfiles))


# Use temp_pipeline_2.csv for original and changed data values

@app.route('/get_features',methods=['GET','POST'])
def get_features():
    df=pd.read_csv('data_prep_storage/'+'temp_pipeline_2.csv', nrows=10) 
    full_columns=df.columns
    return (jsonify({ 'columns':list(full_columns) }))


@app.route('/graph_plot',methods=['GET','POST'])
def graph_plot():
    df=pd.read_csv('data_prep_storage/'+'temp_pipeline_2.csv') 
    full_columns=df.columns
    
    x_axis_obj= request.form['xax']
    x_axis=json.loads(x_axis_obj)

    y_axis_obj= request.form['yax']
    y_axis=json.loads(y_axis_obj)


    return (jsonify({ 'xax_data':list(df[x_axis]), 'yax_data':list(df[y_axis]) }))

@app.route('/graph_plot_post',methods=['GET','POST'])
def graph_plot_post():
    df=pd.read_csv('data_prep_storage/'+'smoothed_temp_df.csv') 
    df_old=pd.read_csv('data_prep_storage/'+'temp_pipeline_2.csv') 
    full_columns=df.columns
    
    x_axis_obj= request.form['xax']
    x_axis=json.loads(x_axis_obj)

    y_axis_obj= request.form['yax']
    y_axis=json.loads(y_axis_obj)
    print(y_axis,x_axis)
    removed_point_indx_obj= request.form['removed_point_indx']
    removed_point_indx=json.loads(removed_point_indx_obj)
    removed_point_indx = list(removed_point_indx)
    print(removed_point_indx)

    json_x_removed = list( df_old[x_axis].iloc[removed_point_indx] )
    json_y_removed = list( df_old[y_axis].iloc[removed_point_indx] )

    return (jsonify({ 'xax_data':list(df[x_axis]), 'yax_data':list(df[y_axis]), 'xax_data_removed':json_x_removed, 'yax_data_removed':json_y_removed }))


@app.route('/preprocess',methods=['GET','POST'])
def preprocess():
    df=pd.read_csv('data_prep_storage/'+'temp_pipeline_2.csv') 
    full_columns=df.columns
    old_df=df.copy()
    smooth_algo_selected_obj= request.form['smooth_algo_selected']
    smooth_algo_selected=json.loads(smooth_algo_selected_obj)

    ss_thershold_value_obj= request.form['ss_thershold_value']
    ss_thershold_value=json.loads(ss_thershold_value_obj)

    iso_frac_obj= request.form['iso_frac']
    iso_frac=json.loads(iso_frac_obj)

    mvg_dist_value_obj= request.form['mvg_dist_value']
    mvg_dist_value=json.loads(mvg_dist_value_obj)
    
    mvg_diff_value_obj= request.form['mvg_diff_value']
    mvg_diff_value=json.loads(mvg_diff_value_obj)
    
    ckd_radius_value_obj= request.form['ckd_radius_value']
    ckd_radius_value=json.loads(ckd_radius_value_obj)

    ckd_thershold_value_obj= request.form['ckd_thershold_value']
    ckd_thershold_value=json.loads(ckd_thershold_value_obj)

    feat_smooth_obj= request.form['feat_smooth']
    feat_smooth=json.loads(feat_smooth_obj)

    new_feats_obj= request.form['new_feats']
    new_feats=json.loads(new_feats_obj)

    if(smooth_algo_selected == "simple_spatial"):
        ss_thershold_value=float(ss_thershold_value)
        new_df, outliers_len, removed_point_indx = find_outlier_simple_spatial(old_df, ss_thershold_value, feat_smooth)
        new_df.to_csv('data_prep_storage/smoothed_temp_df.csv', index=False)

    if(smooth_algo_selected == "iso_forest"):
        iso_frac=float(iso_frac)
        new_df, outliers_len, removed_point_indx = find_outlier_iso_forest(old_df, iso_frac, feat_smooth, new_feats)
        new_df.to_csv('data_prep_storage/smoothed_temp_df.csv', index=False)

    if(smooth_algo_selected == "moving_avg"):
        mvg_dist_value=float(mvg_dist_value)
        mvg_diff_value=float(mvg_diff_value)
        new_df, outliers_len, removed_point_indx = find_outlier_moving_avg(old_df, mvg_dist_value, mvg_diff_value, feat_smooth)
        new_df.to_csv('data_prep_storage/smoothed_temp_df.csv', index=False)

    if(smooth_algo_selected == "spatial_ckd"):
        ckd_radius_value=float(ckd_radius_value)
        ckd_thershold_value=float(ckd_thershold_value)
        new_df, outliers_len, removed_point_indx = find_outlier_spatial_ckd(old_df, ckd_radius_value, ckd_thershold_value, feat_smooth, new_feats)
        new_df.to_csv('data_prep_storage/smoothed_temp_df.csv', index=False)

    old_shape = str(df.shape)
    new_shape = str(new_df.shape)

    dist_new = list(new_df['Distance_new'])
    feat_smooth_new = list(new_df[feat_smooth])

    removed_dist = list(df['Distance_new'].iloc[removed_point_indx].values)
    removed_feat_smooth = list(df[feat_smooth].iloc[removed_point_indx].values)
    del old_df
    del new_df
    del df
    
    l=[1,2,3]
    if(type(removed_point_indx)!=type(l)):
        removed_point_indx1 = list(removed_point_indx)
    else:
        removed_point_indx1 = removed_point_indx

    return (jsonify({ 'input_dim':old_shape, 'output_dim':new_shape, 'outliers_len':str(outliers_len), 'dist_new':dist_new, 'feat_smooth_new':feat_smooth_new, 'removed_dist':removed_dist, 'removed_feat_smooth':removed_feat_smooth, 'removed_point_indx':removed_point_indx1 }))




def find_outlier_moving_avg(sample,thresh_dist,thresh_diff,variable_option):
    outliers=[]
    clus=max(sample['Distance_new'])/thresh_dist
    print(clus)
    prev_dist=0
    for_avg=0
    rev_avg=0

    for l in range(0,int(clus)):
        print("Cluster",l)
        if(l==0):
            start_ind=0
            end_ind=sample[sample['Distance_new']>thresh_dist].index[0]
            while(start_ind==end_ind and start_ind!=sample.index[-1]):
                new_dist=prev_dist+thresh_dist
                end_ind=sample[sample['Distance_new']>new_dist].index[0]
                prev_dist=new_dist
            for i in range(start_ind,end_ind):
                dist_current=sample.iloc[i]['Distance_new']
                if( len(sample[sample['Distance_new']>(dist_current+thresh_dist)].index) > 0 ):
                    gr_dc_in=sample[sample['Distance_new']>(dist_current+thresh_dist)].index[0]
                    for_avg=np.mean(sample[variable_option].iloc[i:gr_dc_in])
                    if(abs(sample.iloc[i][variable_option]-for_avg)>float(thresh_diff)):
                        outliers.append(i) 
                    
        elif(l>0 and l<int(clus)-1):
            
            if(prev_dist+thresh_dist>sample['Distance_new'].iloc[-1]):
                end_ind=len(sample)-1
                
            else:      
                end_ind=sample[sample['Distance_new']>prev_dist+thresh_dist].index[0]
                
            while(start_ind==end_ind and start_ind!=sample.index[-1] ):
                new_dist=prev_dist+thresh_dist
                if(new_dist>sample['Distance_new'].iloc[-1]):
                    end_ind=len(sample)-1
                else:
                    end_ind=sample[sample['Distance_new']>new_dist].index[0]
                prev_dist=new_dist
                
            if(end_ind==len(sample)-1):
                l=int(clus)-2
            else:
                for i in range(start_ind,end_ind):
                    dist_current=sample.iloc[i]['Distance_new']
                    # if( sample.iloc[i]['Distance_new'] - sample.iloc[end_ind]['Distance_new'] > 40 ):
                    #     start_ind=end_ind+2 
                    if( len(sample[sample['Distance_new']>(dist_current+thresh_dist)].index) > 0 ):
                        gr_dc_in=sample[sample['Distance_new']>(dist_current+thresh_dist)].index[0]
                        for_avg=np.mean(sample[variable_option].iloc[i:gr_dc_in])  
                    rev_avg_pts=[]
                    DID_REV=0
                    if( len(sample[sample['Distance_new']<(abs(dist_current-thresh_dist))].index) > 0 ):
                        rev_dc_in=sample[sample['Distance_new']<(abs(dist_current-thresh_dist))].index[-1]
                        if( abs( sample.iloc[rev_dc_in]['Distance_new'] - sample.iloc[start_ind]['Distance_new']) < 30 ):
                            
                            for j in range(rev_dc_in,i):
                                h=j
                                while(h in outliers):
                                    h=h-1  
                                rev_avg_pts.append(sample[variable_option].iloc[h])
                            rev_avg=np.mean(rev_avg_pts)
                            DID_REV=1
                    if( DID_REV == 1):  
                        if((abs(sample.iloc[i][variable_option]-for_avg)>float(thresh_diff))|(abs(sample.iloc[i][variable_option]-rev_avg)>float(thresh_diff))):
                            outliers.append(i) 
                    elif( DID_REV==0 ):
                        if((abs(sample.iloc[i][variable_option]-for_avg)>float(thresh_diff))):
                            outliers.append(i) 

        elif(l==int(clus)-1): 
            end_ind=len(sample)-1
            for i in range(start_ind,end_ind):
            #   print("Index",i)
                dist_current=sample.iloc[i]['Distance_new']
                rev_avg_pts=[]
                DID_REV=0
                if( len(sample[sample['Distance_new']<(abs(dist_current-thresh_dist))].index) > 0 ):
                    rev_dc_in=sample[sample['Distance_new']<(abs(dist_current-thresh_dist))].index[-1]
                    if( abs( sample.iloc[rev_dc_in]['Distance_new'] - sample.iloc[start_ind]['Distance_new']) < 30 ):
                        for j in range(rev_dc_in,i):
                            h=j
                            while(h in outliers):
                                h=h-1  
                            rev_avg_pts.append(sample[variable_option].iloc[h])
                        rev_avg=np.mean(rev_avg_pts)  
                        DID_REV=1
                if( DID_REV == 1):         
                    if((abs(sample.iloc[i][variable_option]-for_avg)>float(thresh_diff))|(abs(sample.iloc[i][variable_option]-rev_avg)>float(thresh_diff))):
                        outliers.append(i)
                elif( DID_REV==0 ):
                    if((abs(sample.iloc[i][variable_option]-for_avg)>float(thresh_diff))):
                        outliers.append(i) 
                            
        prev_dist=sample.iloc[i]['Distance_new']
        
        start_ind=end_ind  
        # if( sample.iloc[start_ind]['Distance_new'] - sample.iloc[end_ind]['Distance_new'] > 40 ):
        #     start_ind=end_ind+2 
        #     end_ind=end_ind+3          
        
    print("Detected number of outliers :",len(outliers))    
    drop_data=sample.drop(outliers,axis=0)    
    drop_data=drop_data.reset_index().drop(['index'],axis=1)    
    print(drop_data.head())
    
    return drop_data, len(outliers), outliers                    



def find_outlier_iso_forest(sample,fraction,variable_option,new_feats):

    lof_data=np.array( sample[new_feats[3:]].drop("Distance_new",axis=1) )
    clf=IsolationForest(contamination=fraction,n_estimators=1000)
    sample['ISO']=clf.fit_predict(lof_data)
    outlier_index=sample[sample['ISO']==-1].index
    drop_data=sample.drop(outlier_index,axis=0)
    drop_data=drop_data.reset_index().drop(['index'],axis=1)
    drop_data.drop('ISO',axis=1,inplace=True)

    return drop_data, len(outlier_index), outlier_index  



def find_outlier_simple_spatial(sample,threshold,variable_option):
    outlier_index=[]
    sample['Outlier_filter']=''
    for i in range(1,len(sample)):  
      h=i-1#0.065
#    j=i+1
      while(h in outlier_index):
          h=h-1
      neg_diff=abs(sample[str(variable_option)].iloc[i]-sample[str(variable_option)].iloc[h])
      if(neg_diff>float(threshold)):
          sample['Outlier_filter'].iloc[i]='Yes'
          outlier_index.append(i)
      else:
          sample['Outlier_filter'].iloc[i]='No'
    print("Detected number of outliers :",len(outlier_index))     
    sample.drop(outlier_index,axis=0,inplace=True)
    sample=sample.reset_index().drop(['index','Outlier_filter'],axis=1) 

    return sample, len(outlier_index), outlier_index  



def find_outlier_spatial_ckd(sample,radius_value,thershold_value,variable_option,new_feats):
    east=new_feats[1]
    north=new_feats[2]
    print(sample.shape)
    latlon = np.array(sample[[east,north]].copy())
    point_tree = spatial.cKDTree(latlon)
    sample_preds = sample[variable_option]
    out_ind=[]
    for i in range(len(latlon)):
        ixs = point_tree.query_ball_point(latlon[i], radius_value)
        avg_all = np.mean(sample_preds[ixs])
        if( abs( sample_preds[i] - avg_all ) > thershold_value ):
            out_ind.append(i)

    print("Detected number of outliers ", len(out_ind) )
    sample.drop(out_ind,axis=0,inplace=True)
    print(sample.shape)
    sample=sample.reset_index().drop(['index'],axis=1)  

    return sample, len(out_ind), out_ind  



@app.route('/save_proc',methods=['GET','POST'])
def save_proc():
    if (os.path.exists('data_prep_storage/temp_pipeline_2.csv')):
        os.remove(  'data_prep_storage/temp_pipeline_2.csv' )
    os.rename( 'data_prep_storage/smoothed_temp_df.csv', 'data_prep_storage/temp_pipeline_2.csv' )
    return jsonify("Done")


@app.route('/discard_proc',methods=['GET','POST'])
def discard_proc():
    if (os.path.exists('data_prep_storage/smoothed_temp_df.csv')):
        os.remove( 'data_prep_storage/smoothed_temp_df.csv' )
    return jsonify("Done")


@app.route('/save_proc_csv', methods=['POST','GET'])
def save_proc_csv():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)

    filename_obj= request.form['filename']
    filename=json.loads(filename_obj)

    print(file_path)
    filename = stripextension(filename.split('/')[-1])
    if (os.path.exists('data_prep_storage/'+filename+'_Processed.csv')):
        os.remove( 'data_prep_storage/'+filename+'_Processed.csv' )
    os.rename( 'data_prep_storage/temp_pipeline_2.csv', 'data_prep_storage/'+filename+'_Processed.csv' )
    shutil.copy( 'data_prep_storage/'+filename+'_Processed.csv' , file_path )
    os.remove( 'data_prep_storage/'+filename+'_Processed.csv' )
    output_fn = filename+'_Processed.csv'

    return (jsonify({ 'stat':"Done", 'output_fn':output_fn }))























































































#### SECTION 13: DATA PREPARATION MODULE - RASTER AND SHP SECTION


############################################################ RASTER AND SHP SECTION #####################################################    
def get_file_extension(filename): # Get the file extension of an input filename
    return filename.split('.')[-1]

@app.route('/data_preparation')
def data_preparation(): # Main module for data preparation - check for license validity before starting module functions
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-04-05 16:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('data_preparation.html',title='Data Preparation') # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error


#### THIS BELOW FUNCTION IS NON-FUNCTIONAL - WE ENTIRELY MOVE AWAY FROM THE METHOD OF COPYING FILES CAN BE LARGE IN SIZE, INSTEAD OPT TO A REMOTE ACCESS BASED METHOD
@app.route('/upload_raster_shp_files',methods=['POST','GET'])
def upload_raster_shp_files(): # Upload the input shape files and raster files - copy them to local file path. NON-FUNCTIONAL
    
    if( request.method=="POST" ): # If the request is POST
        if( request.files ): # If there are input files from the user
            if os.path.exists('data_prep_storage'): # If data_prep_storage folder already exists then delete and and remake the folder - This is the folder to store all dump input shape and raster files and also any processed output files
                shutil.rmtree('data_prep_storage') # Remove any existing folders
            os.mkdir('data_prep_storage') # Create a new folder with name 'data_prep_storage'

            file_count=request.form['file_count'] # Get the file count from javascript - the number of input files
            file_count=int(file_count) # Convert the string to integer count
            print(file_count)
            for i in range(file_count): # For all the values in range of 0 to the total files count
                i+=1 # Increpent 
                req_name="file"+str(i)
                file=request.files[req_name]
                print(file)
                if(os.path.exists( os.path.join(app.config['FILE_SAVE_SHP'], file.filename)) ):
                    os.remove( os.path.join(app.config['FILE_SAVE_SHP'], file.filename) )
                file.save(os.path.join(app.config['FILE_SAVE_SHP'], file.filename))

            print(" file saved ")
            #f"{file.filename}
           
            res = make_response(jsonify({"message":  f"File uploaded successfully "}),200)

            return res
        return jsonify("no files")


#### THIS BELOW FUNCTION IS NON-FUNCTIONAL - WE ENTIRELY MOVE AWAY FROM THE METHOD OF COPYING FILES CAN BE LARGE IN SIZE, INSTEAD OPT TO A REMOTE ACCESS BASED METHOD
app.config['FOLDER_SAVE']="data_prep_storage/"
@app.route('/upload_raster_shp_folder',methods=['POST','GET'])
def upload_raster_shp_folder(): # NON-FUNCTIONAL
    if( request.method=="POST" ):
        if( request.files ):
            if os.path.exists('data_prep_storage'):
                shutil.rmtree('data_prep_storage')
            os.mkdir('data_prep_storage')

            file_count=request.form['file_count']
            file_count=int(file_count)
            file_names=[]
            for i in range(file_count):
                i+=1
                req_name="file"+str(i)
                file=request.files[req_name]
                print(" file name" ,file.filename)
                file_names.append(file.filename)

                # create folder 
                folder_name=file.filename.split('/')[0]
                
                if not os.path.exists('data_prep_storage/'+ folder_name):
                    os.mkdir('data_prep_storage/'+folder_name)

                file.save(os.path.join(app.config['FOLDER_SAVE'], file.filename))
            
            for file_obj in file_names:
                shutil.copy( 'data_prep_storage/'+file_obj, 'data_prep_storage/' )

            print(folder_name)
            print(" file saved ")

            shutil.rmtree('data_prep_storage/'+folder_name)
           
            res = make_response(jsonify( {"message":  f"File uploaded successfully " } ),200)

            return res
        
        return jsonify("no files")


@app.route('/load_raster_shp_files', methods=['POST','GET'])
def load_raster_shp_files(): # This function loads the raster and shp file locations, reads each file to get info on band structure and number of bands etc for each file and send these info back to javascript
    model_files_obj=request.form['model_files_name'] # get the file name - location of the raster and shp files. We don't copy the file to the local 'data_prep_storage' folder because some files can be very large and we do not have to consume extra space, we can just get the file location path, and access/run functions on them without having to copy to the app's location
    model_files_name=json.loads(model_files_obj) # Convert the json object to a list
    print(model_files_name)

    folder_mode_click_obj=request.form['folder_mode_click'] # Get the json object that indicates if the input is folder mode or file mode
    folder_mode_click=json.loads(folder_mode_click_obj) # Convert the json object to a string
    
    print(folder_mode_click)
    import gc
    gc.collect()

    if os.path.exists('data_prep_storage'): # If data_prep_storage folder already exists then delete and and remake the folder - This is the folder to store all dump input shape and raster files and also any processed output files
        shutil.rmtree('data_prep_storage') # Remove any existing folders
    os.mkdir('data_prep_storage') # Create a new folder with name 'data_prep_storage'

    #Check extension for raster(tif) files and count number
    rasterfile_bands_dict={} # Create an empty dict - this will hold raster file names and all the band information

    #### For folder mode
    if(folder_mode_click == "1"):
        dir_name=model_files_name[0] # Get the folder name ( the first name in the list is the folder name - only one input possible and it would be a folder location that the user inputs )

        tif_files=[] # Create an empty list to hold all the tif files name
        shp_files=[] # Create an empty list to hold all the shp files name
        shp_cols=[] # Initialize empty list to hold all the column names present in the shape file
        count_raster=0 # Keep a count of number of raster files
        for file in os.listdir(dir_name): # For all files in the directory
            file_ext=str(str(file).split('.')[-1])   # Split the file name based on '.' and get the last value of this split resulting list
            if(file_ext=="tif"): # Check if the last value of the split resulting list is tif - checking extension for raster (tif) files
                count_raster+=1 # Increment raster count by 1
                full_filename= dir_name + '/' + file # Get the full file path for each tif file in the folder
                tif_files.append( full_filename ) # Append the full file path of each tif file to the list
                if(len(tif_files)>0 ):
                    #for each raster, get number of bands present
                    raster=rasterio.open( full_filename ) # Use rasterio to open and read a tif file
                    bands= raster.count # get the band count of the tif file

                    rasterfile_bands_dict[full_filename] = int(bands) # Store the band count as the value and the filename as the key, to this dict
                
        
        # Read shape file and later ask if user wants to retain any columns
        
            if(file_ext == "shp"): # Now if the last value of the split resulting list is shp - checking extension for shape files
                full_filename= dir_name + '/' + file # Get the full file path for each tif file in the folder
                shp_files.append(full_filename) # Append the full file path of each tif file to the list

        if(len(shp_files)>0 ):
            import geopandas as gpd
            for shp_filename in shp_files: # For each of the shapefiles
                pts1 = gpd.read_file(shp_filename) # Use geopandas library to read shape files and convert into shapefile point values
                shp_data1=pd.DataFrame(pts1) # Create a dataframe from the shapefile point values
                shp_data1.drop(['geometry'],axis=1,inplace=True) # Drop the geometry column which naturally comes while converting point shapefile to dataframe
                shp_cols= list(shp_data1.columns) # Copy all the column names to a list

            del shp_data1 # Delete the temporary dataframe
        if(count_raster==0): # If there is no raster file in the selected folder, show error message to user
            return jsonify( { 'count_raster':count_raster, 'message':"No TIF files found! Please select the correct files/folder ", 'shp_cols': shp_cols } )

        elif(len(shp_files)==0):  # If there is no shape file in the selected folder, show error message to user
            return jsonify( { 'count_raster':count_raster, 'message':"No SHP files found! Please select the correct files/folder ", 'shp_cols': shp_cols, 'len_shp_files':len(shp_files) } )

        else: # If all files are present, then send the band info and shape column names to javascript
            print(rasterfile_bands_dict)
            return jsonify( { 'count_raster':count_raster, 'rasterfile_bands_dict':rasterfile_bands_dict, 'tif_files':tif_files,  'shp_files':shp_files, 'shp_cols': shp_cols } )


    #### For files mode

    if(folder_mode_click == "0"):

        tif_files=[] # Create an empty list to hold all the tif files name
        shp_files=[] # Create an empty list to hold all the shp files name
        shp_cols=[] # Initialize empty list to hold all the column names present in the shape file
        count_raster=0 # Keep a count of number of raster files
        for file in model_files_name: # For all the input files (mix of raster and shape files)
            file_ext=str(str(file).split('.')[-1])   # Split the file name based on '.' and get the last value of this split resulting list
            if(file_ext=="tif"): # Check if the last value of the split resulting list is tif - checking extension for raster (tif) files
                count_raster+=1  # Increment raster count by 1
                tif_files.append( file ) # Append the full file path of each tif file to the list
                #for each raster, get number of bands present
                if(len(tif_files)>0 ):
                    raster=rasterio.open( file ) # Use rasterio to open and read a tif file
                    bands= raster.count # get the band count of the tif file

                    rasterfile_bands_dict[file] = int(bands) # Store the band count as the value and the filename as the key, to this dict
                
        # Read shape file and later ask if user wants to retain any columns
            if(file_ext == "shp"): # Now if the last value of the split resulting list is shp - checking extension for shape files
                shp_files.append(file) # Append the full file path of each tif file to the list
        
        if(len(shp_files)>0 ):
            import geopandas as gpd
            for shp_filename in shp_files: # For each of the shapefiles
                pts1 = gpd.read_file(shp_filename) # Use geopandas library to read shape files and convert into shapefile point values
                shp_data1=pd.DataFrame(pts1) # Create a dataframe from the shapefile point values
                shp_data1.drop(['geometry'],axis=1,inplace=True) # Drop the geometry column which naturally comes while converting point shapefile to dataframe
                shp_cols= list(shp_data1.columns) # Copy all the column names to a list

            del shp_data1 # Delete the temporary dataframe
        if(count_raster==0): # If there is no raster file in the selected folder, show error message to user
            return jsonify( { 'count_raster':count_raster, 'message':"No TIF files found! Please select the correct files/folder ", 'shp_cols': shp_cols } )

        elif(len(shp_files)==0): # If there is no shape file in the selected folder, show error message to user
            return jsonify( { 'count_raster':count_raster, 'message':"No SHP files found! Please select the correct files/folder ", 'shp_cols': shp_cols, 'len_shp_files':len(shp_files) } )

        else: # If all files are present, then send the band info and shape column names to javascript
            print(rasterfile_bands_dict)
            return jsonify( { 'count_raster':count_raster, 'rasterfile_bands_dict':rasterfile_bands_dict, 'tif_files':tif_files,  'shp_files':shp_files, 'shp_cols': shp_cols } )


@app.route('/extract_data', methods=['POST','GET'] )
def extract_data(): # Now extract all available data from all the input files. For generating a final dataframe - we need x coordinate, y coordinate and other values present in the raster or shp files. The easting and northing coordinate values can be extracted from the shape file, the rest of the columns come from the raster bands

    model_files_obj=request.form['model_files_name'] # Get all the files path as a json object
    model_files_name=json.loads(model_files_obj) # Convert the json object to list

    input_features_obj=request.form['input_features'] # Get all the band names that the user inputs - band name for each band of every file
    input_features=json.loads(input_features_obj) # Convert the json object to list
    
    rasterfile_info_obj=request.form['rasterfile_info'] # Get all the raster file and band information which was extracted and saved to a dict, get it as a json object
    rasterfile_info=json.loads(rasterfile_info_obj) # Convert the json object to list
   
    tif_files_obj=request.form['tif_files'] # Get all the tif files path values as a json object
    tif_files=json.loads(tif_files_obj) # Convert the json object to list
    
    shp_files_obj=request.form['shp_files'] # Get all the shp files path values as a json object
    shp_files=json.loads(shp_files_obj) # Convert the json object to list
    
    feats_obj=request.form['feats'] # Get all the user selected features - we ask the user which columns they would want to retain in the contents of the final csv - like id, east, north etc
    feats=json.loads(feats_obj) # Convert the json object to list

    count_keep=0 # Keep count of total number of bands from all the raster files
    raster_band_names={}
    for i in range(len(tif_files)): # For all the tif files in the list

        file_name=tif_files[i] # Get the Ith tif file
        n_bands= int( rasterfile_info['rasterfile_bands_dict'][file_name] ) # Get the number of bands of that raster

        feature_names_store='' # Initialize an empty string
        feature_names_store=input_features[count_keep : count_keep + n_bands] # Get all the band names associated with the band number - the count keep helps keep track of the band index value and the band name (input by user) index value

        count_keep=count_keep + n_bands # Update the value of count keep by adding the number of bands of the current file

        raster_band_names[ str(file_name) ] = feature_names_store # Add more info to the dict - under the associated raster file name, copy the list of feature names(band names) for that raster

    print(raster_band_names)

    # get shp files and make csv

    import geopandas as gpd
    # import shapefile

    for shp_filename in shp_files: # For every shape file
        # pts = gpd.read_file(shp_filename)
        # shp_data=pd.DataFrame(pts)
        # print(shp_data.head())        
        # sf= shapefile.Reader(shp_filename)
        # print(sf.shapeType) # 5 means a polygon, 1 means point

        # shapes = sf.shapes()

        # x_coor=[]
        # y_coor=[]
        # for i in range(len(sf)):
        #     x_coor.append(shapes[i].points[0][0])
        #     y_coor.append(shapes[i].points[0][1])

        # shp_df=pd.DataFrame(columns=['East_generated','North_generated'])
        # shp_df['East_generated']=x_coor
        # shp_df['North_generated']=y_coor
        # print(shp_df.head())
        pts1 = gpd.read_file(shp_filename) # Read the shapefile as point values
        shp_data1=pd.DataFrame(pts1) # Create a dataframe from the shapefile point values
        print(shp_data1.head())
        # just see data end
        x_coor=[] # Initialize empty string to hold all x coordinate values 
        y_coor=[] # Initialize empty string to hold all y coordinate values 
        for i in range(len(shp_data1)): # for all the rows in the point values dataframe
            x_coor.append( np.array(shp_data1['geometry'][i])[0] ) # The geometry column contains x values, y values and other important point values - like a tuple (x,y,other point values). So the x coordinate is the first element of the tuple
            y_coor.append( np.array(shp_data1['geometry'][i])[1] ) # The geometry column contains x values, y values and other important point values - like a tuple (x,y,other point values). So the y coordinate is the second element of the tuple

        shp_df=pd.DataFrame(columns=['East_generated','North_generated']) # Create a dataframe to hold both coordinate values
        shp_df['East_generated']=x_coor # Copy the x coordinate to the east column
        shp_df['North_generated']=y_coor # Copy the y coordinate to the north column
        print(shp_df.head())
        
    coords1 = [(x,y) for x, y in zip(shp_df.East_generated, shp_df.North_generated)] # Create a zip object value for the coordinates - we do this to extract values from the raster bands - for this, we have to pick out values from the exact coordinates that were extracted from the shape files.
    print(len(coords1))

    for raster_filename in tif_files: # For each raster file
        print(raster_filename)
        tmp=pd.DataFrame(columns=['temp']) # We create a temp dataframe
        
        src = rasterio.open( os.path.join('data_prep_storage',raster_filename) ) # Using rasterio to open the raster file and read its contents
        tmp['temp'] = [x for x in src.sample(coords1)] # Using sample function we extract data from raster files only at those specific coordinates present in the shapefile's coordinate values and copy them to the temporary dataframe
        print(tmp.head())
        feature_name=raster_band_names[raster_filename] # Get the band names for each file - get all band names of all bands as a list
        print("Extracting for ", feature_name, len(feature_name)) 

        for feat in feature_name: # For each band name in the current raster file
            if(feat!=''): # If the band name is not empty
                print('in here',feat)
                val_arr=np.array( tmp['temp'].apply( lambda x: x[ feature_name.index(feat) ] ) ) # Copy the band data values from the temporary dataframe of each band, into an array
                shp_df[feat] = val_arr # Add the band name as a new column to the shapefile point values dataframe and copy the array values under that column

    print( " Final dataframe ")

    ## Now that we have extracted all band values into the dataframe, all we have to do now is to read the shapefile as dataframe again and this time copy all the column values that the user wanted to retain into the final dataframe
    # merge shapefile all contents
    pts = gpd.read_file(shp_filename) # Read the shapefile as point values
    shp_only=pd.DataFrame(pts) # Create a dataframe from the shapefile point values
    shp_only.drop(['geometry'],axis=1,inplace=True) # Drop the geometry column which naturally comes while converting point shapefile to dataframe
    shp_cols= shp_only.columns # Copy all the column names to a list
    
    for col in shp_cols: # For each column in the shp_cols list
        if(col in feats): # If this column is present in the user's selected list of features to be retained
            shp_df[col] = shp_only[col].copy() # Then copy that column name and values and add it to the final dataframe 

    print( shp_df.head() )
    shp_df.to_csv('data_prep_storage/Extracted_CSVfile.csv',index=False) # Save the final extracted dataframe as a csv to local file path - under the 'data_prep_storage' folder
    shp_final = json.loads( shp_df.head().to_json(orient='records') ) # Get json object for the first 5 rows of the final dataframe
    del pts1 # Delete the temporary dataframe
    del shp_data1 # Delete the temporary dataframe
    del shp_only # Delete the temporary dataframe
    return (jsonify(shp_final))


@app.route('/save_csv', methods=['POST','GET'])
def save_csv():

    file_path_obj= request.form['file_path']
    file_path=json.loads(file_path_obj)
    print(file_path)
    
    shutil.copy('data_prep_storage/Extracted_CSVfile.csv',file_path)
    os.remove( 'data_prep_storage/Extracted_CSVfile.csv' )

    return jsonify("Done")



#### SECTION 14: SMOOTHING OUTPUT PREDICTIONS-DATA MODULE


############################################################### MODULE 5 START #################################

@app.route('/smoothing_output',methods=['POST','GET'])
def smoothing_output():
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            # Now extract all contents of string by passing into each function
            file_macid=extract_mac_id_file(lic_content) # Get mac id from license file
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            file_subscription_days=extract_subscription_days(lic_content) # Get subscription days / validity days from license file
            print(file_subscription_days)
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
            host_name= extract_hostname( lic_content ) # Get hostname from license file
            machine_sid = extract_machinesid( lic_content ) # Get machine sid from license file
            pc_macid=get_mac_id_pc() # Get macid of local machine
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine
            pc_hostname= get_current_hostname() # Get hostname of local machine
            pc_machinesid = get_current_machinesid() # Get machine sid of local machine

            while("" in machine_sid) : # Remove spaces and empty values from list
                machine_sid.remove("")

            while("" in pc_machinesid) : # Remove spaces and empty values from list
                pc_machinesid.remove("")

            while("" in file_macid) : # Remove spaces and empty values from list
                file_macid.remove("")

            while("" in pc_macid) : # Remove spaces and empty values from list
                pc_macid.remove("")
                    
            print(file_macid,pc_macid)
            match_macid=0
            for i in pc_macid: # Check if atleast one mac id from file and from current machine matches
                if(i in file_macid): # If any values match
                    match_macid=1 # Make this value from 0 to 1
            
            match_sid=0
            for i in pc_machinesid: # Check if atleast one machine sid from file and the current machine matches
                if(i in machine_sid): # If any values match
                    match_sid=1 # Make this value from 0 to 1
            #match_macid=set(pc_macid).issubset(set(file_macid))
            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            VERSION_UNI_DATE = '2021-04-05 16:00' # This is the main part - to ensure user pays money to use further updates after one year from the start time in license. The version uni date should be less than one year of start date of license to be able to run the current version - this date changes for each release
            VERSION_UNI_DATE_dt =datetime.strptime(VERSION_UNI_DATE, format) # Convert the string date-time value to date-time object 
            updates_validity = VERSION_UNI_DATE_dt - file_starttime_new # Subtract the app release version date to the start time in license to check validity of the one year updates
            
            if( int(updates_validity.days) <= 365 ): #If this app version release date is no more than one year of the user's license start date, then continue
                if(match_macid): # If the mac id matched - at least one value is same
                    print(" Mac id Match ")
                    if( (match_sid == 1) ): # If the machine sid match - at least one value is same
                        if( int(days_rem.days) < int(file_subscription_days)): # Check if the remaining days is less than total subscription days, if so continue
                            if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                                if(os.path.exists('mac_id_request.txt')): # Check if this text file is present - this text file is created when the user requests for license if he does not have any
                                    os.remove('mac_id_request.txt') # Remove the text file
                                return render_template('smoothing_output.html',title='Smoothing output') # If all conditions match - enter the app
                            else:
                                return render_template('upgrade.html',title='Upgrade', message=" Inappropriate License file! Please select a proper license file ")   # If the license key is for some other tool then show this error
                        else:
                            return render_template('upgrade.html',title='Upgrade', message=" Your current subscription has expired! Please Upgrade to use the software  ") # If the number of days left is past the subscription days, then show this error
                    else:
                        return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ")    # If none of the machine sid from both lists match, then show this error                      
                else:
                    return render_template('upgrade.html',title='Upgrade', message=" This license is not valid on the current Machine! Please visit the Pricing plans for Subscription ") # If none of the mac ids from both lists match, then show this error
            else:
                return render_template('upgrade.html',title='Upgrade', message=" Your license does not support this version of the app and software updates are disabled. Please contact software.support@landriversea.com for further information ") # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version
        except:
            return render_template('upgrade.html',title='Upgrade', message=" Invalid License file! Please select a proper license file ") # If there is a faulty license file present, then show this error
    else:
        return render_template('upgrade.html',title='Upgrade', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ") # If no license file found in the path, then show this error


# data input and process
@app.route('/remove_outliers_input',methods=['POST','GET'])
def remove_outliers_input():
    full_files_name_obj=request.form['full_files_name'] # get the file name - location of the full-data csv file. We don't copy the file to the app's location folder because full-data is usually very large and we do not have to consume extra space, we can just get the file location path, and access/run functions on the dataset without having to copy the csv
    full_files_name=json.loads(full_files_name_obj) # Convert the json object to string
    df=pd.read_csv(full_files_name[0], nrows=10)  # read first 10 rows and send to javascript to view in app page
    full_columns=df.columns # Copy the column names from the dataset
    df_jsonfiles = json.loads(df.head().to_json(orient='records')) # Always load as json and then jsonify the data (next two lines) before sending - only way to communicate between python and javascript
    return (jsonify({'data':df_jsonfiles,'columns':list(full_columns)}))
    


@app.route('/remove_outliers',methods=['POST','GET'])
def remove_outliers(): # This function helps detect and remove outliers from the input csv's prediction column - by using spatial ckd algorithm. Inputs from user would be radius value, thershold value, feature columns - which also include the target column (predictions column)

    radius_value_obj=request.form['radius_value'] # Get the radius value from the user as a json object
    radius_value=json.loads(radius_value_obj) # Convert the json object to string

    thershold_value_obj=request.form['thershold_value'] # Get the threshold value from the user as a json object
    thershold_value=json.loads(thershold_value_obj) # Convert the json object to string
    
    send_feats_train_obj=request.form['send_feats_train'] # Get the feature columns from the user as a json object
    send_feats_train=json.loads(send_feats_train_obj) # Convert the json object to list

    full_files_name_obj=request.form['full_files_name'] # Get the file full path from the user as a json object
    full_files_name=json.loads(full_files_name_obj) # Convert the json object to string

    ids=send_feats_train[0] # Get the ID column name (first item in the column names' list)
    east=send_feats_train[1] # Get east coordinate column name
    north=send_feats_train[2] # Get north coordinate column name
    pres=send_feats_train[3] # Get the output predictions column name

    radius_value=float(radius_value) # Convert the string radius value to float
    thershold_value=float(thershold_value) # Convert the string threshold value to float

    data=pd.read_csv(full_files_name[0]) # Read the file from its path - only one input file present
    data=data.dropna() # Drop nan values
    data=data.sort_values(by=ids) # Sort the dataset by its id column
    data=data.reset_index().drop(['index'],axis=1) # Reset the dataframe and remove the index column

    inp_shape = data.shape # Get the shape of the dataframe
    latlon = np.array(data[[east,north]].copy()) # Conver the east and north coordinate values to array - the spatial ckd tree algorithm needs lat and lon values
    point_tree = spatial.cKDTree(latlon) # Create a point tree from spatial ckd tree algorithm, using teh lat and lon values - look more on point trees and spatial ckd algorithm
    data_preds = data[pres] # Get the prediction values from the dataset
    out_ind=[] # Initialize empty list to keep track of all the removed output indices - some values would be removed as part of the smoothing process
    for i in range(len(latlon)): # For all rows in the dataset 
        ixs = point_tree.query_ball_point(latlon[i], radius_value) # Apply this spatial point tree algorithm that checks each row's lat and lon values and compares with neighbouring values within the given radius
        avg_all = np.mean(data_preds[ixs]) # Get the average of all points within the given radius value, around the current point 
        if( abs( data_preds[i] - avg_all ) > thershold_value ): # Compare if the current point's prediction values are greater than the threshold value of the neighbouring points
            out_ind.append(i) # If so, add the index of the current point to the out_ind list

    print("removed outliers ", len(out_ind) )
    data.drop(out_ind,axis=0,inplace=True) # Drop all the point indices that are present in the out_ind list
    data=data.reset_index().drop(['index'],axis=1)   # Reset the updated dataframe and remove the index column
    del data_preds # Delete temporary dataframe
    del latlon # Delete temporary dataframe

    file_name = stripextension(full_files_name[0].split('/')[-1]) # Get the input file name without extensions
    
    data.to_csv(file_name + '_smoothed.csv',index=False) # Save the output csv file with the same name but with an additional string to specify that it is the smoothed version of the input file, and save this csv file to the app's local folder location
    new_shp = str(data.shape) # Get the new shape of the updated dataframe and send it to javascript, for the user to see how many rows have been removed

    del data # Delete temporary dataframe
    return (jsonify({ 'input_dim':str(inp_shape), 'output_dim':new_shp, 'numb_out':str(len(out_ind)) }))



@app.route('/save_model_files_smo', methods=['POST','GET'])
def save_model_files_smo():

    file_path_obj= request.form['file_path'] # Get a json object of the file path - of the user's selected location
    file_path=json.loads(file_path_obj) # Convert the json object to string

    full_files_name_obj=request.form['full_files_name'] # Get a json object of the input file name - full file path of the input file
    full_files_name=json.loads(full_files_name_obj) # Convert the json object to string

    print(file_path)
    
    file_name = stripextension(full_files_name[0].split('/')[-1]) + '_smoothed.csv'
    shutil.copy(file_name ,file_path)
    # os.remove(file_name)

    return jsonify(file_name)


def stripextension(input): # strips the extension off a filename
    return os.path.splitext(input)[0]


############################################################### MODULE 5 END #################################
## The license will allow the software to work forever, 
## however updates will only work for 365 days from the date of the license, after that they need to pay an SMA to continue getting updates.
## To ensure user pays money to use further updates after one year from the start time in license. The version uni date 
## should be less than one year of start date of license to be able to run the current version - this date changes for each release

@app.route('/updates', methods=['POST','GET'])
def updates(): # This function allows the user to check for any updates to the software and also have the option to auto-update and install the newer version
    import pytz
    import base64
    from datetime import datetime 
    from tzlocal import get_localzone
    # License file check:

    # getting my documents path to save license and not ask to upload at every update release
    import ctypes.wintypes
    CSIDL_PERSONAL = 5       # My Documents
    SHGFP_TYPE_CURRENT = 0   # Get current, not default value
    buf= ctypes.create_unicode_buffer(ctypes.wintypes.MAX_PATH) # Get a buffer that containes the documents path
    ctypes.windll.shell32.SHGetFolderPathW(None, CSIDL_PERSONAL, None, SHGFP_TYPE_CURRENT, buf) # Set the buffer to windows sh folder path and get the current documents path as a string

    if(os.path.exists(str(buf.value)+'/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm')): # Using the above string, we check if the license file is there - if the path exists. If it's there, then continue
        # check credentials
        try:
            with open(str(buf.value)+"/Optical Bathymetry Toolkit/Licensefile_LRSC_Bathymetry.skm", mode='rb') as file: # b is important -> binary
                cipher_read = file.read() # Open the file and read the contents in binary mode
            #obj2 = AES.new('This is a key123', AES.MODE_CFB, 'This is an IV456')
            #lic_content=obj2.decrypt(cipher_read).decode()

            # new decrypt
            message_bytes = base64.b64decode(cipher_read) # First decode with base64 into ascii bytes
            lic_content = message_bytes.decode('ascii') # Then decode the ascii bytes to string
            
            # Now extract all contents of string by passing into each function
            file_timezone=extract_time_zone_file(lic_content) # Get timezone from license file
            file_starttime=extract_start_time_file(lic_content) # Get start time from license file
            unique_key= extract_tool_unique_key(lic_content) # Get unique tool name key from license file
           
            pc_timezone=get_time_zone_pc() # Get timezone of local machine
            pc_currenttime=get_current_time_pc(pc_timezone,file_timezone) # Get current time of local machine

            format='%Y-%m-%d %H:%M' # Need date-time to be in this format 
            if(len(file_starttime)>16): # If the length of start time is > 15 characters, then restrict to first 15 
                file_starttime=file_starttime[:16] # Restrict start time to first 15 digits -just hour,min and seconds enough
            file_starttime_new =datetime.strptime(file_starttime, format) # Strip to same format as '%Y-%m-%d %H:%M'
            days_rem=pc_currenttime-file_starttime_new # Subtract the current machine time and start time in license - both are changed to the same time zone. This subtracted value is then later compared with the subscription days to check validity
            print(days_rem)
            
            if( int(days_rem.days) <= 365 ): # If this app version release date is no more than one year of the user's license start date, then continue
                if( unique_key == "Optical Bathymetry Toolkit"): # Check if the license belongs to this tool
                    return render_template('updates_check.html',title='Updates Check',message=" Your license is eligible for software updates ", update_flag=1) # If all conditions match - enter the page and update_flag=1 is to indicate the app to allow the user to update a newer version if any
                else:
                    return render_template('updates_check.html',title='Updates Check', message=" Inappropriate License file! Please select a proper license file ", update_flag=0)    # If the license key is for some other tool then show this error - and do not let the user check for updates for the software
            else:
                return render_template('updates_check.html',title='Updates Check', message=" Software updates are disabled for your license. Please contact software.support@landriversea.com for further information ", update_flag=0) # If the releas date of the current app version is more than one-year of the user's start data in license, then show this error. Will work if the user either upgrades plan or uses an older app version - and do not let the user check for updates for the software
        except:
            return render_template('updates_check.html',title='Updates Check', message=" Invalid License file! Please select a proper license file ", update_flag=0) # If there is a faulty license file present, then show this error - and do not let the user check for updates for the software
    else:
        return render_template('updates_check.html',title='Updates Check', message=" No License file found on this Machine! Please visit the Pricing plans for Subscription ", update_flag=0) # If no license file found in the path, then show this error - and do not let the user check for updates for the software


#to read https://towardsdatascience.com/data-science-you-need-to-know-a-b-testing-f2f12aff619
#  in case we dont use flask run and instead opt to use the python run command, then we have to use __name__ to run # to run using flask run - do export env things
if __name__=='__main__':
    #Timer(1.5, open_browser).start()
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # To randomly assign a free available port for python and flask to run
    sock.bind(('localhost', 0)) # Bind the socket to the first port out of all the free available ports
    port = sock.getsockname()[1] # Get the port number
    sock.close() # Close the socket
    f= open("Bathymetry_PORT.txt","w+") # Open a file objec and create a text file with write mode to save the port number to the file - we do this so that electron can listen on this port and open window once python and flask is running
    f.write(str(port)) # Write the port number to file
    f.close() # Close the file opbject
    app.run(port=port) # Run the app on this port

# app.run(host='127.0.0.1', port=5000, debug=False)

#https://www.fullstackpython.com/environment-configuration.html

# charts:
#https://blog.ruanbekker.com/blog/2017/12/14/graphing-pretty-charts-with-python-flask-and-chartjs/
#https://mdbootstrap.com/docs/jquery/javascript/charts/

# PLS AND PCA:
#https://medium.com/analytics-vidhya/dimensionality-reduction-in-supervised-framework-and-partial-least-square-regression-b557a4c6c049#:~:text=As%20discussed%20in%20the%20above,the%20data%20in%20a%20latent



#LICENSING LINKS
#https://realpython.com/python-send-email/
#https://docs.python.org/3/library/uuid.html
